{"pages":[],"posts":[{"title":"blast","text":"继上一篇edgeR基因差异分析，通过采用R中的edgeR/limma这两个包，完成了差异基因的分析。在这个分析中总共分为了9组进行差异分析，最终结果取了并集，然后对并集进行blast，进而统一进行blast2go注释，再对每一组差异分析单独的从结果中匹配出相应的注释。现在，我需要做的就是这个并集的blast，为blast2go做准备。值得注意的是，这里的blast效率可能并不高，因此会耗费大量时间，可以考虑拆分文件，开启多个进程同时比对。 1. blast简介blast全称是Basic Local Alignment Search Tool，是NCBI运营的一个基础序列比对软件，通过这个软件可以做到很好的多序列比对，比对到蛋白质或者核酸数据库中。众所周知，序列比对是生信分析的基础，不管是哪一个组学的内容，都必须进行比对才能开启下一步的工作。blast不是单纯的一个program，而是多个program的综合称呼：|program|progress||:—-|:—-||blastn|nr–&gt;nr||blastx|nr–&gt;pro||tblastn|pro–&gt;nr||blastp|pro–&gt;pro| 现在在这个项目中，我们需要做的下一步是比对到nr数据库，nr数据库是一个蛋白质序列库，我们的序列是核酸序列，因此我们使用的是blastx program，即核酸比对到蛋白质。还有另外一个数据库——nt，这个数据库是一个核酸数据库，如果需要比对到核酸的话，就可以选择这个数据库进行比对。 2. blast使用如果是第一次使用blast，那么需要做一些前期的准备工作，不过这个准备工作是通用的，完成一次后，下一次可以直接使用，现在我们从准备工作开始做起。 2.1 下载安装blast可以进入官网寻找到下载地址ftp://ftp.ncbi.nlm.nih.gov/blast/executables/blast+/LATEST/，点击网址进入选择合适的版本，download到自己的服务器或者本地中。 解压blast文件，将bin目录下内容copy到/usr/local/bin中 这一步的目的是为了让blast能够直接在服务器任何一个地方使用，而不用输入绝对路径，相当于是添加到了环境变量中，因为/usr/local/bin就是环境变量中的一个，所以同样也可以通过添加新环境变量的方式来达到以上目的。 制作软链接，将解压后的的bin目录软链接到/home/username 编辑.bashrc文件，添加export PATH=/home/username/bin/:$PATH;，即添加环境变量2.2 准备数据库在这里除了blast程序之外，还得有本地的数据库才行。so，第一步，先下载blast对应的一些数据库。 下载数据库（nr nt等）：ftp://ftp.ncbi.nlm.nih.gov/blast/ 也可以先浏览一下这个网站：https://www.ncbi.nlm.nih.gov/books/NBK62345/ 通过2.1中下载的blast软件构建索引|makeblastdb makeblastdb -in mature.fa -input_type fasta -dbtype nucl -title miRBase -parse_seqids -out miRBase -logfile File_Name options explain -in 你要格式化的fasta序列 -dbtype database的类型，nucl为核酸，prot为蛋白 -title 给数据库起名字，不能作为-db的参数使用 -parse_seqids 暂时未知 -out 给数据库起个名字，能在blast+时作为-db的参数使用 -logfile 日志文件，默认输出到屏幕 2.3 开始使用blast最好保持一个良好习惯，先新建好文件目录，整理好文件，做好记录，然后再进行blast比对。 nohup blastx -query ../up1.fasta -db /B313/public_db/blast/nr -outfmt 5 -num_threads 20 -out up1-blastx &gt;log1.out 2&gt;&amp;1 &amp; nohup blastx -query ../up4.fasta -db /B313/public_db/blast/fish_prot -outfmt 5 -num_threads 8 -out up4-blastx &gt;log4.out 2&gt;&amp;1 &amp; nohup blastx -query ../down1.fasta -db /B313/public_db/blast/fish_prot -outfmt 5 -num_threads 8 -out down1-blastx &gt;dlog1.out 2&gt;&amp;1 &amp; nohup blastx -query ../gene_data/53vsX-downgenes.fasta -db /B313/public_db/blast/fish_prot -outfmt 5 -num_threads 8 -out 53vsXdown-blastx &gt;dlog_53.out 2&gt;&amp;1 &amp; nohup blastx -query ../gene_data/53vsX-upgenes.fasta -db /B313/public_db/blast/fish_prot -outfmt 5 -num_threads 8 -out 53vsXup-blastx &gt;ulog_53.out 2&gt;&amp;1 &amp; nohup blastx -query ../gene_data/54vsY-downgenes.fasta -db /B313/public_db/blast/fish_prot -outfmt 5 -num_threads 8 -out 54vsYdown-blastx &gt;dlog_54.out 2&gt;&amp;1 &amp; nohup blastx -query ../gene_data/54vsY-upgenes.fasta -db /B313/public_db/blast/fish_prot -outfmt 5 -num_threads 8 -out 54vsYup-blastx &gt;ulog_54.out 2&gt;&amp;1 &amp; 参数解释：|options|explain||:—-|:—-||-query|序列，需要比对的序列文件，fasta格式||-db|database，在上一步中构建时取得名字，常用为nr、nt两个数据库||-outfmt|选择比对输出的类型，有0-14可以选，5是xml文件输出，方便blast2go的进行||-out|输出文件||-num_threads|线程数量，即使用逻辑CPU的数量，可以加快速度|注：还有其他的一些参数，可以进一步查询blast官网文档 2.4 blast结果我还没有跑完出来，不太清楚，但是可以先留个位置，等结果出来后进行展示。 总结 blast基本使用，生信基本工作 构建数据库，这一个要掌握好，以后很有可能要自己搭建数据库，满足个性化需求 结果文件的输出，选择xml，方便接下来进行注释","link":"/2019/12/14/Bioinformatics/blast/"},{"title":"blast2go","text":"前言在转录组的下游分析中，GO和KEGG注释是非常常规的分析，也是基础工作。blast2go这个软件则是一个可视化的工具，可以帮助我们做好这两个分析，不过这也有些可惜，那便是会受制于人，因为这个软件是一个商业软件，所以最好不要过分依赖这种工具，不过这一次分析是初次，而且稍微时间比较紧，所以暂且用一下，如果后续有时间，一定要学习一下如何利用编程，自己完成这些工作。 1. 初识blast2go软件界面介绍如下： 2. 使用blast2go 导入数据：可以在file中点击load file或者打开file，可以自选 选择数据：可以全选数据或者部分 选择工作：选择需要进行的分析，点击运行，但是这里一般是有顺序的，如下： 当逐步分析以后就可以看到一些结果，注意保存数据文件，在结果编辑栏中可以找到导出数据。 3. 分析结果示例 GO结果 KEGG结果 导出数据 4. 总结经过一次使用，blast2go的总体使用体验还是很不错的，确实很方便、很直观，但是确实也有一些限制，自主性不是很高，如果是为了方便快捷得到一个基本结论，那么可以选择使用这个软件，虽然在mapping分析的时候稍微比较慢，但是如果想选择更高的自主性，那么还是建议使用编程，自己分析。","link":"/2019/12/18/Bioinformatics/blast2go/"},{"title":"edgeR基因差异分析","text":"这部分内容还是第一次转录组的实操练习，之前已经完成了序列的拼接、去冗余，并且已经做了层次聚类，统计了每个transcript的raw counts，现在进入了差异基因分析的步骤。因为不管采用什么软件，本质的方法是一致的，因此也具有一定的通用性，或者可以迁移到其他基因分析的项目中，因此要用心学习。 前言这里采用的是R中的edgeR包来进行分析，因此需要先做环境准备，现在开始。 1. 环境准备 安装R，可以到官网download 安装相应包，如下 if (!requireNamespace(“BiocManager”, quietly = TRUE)) install.packages(“BiocManager”)if (!requireNamespace(“edgeR”, quietly = TRUE)) BiocManager::install(“edgeR”)if (!requireNamespace(“airway”, quietly = TRUE)) BiocManager::install(“airway”) 2. 全过程代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354# 导入依赖包library(limma)library(edgeR)library(ggplot2)# 设置工作区setwd('E://Desktop//Transcript')#读取数据，设置分组targets &lt;- data.frame(read.csv('all_counts.csv',header=1,row.names=1))group &lt;- 1:2counts &lt;- targets[,1:2]#构建DGEList对象dgelist &lt;- DGEList(counts = counts, group = group)dgelist#CPM 标准化，过滤低表达countskeep &lt;- rowSums(cpm(dgelist) &gt; 1 ) &gt;= 2 #这里的设置很重要，会很大程度影响结果dgelist &lt;- dgelist[keep, ,keep.lib.sizes = FALSE]#TMM 标准化dgelist_norm &lt;- calcNormFactors(dgelist, method = 'TMM')dgelist_norm#估算离散值design &lt;- model.matrix(~group) #构建分组矩阵#dge &lt;- estimateDisp(dgelist_norm, design, robust = TRUE) #估算离散值dgelist_norm$common.dispersion &lt;- 0.1#差异基因分析#negative binomial generalized log-linear model #拟合fit &lt;- glmFit(dgelist_norm, design, robust = TRUE) #拟合模型lrt &lt;- glmLRT(fit) #统计检验#查看默认方法获得的差异基因dge_de &lt;- decideTestsDGE(lrt, adjust.method = 'fdr', p.value = 0.01) summary(dge_de)# 添加标签，区别up down no三种基因diff_stat &lt;- lrt$tablediff_stat[which(diff_stat$PValue &lt; 0.01 &amp; diff_stat$logFC &gt;= 2),'diff'] &lt;- 'up'diff_stat[which(diff_stat$PValue &lt; 0.01 &amp; diff_stat$logFC &lt;= -2),'diff'] &lt;- 'down'diff_stat[!(diff_stat$diff %in% c('up', 'down')),'diff'] &lt;- 'no'# 查看并保存添加标签后的数据head(diff_stat,n=10)write.csv(diff_stat,file = \"edgeR.csv\")# 做火山图ggplot(diff_stat, aes(x=logFC, y=-log10(diff_stat$PValue))) +geom_point(aes(color = diff), size = 0.5) +scale_colour_manual(limits = c('up', 'down', 'no'), values = c('blue', 'red', 'gray40'), labels = c('Up OTUs', 'Down OTUs', 'No diff OTUs')) +labs(title=\"Groups 13vs15\",x = 'log2 Fold Change', y = '-log10 p-value') 成果示例： 总结 在这个包的分析过程中，可以调整dispersion/pvalue/logFC这3个参数以及CPM 标准化的过滤阈值来调控我们最终获取的差异基因的数量，一般我们能分别获得几千个上下调基因就可以了，可以适当调整参数，方便下一步分析。 在提取出来后，我们只是得到对应的id和它的down或者up标签，还需要把他们从原来的fasta文件中提取出来，这样才能接着做blast分析，这里使用的工具是TBtools这个软件，可以很好的做到这一点，但是建议还是得自己编程摸索一下，避免对别人的软件过渡依赖。 在Linux中还有专门的split命令，可以直接一句命令进行文件拆分，拆分的依据可以是行数，也可以是大小（如10M），还可以是其他标准，这个非常方便，能够快速的进行拆分，然后我们只需要对拆分的文件进行重命名就好了。 在使用TBtools的时候，要注意的是，原始的fasta文件不要把其中的id前面的“&gt;”去掉了，去掉后，这个软件没有办法定位哪里是序列开头，这样子就会导致提取序列出错。","link":"/2019/12/05/Bioinformatics/edgeR基因差异分析/"},{"title":"bash","text":"前言bash是Linux中默认的shell，通过bash及bash提供的工具我们可以很高效的使用Linux完成我们的工作，接下来让我们来学习学习bash。 1. Shell介绍shell是一个程序，可以实现外部与内核的通信功能，通过shell我们可以控制内核完成一些复杂的工作，从而完成我们的任务。Linux中常见的shell有bash/bsh/ash/tsh/zsh/ksh等，其中bash是Linux中默认的shell，这些shell本质上差异不大，只是一些语法的实现上有些区别。因为bash是Linux默认的shell，而且bash确实功能强大，使用也方便，所以接下来我们主要来学习bash，不过shell也和编程类似，可以触类旁通。 2. bash特点 history命令history命令会列出记录的所有bash命令!number：执行.bash_history文件中第n条命令!!：执行上一条命令!command：执行以command开头的最近的一条命令 bash下组合按键ctrl+C：终止目前的命令ctrl+D：输入结束（EOF)ctrl+Z：暂停目前的命令 通配符 *：代表0到无穷多个任意字符?：代表一定有一个任意字符[]: 代表一定有一个在中括号内的字符（非任意字符），例如[abcd]，代表一定有一个字符，可能是a,b,c,d这四个中任意一个[-]：若有减号在中括号内，代表在编码顺序内的所有顺序，例如[0-9]代表0到9之间的所有数字[^]：若中括号内的第一个字符为指数符号^，那表示原向选择，例如[ ^abc]代表一定有一个字符，只要是非a,b,c的其他字符就接受的意思 bash其他符号#：注释符号，后面内容不执行\\：转移符号|：管道符号，分隔两个管道命令的界定～：用户的主文件夹;：连续命令执行分隔符$：变量前导符&amp;：作业控制，将命令变成背景下工作！：逻辑运算符非/：目录符号 ** &gt;,&gt;&gt; **：数据流重定向，分别是“替换”和“累加”,1&gt;表示输出正确信息，2&gt;表示输出错误信息，同时输出则为2&gt;&amp;1&lt;,&lt;&lt;：数据流重定向，输入导向“：两个反单引号中间为先执行命令，也可使用$() 数据管理命令cut：对于比较规整的信息，cut可以截取每一行特定部分的字符grep：对输出结果进行查询筛选sort：对输入内容进行排序uniq：对输入去重的操作wc：对输入进行统计计数tee：同时将数据屏幕输出和保存到文件xargs：将输入通过空格符或者断行符分开作为参数 -的用途：有些命令需要用到文件名作为参数，可以用-代指srdout和stdin输出的文件名awk: 编辑文档，方便列操作paste: 合并文档，可以用’-d’指定分隔符cat: 查看文档，也可以重定向成合并文档 正则表达式 字符集[:alnum:]：代表英文大小写字符及数字，即0-9,A-Z，a-z[:alpha:]：代表任何英文大小写字符，即A-Z,a-z[:blank:]：代表空格键与[Tab]按键[:cntrl:]：代表键盘上面的控制按键，即包括CR，LF，Tab，Del等[:digit:]：代表数字字符，即0-9[:graph:]：除了空格符（空格键与[Tab]按键）外的其他所有按键[:lower:]：代表小写字符，即a-z[:print:]：代表任何可以被打印出来的字符[:punct:]：代表标点符号[:upper:]：代表大写字符，即A-Z[:space:]：任何会产生空白的字符，包括空格键[Tab]CR等[:xdigit:]：代表十六进制的数字类型，因此包括0-9,A-F,a-f等基础字符表达式与应用.：代表一定有一个任意字符：代表重复前一个0到无穷次^word：待查找的字符串在行首word$：待查找的字符串在行尾[list]：从字符集合的RE字符里面找出想要选取的字符[^list]：从字符集合的RE字符里面中找出不要的字符串或范围[n1-n2]：从字符集合的RE字符里面找出想要选取的字符范围[^$]：指定空白行{n1,n2}：连续n到m个的前一个RE字符，若为{n}则是连续n个的前一个RE字符，若为{n,}则是连续n个以上的前一个RE字符*扩展的正则表达式**+：重复一个或者一个以上的前一个RE字符?：零个或者一个的前一个RE字符|：用或（or）的方法找出数个字符串()：找出”组“字符串()+：多个重复组的判别 文本工具 sed命令作用：sed本身是一个管道命令，可以分析stdin，还可以将数据进行替换、删除、新增、选取特定行的功能 语法：sed [n1] [n2] function;function选项如下所示 a:新增，a后面可以接字符串，而这些字符串会在新的一行中出现 c：替换，c的后面可以接字符串，这些字符串可以替换n1,n2之间的行 d：删除，因为是删除，所以d后面通常不接任何参数 i：插入，i的后面可以接字符串，而这些字符串会在新的一行出现 p：打印，也就是将某一个选择的数据打印出来，通常p会与sed -n一起运行 s：替换，可以直接进行替换的工作，通常这个s的动作可以搭配正则表达式，例如1,20s/word1/word2/g,即将1到20行中word1替换成word2 文件比较工具 diff [-bBi] from-file to-file：比较from-file和to-file的差别，注意不要用diff命令去比较不相干的文件，这样没有比较的结果 cmp [-s] file1 file2：比较file1和file2，返回第一个发现的不同点字节处和行数 3. bash理解在翻看了许多资料之后，对于bash的理解又有了新的进步。bash是一个默认的shell，当我们进入Linux系统是，Linux就会给每一个账户分配一个shell，因此，当我们开始进入了以后，就是已经开始在使用bash了，原本我以为是只在调用bash的时候才使用，没想到bash一直都是我工作的环境，正是在这个环境下，我们能够方便的使用上下键查看历史命令、使用tab键补全路径、还有直接使用grep、awk、sed等这些工具进行工作等等，还有数据重定向、命令管道符等等，非常的方便，bash果然是非常实用、好用。 如果需要切换shell的话，也是可以的，可以有如下操作： 方法一: chsh -s /bin/ksh 方法二: usermod -s /bin/ksh root 查看当前shell：echo $SHELL或者egrep ‘root’ /etc/passwd 查看所有shell：cat /etc/shells或者chsh -l","link":"/2019/12/17/Linux/bash/"},{"title":"linux_shell杂谈","text":"概述第一次听说的bash时是一脸懵逼的，不明白此为何物，最近看了一下相关书籍，捋了捋关于shell/bash/linux/GNU之间的关系，也加深了对于操作系统的理解，顺便还了解了一下shell编程的一些知识，接下来我们按顺序聊聊。 GNU与Linux在谈论Linux的时候，就不得不提到GUN，虽然现在大家一般把某些操作系统称之为Linux，但是实际上，更准确的称呼应该是GUN/Linux操作系统。这是为什么呢？这就得从上世纪90年代说起了。 在上世纪90年代，随着计算机技术的发展，计算机软件也日新月异，但是随之而来的是计算机软件的商业化浪潮，Microsoft就是这个浪潮上的领军企业，在这个浪潮中Microsoft的windows系统逐渐奠定了世界PC操作系统之王的地位。在这个时候，有这么一批人，他们不愿意看到商业化软件限制了大家灵活使用计算机的自由，因此，他们提出来一个计划，这就是自由软件计划，其中GNU就是一个集大成者。 GNU工程的初心就是打造一个完全由自由软件组成的操作系统，GNU的名字全称是——GNU’s Not Unix。在GNU工程的计划里，这个操作系统包括了文件管理系统、内存管理系统、邮箱、游戏等方面，经过几年的工作，GNU工程完成了几乎所有除内核以外的其他软件的收集或编写，一个GNU系统即将诞生，但是在写操作内核的时候，GNU团队遇到了一定的困难，这个内核非常难写，但是与此同时，Linus写出来一个操作内核Linux，这个核心很符合GNU工程的需求，因此GNU工程的工程师们决定把Linux适配到GNU中。这个过程并不容易，但是经过大家的努力，最终还是完成了这个任务，GNU系统正式发布，由于Linux已经广为人知，因此在这个系统出来后，大家还是习惯性的称之为Linux操作系统，这也是个历史原因，导致现在大家也都称之为Linux操作系统，但是实际上，Linux只是GNU系统的内核，我们不会因为一个内核就把整个操作系统称之为LINUX，就像联想电脑不会因为用了Intel的CPU就叫做Intel电脑。不过对于普通大众来说，这已经不重要了，大家只是习惯性的称之为Linux系统，但是对于每一个学习计算机的人来说，把这个系统称为GNU/LINUX是对当初所有工程师的尊重，是对他们的致敬。现在的所有的Linux发行版，实际上是GNU/LINUX发行版，例如Ubuntu、CentOS、Debian等一系类发行版。所有这些发行版就像是不同的手机主题，本质上的核心及框架都是一样的，只是为了某些领域做了适应性的配置。 Linux与shell在一个操作系统中，操作内核是完成工作的基础，所有工作都必须经过核心来完成，所以我们要使用计算机完成特定的任务时，我们必须通过操作内核来完成。但是，内核是复杂而脆弱的，稍有不慎，就可能会导致系统崩溃，为了避免这个问题的出现，工程师们设计了一个和内核交互的窗口，通过这个窗口可以和内核通信，把我们的指令传递给内核，这个窗口就称之为shell。因为在整个操作系统中，这个窗口是在计算机核心外部的，就像是计算机的最外层，所以也就生动形象的称为Shell——外壳。 shell与bash在Linux中，默认的一个通信窗口即shell是bash，但是其实，在Linux中有着很多的可以使用的shell，这也是在上世纪90年代到21世纪之初的时候，在计算机中爆发出来的大量的shell，当时可能盛行很多，其中比如有ksh、sh等。其实bash是sh的增强版，功能非常强大，所以要想真正的灵活使用操作系统，了解一个shell很重要。 那么在Linux中，学习bash就是顺理成章的。接下来就是就是bash的基础学习。见下一篇！","link":"/2019/12/09/Linux/linux-shell杂谈/"},{"title":"pipe-Linux","text":"使用管道操作符“|”可以把一个命令的标准输出传送到另一个命令的标准输入中，连续的|意味着第一个命令的输出为第二个命令的输入，第二个命令的输入为第一个命令的输出，依次类推。 如下examples ps -ef|less ps 用于查看进程，less用于分页显示，这个命令是表示把ps的进程按分页展示 find . -name “*.bam”|xargs rm -rfv 首先查询在当前目录下匹配得上尾缀为.bam的文件，然后通过管道符传递给xargs中的命令rm，这样就可以批量删除，因为rm默认是不支持正则匹配的，所以可以通过这种方式来做到批量删除，同样，采用元字符也能达到一定的目的，如下 1rm -rf test-{1..100}.txt #可以删除test-1.txt test-2.txt .... test-100.txt等一系列文件，这也是批量删除。 补充xargs 是给命令传递参数的一个过滤器，也是组合多个命令的一个工具。xargs 可以将管道或标准输入（stdin）数据转换成命令行参数，也能够从文件的输出中读取数据。xargs 也可以将单行或多行文本输入转换为其他格式，例如多行变单行，单行变多行。xargs 默认的命令是 echo，这意味着通过管道传递给 xargs 的输入将会包含换行和空白，不过通过 xargs 的处理，换行和空白将被空格取代。xargs 是一个强有力的命令，它能够捕获一个命令的输出，然后传递给另外一个命令。之所以能用到这个命令，关键是由于很多命令不支持|管道来传递参数，而日常工作中有有这个必要，所以就有了 xargs 命令，例如： 12find /sbin -perm +700 |ls -l #这个命令是错误的find /sbin -perm +700 |xargs ls -l #这样才是正确的","link":"/2019/11/09/Linux/pipe/"},{"title":"symbolic-hard_link","text":"在Linux的应用当中，掌握软硬链接的方法很重要，学会软硬链接可以让我们更方便的使用Linux系统中的文件，特别是以项目的方式去管理文件时，可以在单个项目之内创建软硬链接，而在其他的项目当中也可以创建同样的软硬链接指向同样的脚本、软件等，这样子可以实现文件个高效复用，既能便捷实用，又能避免重复造轮子。 1. 硬链接 hard link创建方法： 1ln -d dir1/source_file dir2/hard-link_file 理解： 在Linux系统之中，文件储存在某一些储存空间上，删除文件的方式其实就是把对应的索引链接都删除就可以了，那么这一片空间就没有索引来访问了，那么这样就相当于是删除了文件，下次写文件的时候就会新建对应的索引链接，然后直接开始写入文件到储存空间，实现覆盖。 对于硬链接来说，就是创建一个新的索引，和原来文件的索引同级（原文件索引即原文件的名字），相当于是给文件创建了一个新的名字，但是没有创建新的文件，当原文件的索引被删除时，不会影响到硬链接，仍然可以通过硬链接访问文件，并且进行编辑。 如果需要彻底删除文件，那就意味着必须把所有的硬链接都删除掉，这样才是真正的删除，系统文件才无法找到这片空间中的原文件。 硬链接有点像是到一座小岛上的另一座桥，和原来通往这座岛的桥一样，都连接着同样的一个小岛，所以并没有创建新的文件，但是却多了一个保障，一旦原来的桥塌了（误删），仍然可以通过这座桥登上小岛。而不管通过哪座桥登上小岛进行建设规划，都会直接改动小岛真实的面目。 2. 软连接 symbolic link创建方法： 1ln -s dir1/souce_file dir2/symbolic_link 理解 软连接是真正的快捷方式既视感。软连接是一个文件，这个文件的内容是原文件的储存路径之类的，但是也只有这些，并没有原文件的具体内容，所以只是一个快捷方式，通过这个快捷方式可以快速的访问到文件，并进行使用。 因此，当在删除了原文件后，软连接就失效了，无法再访问原文件。 同样是用小岛来打比方，软连接就像是路标，指明了哪里有桥可以上岛，对于具体的小岛来说，软连接就指向了原来那座桥所在的方位，所以可以很快的找到桥然后上岛。所以拆掉路标不会影响桥的存在，但是一旦把桥拆了，那么路标也就没有用了，小岛已经再也上不去了，相当于在世界上消失了。 3. 软硬链接的区别 硬链接 hard link 软链接 symbolic link 不能给目录创建硬链接 可以给目录创建软链接 不能跨文件系统创建链接 可以跨文件系统创建软链接 文件的新索引，和原文件名互为别名 新文件，保存原文件路径，相当于快捷方式 不能给不存在的文件创建硬链接，因为硬链接存在即表示原文件存在，而不存在的文件就表示没有文件，也就没有链接可言。 可以给不存在的文件创建链接，因为路标可以随便建，有没有效无所谓，反正可以建。 4. ln 命令使用方法：” ln [options] 目标 “ 参数如下 -b 删除，覆盖以前建立的链接 -d 允许超级用户制作目录的硬链接-f 强制执行-i 交互模式，文件存在则提示用户是否覆盖-n 把符号链接视为一般目录-s 软链接(符号链接)-v 显示详细的处理过程 5. 实践 硬链接 hard link 软链接 symbolic link","link":"/2019/11/03/Linux/symbolic-hard-link/"},{"title":"working_with_files","text":"在Linux中，有着处理文本的三大常用的、非常强大的工具——awk/grep/sed，其中awk是报告生成工具，适合处理列数据；grep是文本过滤搜索工具；sed是文本编辑工具，完成一般的编辑工作。接下来我将学习如何使用这3个工具。PS：这三个工具操作的文本都可以通过重定向的方式输出到新文件中保存下来。 1. 预备内容 在正式学习这三个工具之前，先来了解一下Linux中的正则表达式。 在Linux中可以通过正则的方式对文本进行匹配处理，这就相当于是一个通用模板，awk、grep和sed都可以识别这套模板，相当于正则表达式可以成为这3个工具的拓展模板，通过结合正则，让这3个工具更加强大。 1.1. 正则的组成 元字符 功能 意思 ^ 匹配行首 表示以某个字符开头 $ 匹配行尾 表示以某个字符结尾 ^$ 空行的意思 表示空行的意思 . 匹配任意单个字符 表示任意一个字符 * 字符* 匹配0或多个此字符 示重复的任意多个字 |屏蔽一个元字符的特殊含义 表示去掉有意义的元字符的含 [] 匹配中括号内的字符 表示过滤括号内的字符 .* 代表任意多个字符 就是代表任意多个字符 以上就是常见的元字符，正则还有一些其他字符，如下图 2. awk使用使用文档如下： 3. grep使用使用文档如下： 4. sed使用使用文档如下：","link":"/2019/10/31/Linux/working-with-files/"},{"title":"Machine-learning mind map","text":"最近总是会忘记一些概念，对于机器学习的框架有些混淆模糊了，所以做了一个思维导图帮助理清框架，以期学习时有的放矢。 1. 什么是机器学习？ 机器学习是通过编程让计算机从数据中进行学习的科学和艺术。充分利用这些技术可以极大的提高人类生产的效率，是新一代的技术革命。 2. 为什么要用机器学习方法呢？ 原因如下： 需要进行大量手工调整或需要拥有长串规则才能解决的问题：机器学习算法通常可以简化代码、提高性能。 问题复杂，传统方法难以解决：最好的机器学习方法可以找到解决方案。 环境有波动：机器学习算法可以适应新数据。 洞察复杂问题和大量数据 3. 机器学习知识框架 知识框架 算法框架","link":"/2019/11/18/Machinelearning/Machine-learning/"},{"title":"ANOVA","text":"前言方差分析（analysis of variance）是在概率与统计学中很重要的一部分内容，方差分析用于检验多对数据的平均数差之间的显著性，如果是采用t检验的话，只能每次对2组数据进行检验，对于n组数据则需要进行2的排列组合次，并且即使每次的置信度为95%，累乘起来的置信度也不高，即容易犯错，所以需要新方法，方差分析就是解决这个问题的方法。在这一篇博客中我们只讨论单因素方差分析。 1. 基本概念 概念 理解 因素 在实验设计中，控制某个变量，如温度、光照、反应物浓度等等，这就是某个因素 水平 在因素的基础上，进一步选择一些度量，比如具体的温度梯度，每一个梯度就是一个水平 注意：在水平下还有多次重复样本，同一个水平中的方差称为组内方差，不同水平之间的方差称为组间方差，这是在稍后进行方差分解计算的基础。 2. 方差分析基本流程 收集所有待处理数据，提出原假设 计算数据的各项参数，进行方差分解 进行自由度分解 计算均方差，检验随机误差的方差与处理效应的方差的显著性 根据F检验判断是否具有显著性，得出结论 3. 两种效应计算 在处理效应中，具有固定效应和随机效应，固定效应指的的在实际的因素中指定特定的几个水平进行计算，得出的结论只适用于这几个水平，不能推广到所有水平，此时这个因素也可称为固定因素，相应的处理效应为固定效应；随机效应指的是把所有水平当做一个总体分布，从中随机抽取一些水平进行计算，此时这个因素也可以称为随机因素，相应的效应为随机效应，得到的结论可以推广到所有水平中使用。 符号说明： $$y_{i}.=\\sum_{j=1}^ny_{ij}$$ $$\\bar y_i.=\\frac1ny_i.\\ (i=1,2,3,…,a)$$ $$y..=\\sum_{i=1}^a\\sum_{j=1}^ny_{ij},\\ \\bar y..=\\frac1{an}y..$$ 3.1. 固定效应","link":"/2019/11/15/Mathematics/ANOVA/"},{"title":"linear-algebra","text":"I am going to share some experiences in the process that I study the linear Algebra, it is really excited that I just find a super nice textbook named Linear Algebra and the Applications. I really recommend it if you are going to study the linear algebra especially when you are mindless in which book that you should read. Life tips: Never judge others by your value, learn to accept the diversity, this is what the world looks like. Respect each other! Chapter 1 线性方程组content: 1.1. 线性方程组 1.2. 行化简与阶梯型矩阵 1.3. 向量方程 1.4. 矩阵方程 Ax=b 1.5. 线性方程组的解集 1.6. 线性方程组的应用 1.7. 线性无关 1.8. 线性变换介绍 1.9. 线性变换的矩阵 1.10. 商业、科学和工程中的线性模型","link":"/2019/11/07/Mathematics/linear-algebra/"},{"title":"概率统计","text":"概率论与数理统计是一门很重要的基础数学课，现在的机器学习的基础之一。现在这里添加的是这门学科的总览图，后期可能会做进一步的详细笔记。 总结概率论与数理统计这门学科的内容是具有继承性的。首先先讨论概率论，概率论的发展成为了数理统计的基础，特别是当大数定律和中心极限定理提出以后，数理统计变得有理论基础，开始快速发展，而在数理统计发展起来后，在应用方面，除了假设检验，方差分析和回归分析都是很重要的应用，对于我们建立模型有很大的帮助，是我们探寻变量关系的基础。","link":"/2019/11/04/Mathematics/概率统计/"},{"title":"nlp-2-数据读取与数据分析","text":"前言昨天我们了解了比赛的基本内容以及一些思路，下载了比赛的数据，今天我们就来探索一下我们现有的数据。 1. 数据概况通过网站的链接，我们下载到了三个数据——train_set/test_a/test_a_sample这三个数据的csv文件，接下来我们来看看这三个数据长啥样。 1234567891011# 导入工具包import pandas as pdtrain_set = pd.read_csv('./train_set.csv', sep='\\t', encoding='utf-8')# 查看基本统计结果train_set.describe()# 查看具体信息train_set.info# 查看前5行数据train_set.head(5) 结果如下： 2. 简单探索 文本长度统计12train_set['text_len'] = train_set['text'].apply(lambda x: len(x.split(' ')))print(train_set['text_len'].describe()) 文本长度直方图123456%matplotlib inlineimport matplotlib.pyplot as plt _ = plt.hist(train_set['text_len'], bins=200)plt.xlabel('Text char count')plt.title(\"Histogram of char count\") 类别统计123train_set['label'].value_counts().plot(kind='bar')plt.title('News class count')plt.xlabel(\"category\") 字符统计123456789101112131415from collections import Counterall_lines = ' '.join(list(train_set['text']))word_count = Counter(all_lines.split(\" \"))word_count = sorted(word_count.items(), key=lambda d:d[1], reverse = True)print(len(word_count))print(word_count[0])print(word_count[-1])# 输出结果2405('3750', 3702)('5034', 1) 12345678910111213141516from collections import Countertrain_set['text_unique'] = train_set['text'].apply(lambda x: ' '.join(list(set(x.split(' ')))))all_lines = ' '.join(list(train_set['text_unique']))word_count = Counter(all_lines.split(\" \"))word_count = sorted(word_count.items(), key=lambda d:int(d[1]), reverse = True)print(word_count[0])print(word_count[1])print(word_count[2])# 输出结果('3750', 99)('900', 99)('648', 96) 数据分析的结论通过上述分析我们可以得出以下结论： 赛题中每个新闻包含的字符个数平均为1000个，还有一些新闻字符较长； 赛题中新闻类别分布不均匀，科技类新闻样本量接近4w，星座类新闻样本量不到1k； 赛题总共包括7000-8000个字符； 通过数据分析，我们还可以得出以下结论： 每个新闻平均字符个数较多，可能需要截断； 由于类别不均衡，会严重影响模型的精度；","link":"/2020/07/22/NLP/nlp-2-数据读取与数据分析/"},{"title":"nlp-3-基于机器学习的文本分类","text":"1. 前言文本型数据的读取与训练与图片等其他格式较为一致的数据不同，文本数据一般不定长，所以如果要进行机器学习的矩阵训练，需要先对文本数据进行归一化处理，把文本转换成可以进行运算的shape相同的向量，然后输入算法进行学习。转换的方法有几种，下面的文本表示方法引用自Datawhale零基础入门NLP赛事 - Task3 基于机器学习的文本分类。 2. 文本表示方法 Part1在机器学习算法的训练过程中，假设给定$N$个样本，每个样本有$M$个特征，这样组成了$N×M$的样本矩阵，然后完成算法的训练和预测。同样的在计算机视觉中可以将图片的像素看作特征，每张图片看作hight×width×3的特征图，一个三维的矩阵来进入计算机进行计算。 但是在自然语言领域，上述方法却不可行：文本是不定长度的。文本表示成计算机能够运算的数字或向量的方法一般称为词嵌入（Word Embedding）方法。词嵌入将不定长的文本转换到定长的空间内，是文本分类的第一步。 One-hot这里的One-hot与数据挖掘任务中的操作是一致的，即将每一个单词使用一个离散的向量表示。具体将每个字/词编码一个索引，然后根据索引进行赋值。 One-hot表示方法的例子如下： 12句子1：我 爱 北 京 天 安 门句子2：我 喜 欢 上 海 首先对所有句子的字进行索引，即将每个字确定一个编号： 1234{ '我': 1, '爱': 2, '北': 3, '京': 4, '天': 5, '安': 6, '门': 7, '喜': 8, '欢': 9, '上': 10, '海': 11} 在这里共包括11个字，因此每个字可以转换为一个11维度稀疏向量： 1234我：[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]爱：[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]...海：[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1] Bag of WordsBag of Words（词袋表示），也称为Count Vectors，每个文档的字/词可以使用其出现次数来进行表示。 12句子1：我 爱 北 京 天 安 门句子2：我 喜 欢 上 海 直接统计每个字出现的次数，并进行赋值： 12345句子1：我 爱 北 京 天 安 门转换为 [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]句子2：我 喜 欢 上 海转换为 [1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1] 在sklearn中可以直接CountVectorizer来实现这一步骤： 123456789from sklearn.feature_extraction.text import CountVectorizercorpus = [ 'This is the first document.', 'This document is the second document.', 'And this is the third one.', 'Is this the first document?',]vectorizer = CountVectorizer()vectorizer.fit_transform(corpus).toarray() N-gramN-gram与Count Vectors类似，不过加入了相邻单词组合成为新的单词，并进行计数。 如果N取值为2，则句子1和句子2就变为： 12句子1：我爱 爱北 北京 京天 天安 安门句子2：我喜 喜欢 欢上 上海 TF-IDFTF-IDF 分数由两部分组成：第一部分是词语频率（Term Frequency），第二部分是逆文档频率（Inverse Document Frequency）。其中计算语料库中文档总数除以含有该词语的文档数量，然后再取对数就是逆文档频率。 12TF(t)= 该词语在当前文档出现的次数 / 当前文档中词语的总数IDF(t)= log_e（文档总数 / 出现该词语的文档总数） 3. 算法实现 The first 123456789101112131415161718192021# Count Vectors + RidgeClassifierimport pandas as pdfrom sklearn.feature_extraction.text import CountVectorizerfrom sklearn.linear_model import RidgeClassifierfrom sklearn.metrics import f1_scoretrain_df = pd.read_csv('./train_set.csv', sep='\\t', nrows=15000)vectorizer = CountVectorizer(max_features=3000)train_test = vectorizer.fit_transform(train_df['text'])clf = RidgeClassifier()clf.fit(train_test[:10000], train_df['label'].values[:10000])val_pred = clf.predict(train_test[10000:])print(f1_score(train_df['label'].values[10000:], val_pred, average='macro'))#结果输出# 0.65441877581244 The second 123456789101112131415161718192021# TF-IDF + RidgeClassifierimport pandas as pdfrom sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.linear_model import RidgeClassifierfrom sklearn.metrics import f1_scoretrain_df = pd.read_csv('./train_set.csv', sep='\\t', nrows=15000)tfidf = TfidfVectorizer(ngram_range=(1,3), max_features=3000)train_test = tfidf.fit_transform(train_df['text'])clf = RidgeClassifier()clf.fit(train_test[:10000], train_df['label'].values[:10000])val_pred = clf.predict(train_test[10000:])print(f1_score(train_df['label'].values[10000:], val_pred, average='macro'))#结果输出# 0.8719098297954606 4. 补充 f1_score是什么？f1_score就是模型的准确率和召回率的调和平均数。精确率和召回率是对于分类任务来说的。精确率就是预测正确的结果中，有多少是真正正确的，也就是预测正确的样本中，有多少是正确的对上你认为正确的样本数；召回率(将正类样本预测成正类样本的个数对上全部真正正确的样本的比例)。 举个栗子 比如我们有一群外出的鸭子，晚上没有回家，全部都跑到别人家里和别人家的鸭子混在了一起，现在我们要从一大群鸭子里面找出我们家的鸭子，这群鸭子中，有我们的也有别人的，总共鸭子有200只，我们从里面找出来一群鸭子假设为(100只)，以为全部都是我们家的，剩下的100只是别人家的，然后仔细看了一下，里面只有80只，是我们家的，其他都是别人家的，这样我们的准确率就只有80/100也就是0.8，这个时候我爸来说我们家总共有120个鸭，这样我们的召回率就只有80/120也就是0.67左右了，我们列个表格看一下","link":"/2020/07/23/NLP/nlp-3-基于机器学习的文本分类/"},{"title":"nlp-4-基于深度学习的文本分类","text":"前言与传统机器学习不同，深度学习既提供特征提取功能，也可以完成分类的功能。从本章开始我们将学习如何使用深度学习来完成文本表示。 文本表示方法 Part2在上一章节，我们介绍几种文本表示方法： One-hot Bag of Words N-gram TF-IDF 也通过sklean进行了相应的实践，有了初步的认知。但上述方法都或多或少存在一定的问题：转换得到的向量维度很高，需要较长的训练实践；没有考虑单词与单词之间的关系，只是进行了统计。 与这些表示方法不同，深度学习也可以用于文本表示，还可以将其映射到一个低纬空间。其中比较典型的例子有：FastText、Word2Vec和Bert。在本章我们将介绍FastText，将在后面的内容介绍Word2Vec和Bert。 FastTextFastText是一种典型的深度学习词向量的表示方法，它非常简单，通过Embedding层将单词映射到稠密空间，然后将句子中所有的单词在Embedding空间中进行平均，进而完成分类操作。所以FastText是一个三层的神经网络，输入层、隐含层和输出层。 基于FastText的文本分类FastText可以快速的在CPU上进行训练，最好的实践方法就是官方开源的版本：https://github.com/facebookresearch/fastText/tree/master/python 安装fasttext pip安装 1pip install fasttext 源码安装 123git clone https://github.com/facebookresearch/fastText.gitcd fastTextsudo pip install . 在Windows上安装fasttext时会出现一定的报错，这个时候解决比较繁琐，于是干脆采用第三方python库手动下载好fasttext的wheels文件，手动安装就好。进入python第三方库，按ctrl+f查找我们要的package（即fasttext），然后选择和自己电脑合适的版本，点击下载，下载完毕后，进入下载目录，命令行运行 pip install ./&lt;fasttext_version_file&gt;.wheel 代码构建1234567891011121314import pandas as pdfrom sklearn.metrics import f1_score# 转换为FastText需要的格式train_df = pd.read_csv('./train_set.csv', sep='\\t', nrows=15000)train_df['label_ft'] = '__label__' + train_df['label'].astype(str)train_df[['text','label_ft']].iloc[:-2500].to_csv('train.csv', index=None, header=None, sep='\\t')import fasttextmodel = fasttext.train_supervised('./train_set.csv', lr=1.0, wordNgrams=2, verbose=2, minCount=1, epoch=25, loss=\"hs\")val_pred = [model.predict(x)[0][0].split('__')[-1] for x in train_df.iloc[-2500:]['text']]print(f1_score(train_df['label'].values[-2500:].astype(str), val_pred, average='macro')) 调参提升在这个模型当中存在一些超参数，合理的调整这些参数可以让我们的模型更加准确，但是该怎么调整这些参数呢？可以采用验证集进行调参。 通过阅读文档，要弄清楚这些参数的大致含义，那些参数会增加模型的复杂度 通过在验证集上进行验证模型精度，找到模型在是否过拟合还是欠拟合 首先对数据集进行分组，接下来进行交叉验证，选择最后一次验证的验证集的结果来配合调整参数，采用反馈调节——先调整参数，然后观察验证结果，根据结果进行下一次调整。 总结fasttext最大的有点就是快，相较于传统的机器学习算法，fasttext映射出来的向量维度更低，有着更快的训练速度，同时在加大训练量时，也能够达到不错的训练结果，能够适应一定场景下的应用。后面还会接着学习word2vec和bert模型，很期待这两种方法能够带来怎样的惊喜。","link":"/2020/07/24/NLP/nlp-4-基于深度学习的文本分类/"},{"title":"nlp-5-基于深度学习的文本分类2","text":"前言昨天我们采用了fasttext进行文本处理分类，今天我们使用word2vec模型进行训练。 什么是word2vec模型？word2vec工具主要包含两个模型：跳字模型（skip-gram）和连续词袋模型（continuous bag of words，简称CBOW），以及两种高效训练的方法：负采样（negative sampling）和层序softmax（hierarchical softmax）。值得一提的是，word2vec词向量可以较好地表达不同词之间的相似和类比关系。 自然语言是一套用来表达含义的复杂系统。在这套系统中，词是表义的基本单元。在机器学习中，如何使用向量表示词？顾名思义，词向量是用来表示词的向量，通常也被认为是词的特征向量。近年来，词向量已逐渐成为自然语言处理的基础知识。 NLP（自然语言处理）里面，最细粒度的是 词语，词语组成句子，句子再组成段落、篇章、文档。所以处理 NLP 的问题，首先就要拿词语开刀。词语，是人类的抽象总结，是符号形式的（比如中文、英文、拉丁文等等），所以需要把他们转换成数值形式，或者说——嵌入到一个数学空间里，这种嵌入方式，就叫词嵌入（word embedding)，而 Word2vec，就是词嵌入（ word embedding) 的一种。简单点来说就是把一个词语转换成对应向量的表达形式，来让机器读取数据。 最后强调一下，word2vec一个NLP工具，它可以将所有的词向量化，这样词与词之间就可以定量的去度量他们之间的关系，挖掘词之间的联系。word2vec主要包含两个模型Skip-gram和CBOW。以及两种高效的训练方法负采样，层序softmax。引用博客来源 搭建word2vec模型123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141## 1import loggingimport randomimport numpy as npimport torchlogging.basicConfig(level=logging.INFO, format='%(asctime)-15s %(levelname)s: %(message)s')# set seedseed = 666random.seed(seed)np.random.seed(seed)torch.cuda.manual_seed(seed)torch.manual_seed(seed)## 2# split data to 10 foldfold_num = 10data_file = './train_set.csv'import pandas as pddef all_data2fold(fold_num, num=10000): fold_data = [] f = pd.read_csv(data_file, sep='\\t', encoding='UTF-8') texts = f['text'].tolist()[:num] labels = f['label'].tolist()[:num] total = len(labels) index = list(range(total)) np.random.shuffle(index) all_texts = [] all_labels = [] for i in index: all_texts.append(texts[i]) all_labels.append(labels[i]) label2id = {} for i in range(total): label = str(all_labels[i]) if label not in label2id: label2id[label] = [i] else: label2id[label].append(i) all_index = [[] for _ in range(fold_num)] for label, data in label2id.items(): # print(label, len(data)) batch_size = int(len(data) / fold_num) other = len(data) - batch_size * fold_num for i in range(fold_num): cur_batch_size = batch_size + 1 if i &lt; other else batch_size # print(cur_batch_size) batch_data = [data[i * batch_size + b] for b in range(cur_batch_size)] all_index[i].extend(batch_data) batch_size = int(total / fold_num) other_texts = [] other_labels = [] other_num = 0 start = 0 for fold in range(fold_num): num = len(all_index[fold]) texts = [all_texts[i] for i in all_index[fold]] labels = [all_labels[i] for i in all_index[fold]] if num &gt; batch_size: fold_texts = texts[:batch_size] other_texts.extend(texts[batch_size:]) fold_labels = labels[:batch_size] other_labels.extend(labels[batch_size:]) other_num += num - batch_size elif num &lt; batch_size: end = start + batch_size - num fold_texts = texts + other_texts[start: end] fold_labels = labels + other_labels[start: end] start = end else: fold_texts = texts fold_labels = labels assert batch_size == len(fold_labels) # shuffle index = list(range(batch_size)) np.random.shuffle(index) shuffle_fold_texts = [] shuffle_fold_labels = [] for i in index: shuffle_fold_texts.append(fold_texts[i]) shuffle_fold_labels.append(fold_labels[i]) data = {'label': shuffle_fold_labels, 'text': shuffle_fold_texts} fold_data.append(data) logging.info(\"Fold lens %s\", str([len(data['label']) for data in fold_data])) return fold_datafold_data = all_data2fold(10)## 3# build train data for word2vecfold_id = 9train_texts = []for i in range(0, fold_id): data = fold_data[i] train_texts.extend(data['text']) logging.info('Total %d docs.' % len(train_texts))## 4logging.info('Start training...')from gensim.models.word2vec import Word2Vecnum_features = 100 # Word vector dimensionalitynum_workers = 8 # Number of threads to run in paralleltrain_texts = list(map(lambda x: list(x.split()), train_texts))model = Word2Vec(train_texts, workers=num_workers, size=num_features)model.init_sims(replace=True)# save modelmodel.save(\"./word2vec.bin\")## 5# load modelmodel = Word2Vec.load(\"./word2vec.bin\")# convert formatmodel.wv.save_word2vec_format('./word2vec.txt', binary=False) 结语现在虽然明白了word2vec的工作原理，但是亲手实践这个模型的能力还是略为欠缺，需要仔细研究一下源码，好好学习一下。另外，这个模型非常大，一般电脑没有办法训练这个网络，因为权重矩阵太大，动不动就是几百上千万的参数，所以需要很长的训练时间，不建议使用个人电脑来运行这个模型，可以考虑在服务器中运行此模型。","link":"/2020/07/25/NLP/nlp-5-基于深度学习的文本分类2/"},{"title":"nlp_天池_01","text":"一、前言 NLP是机器学习当中的掌上明珠，只有通过nlp才可能真正的达到“知情达意”的AI。这是一个从零开始学习NLP的项目，通过对项目的探索来达到了解、掌握基本nlp知识的目的。此项目来源于阿里云天池竞赛中的一个新闻文本分类的nlp赛题。 二、什么是NLPNLP全称是nature language process，也就是自然语言处理。在理解自然语言处理之前，我们需要先知道什么是自然语言。自然语言是人类在进化过程中的智慧的结晶，通过最原始的不同声音表达一定的含义，到提炼出音节，再构成词，最终连成句，这样的一个发展的过程看似简单，实则是经历了漫长的人类文明变迁史的淘洗，洗尽铅华，终于得到了“语言”这一瑰宝。现存的人类语言大概有1900多种，人类历史进程中学习到的知识超过80%通过文字语言记录下来。 几个概念 语音学：语音学（Phonetics）是一种非常基本的理论，一个正常的人类，有着相同的人体器官和相同的发声结构，就会遵循着相同的发声规则和原理。 语音体系：语音体系（Phonology）是有语义的声音的合集，不同的文明都有着各自的语音体系。 音素：音素（Phoneme）是语音中划分出来的最小语音单位，分为元音和辅音。 整体来理解就是，人类的语言通过音素进行构建，搭建语音体系，形成语言交流系统，再通过符号化演变成文字，这样人类的语言就诞生了。人类的语言对于人类的发展来说意义重大，这是帮助人类文明不断演化的最大助力。到了信息时代，怎么让计算机来理解人类的语言文字就成了一个非常自然的课题，如果能够做到这一点，那么人类和计算机之间的交互融合将达到质的飞跃，人类文明很可能进入下一个时代。因此，现在这方面的研究都意义非凡，现在就让我们开始来进行学习吧。 三、nlp包括了什么NLP是人工智能的一个子领域，在这个领域里，研究又分成了很多具体的方向，每个具体的研究方向又有着许多的研究课题及成果，下图就大概罗列了一下nlp包括的研究方向。 四、天池新闻文本分类比赛1. 赛题 赛题以自然语言处理为背景，要求选手根据新闻文本字符对新闻的类别进行分类，这是一个经典文本分类问题。通过这道赛题可以引导大家走入自然语言处理的世界，带大家接触NLP的预处理、模型构建和模型训练等知识点。 赛题名称：零基础入门NLP之新闻文本分类 赛题目标：通过这道赛题可以引导大家走入自然语言处理的世界，带大家接触NLP的预处理、模型构建和模型训练等知识点。 赛题任务：赛题以自然语言处理为背景，要求选手对新闻文本进行分类，这是一个典型的字符识别问题。 2. 赛题难点 本题原本是一道普通的文本分类的题目，比较常见于日常生活，但是在此次比赛中，为了避免一些其他情况出现，数据被进行了匿名化处理，统一转换成了字符，即不可所见即所得，因此需要先对当前的数据进行一定的了解，之后再通过一些独特的建模方式来达到分类的目的。 3. 解题思路如何进行文本的分类呢？分类的基础是不同的文本在使用的词汇、句式等特征上存在差异，而这些差异就是区分不同文本的依据，那我们做的第一步就是要选取有效特征，并且将这些特征量化表示，用来评价不同的文本，从而达到分类的目的。在这个比赛的过程中，有个难点就是文本数据进行了匿名化处理，也就是进行了转码，我们能够看到的数据并不是我们常规认识的数据，因此我们没有办法进行简单的分词、统计等等，这给我们提取特征造成了困扰。但是，其实对于计算机来说，不同的字符表达是等效的，只要采用的是统一的文本字符映射方法，那么在原文本中存在的特征，在新数据当中也存在，只不过表现形式可能不太相同，因此，我们可以通过相同的方式来进行特征提取，最终用这些特征来标记不同的文本，从而搭建分类模型进行分类。 官方提供了一些基本的解题思路，如下： 思路1：TF-IDF + 机器学习分类器 直接使用TF-IDF对文本提取特征，并使用分类器进行分类。在分类器的选择上，可以使用SVM、LR、或者XGBoost。 思路2：FastText FastText是入门款的词向量，利用Facebook提供的FastText工具，可以快速构建出分类器。 思路3：WordVec + 深度学习分类器 WordVec是进阶款的词向量，并通过构建深度学习分类完成分类。深度学习分类的网络结构可以选择TextCNN、TextRNN或者BiLSTM。 思路4：Bert词向量 Bert是高配款的词向量，具有强大的建模学习能力。 实践上一部分中阐述的解题思路都可以进行尝试，在尝试的过程中要尝试去理解算法的含义，不能简单的调包了事，这样才能真正的有所学。实践将在后面的系列博客中逐一记录。","link":"/2020/07/21/NLP/nlp-天池-01/"},{"title":"Poetry Of Life","text":"这是生活中最甜美的一首诗！ 生活中遇到的每个人都是一部独一无二的书，每一本书都需要真心坦诚的读才可能会有所感悟。我的生活也是一本书，在我的书里，我希望她会是一首优美、独特的诗篇，在我读到她的诗句时，便能觉得生活是甜的！","link":"/2019/08/14/Poetry with Life/Lovelygirl/"},{"title":"从前慢","text":"从前慢—-木心记得早些少年时大家诚诚恳恳说一句 是一句 清早上火车站长街黑暗无行人卖豆浆的小店冒着热气 从前的日色变得慢车，马，邮件都慢一生只够爱一个人 从前的锁也好看钥匙精美有样子你锁了 人家就懂了","link":"/2019/07/28/Poetry with Life/从前慢/"},{"title":"以梦为马","text":"——海子从明天起，做一个幸福的人喂马、劈柴，周游世界 从明天起，关心粮食和蔬菜我有一所房子，面朝大海，春暖花开 从明天起，和每一个亲人通信 告诉他们我的幸福 那幸福的闪电告诉我的我将告诉每一个人 给每一条河每一座山取一个温暖的名字陌生人，我也为你祝福 愿你有一个灿烂的前程 愿你有情人终成眷属 愿你在尘世获得幸福 我只愿面朝大海，春暖花开 “从明天起，做一个幸福的人”，多么美好的诗句呀，做一个幸福的人，这应该是每一个人的梦想，但是在生活的洪流里面，多数人都为了生存而疲于奔波，连生存都困难，又怎么谈得上幸福呢，但是生活总是不只有眼前的苟且，还有诗和远方，我们总是会有千千万万的理由牵绊着我们去拥抱幸福，有时候拿起来很需要勇气，但是有时候放下更需要勇气，或许有一天，我们能够放下牵绊自己的理由，下定决心，从明天起，做一个幸福的人，安静的品味自己的生活，也不枉来人间走一遭。从明天起，开始悉心的感受生活，从观察开始，发现生活中以前不曾注意的细节，说不定，生活会开始变得不一样。","link":"/2019/08/29/Poetry with Life/以梦为马/"},{"title":"偶然","text":"偶然—-徐志摩 我是天空里的一片云， 偶尔投影在你的波心。 你不必讶异，更无须欢喜， 在转瞬间消灭了踪影。 你我相逢在黑夜的海上， 你有你的，我有我的，方向； 你记得也好，最好你忘掉， 在这交会时互放的光亮！ 我是一片云，曾走近你的心，但是我是云，远隔万里，随风而动，最终，又飘离了你的心，不奢求天长地久、海枯石烂，但是至少曾经美好的爱过，所以你我不必为此惆怅难过，我们曾经一起度过一段最美好的时光，这是你我回忆里的光辉，你若记得，那也好，但是，你最好忘了吧，从现在开始拥抱着你的新生活，不用担心这份美好会在时光的长河里淹没，因为我一直铭记心中，不曾丝毫遗忘褪色。 这首诗是徐志摩与林徽因在欧洲别离又在国内相遇时作的诗。徐志摩与林徽因曾经是恋人，但是再次相遇时，林徽因已经嫁给梁思成为人妻了，正所谓“物是人非事事休，未语泪先流。”，不过虽然如此，徐志摩还是写下了这首诗送给了林徽因。","link":"/2019/09/05/Poetry with Life/偶然/"},{"title":"再别康桥","text":"—-徐志摩 轻轻的我走了，正如我轻轻的来；我轻轻的招手，作别西天的云彩。那河畔的金柳，是夕阳中的新娘；波光里的艳影，在我的心头荡漾。软泥上的青荇，油油的在水底招摇；在康河的柔波里，我甘心做一条水草！那榆阴下的一潭，不是清泉，是天上虹；揉碎在浮藻间，沉淀着彩虹似的梦。寻梦？撑一支长蒿，向青草更青处漫溯；载满一船星辉，在星辉斑斓里放歌。但是我不能放歌，悄悄是别离的笙箫；夏虫也为我沉默，沉默是今晚的康桥！悄悄的我走了，正如我悄悄的来；我挥一挥衣袖，不带走一片云彩。 初次读到徐志摩的这一首诗是在初中，兴许早在很久之前就听闻过 “我挥一挥衣袖，不带走一片云彩。” 这些诗句了。时隔多年，再读 《再别康桥》 ，品味到一种当初无法体会到的美感。整首诗读起来一气呵成，感情犹如潺潺溪水，绵延不绝，柔情仿佛就是轻柔的月光，一下子倾泻而出，别离之情即真挚热烈，又温柔似水，诗情画意，尤为动人，真不愧是诗词才子。","link":"/2019/09/25/Poetry with Life/再别康桥/"},{"title":"初相遇","text":"—-席慕蓉 美丽的诗和美丽的梦一样 都是可遇不可求的 常常在最没料的时刻里出现 我喜欢那样的梦 在梦里，一切都可以重新开始 一切都可以慢慢解释 心里甚至还能感觉到所有被浪费的时光 竟然能够重回时的狂喜和感激 胸怀中满溢着幸福 只因为你就在我眼前 对我微笑 一如当年 我真喜欢那样的梦 明明知道你已为我跋涉千里 却又觉得芳草鲜美 落英缤纷 好像你我才初相遇 >第一次相遇，或许未曾察觉到你我之间将会有一段美好的故事，就像是无数的其他平凡的相遇——相遇 擦肩 遗忘，但这不是你我的剧本，我们会有不一样的邂逅，然后铭记彼此。也许在许多年以后，我们已经走进了生活的五光十色里，彼此再相遇，这一次，我们是否会像初相遇时，再一次目光相遇，便已察觉到，我们故事——未完待续。","link":"/2019/09/13/Poetry with Life/初相遇/"},{"title":"半截的诗","text":"—-海子 你是我的半截的诗，不许别人更改一个字。 简单的一句诗，却颇有一种浪漫从字里行间迸发而出，这爱热情似火，纵世界五彩缤纷，我只宠你一人；这爱始终如一，矢志不渝，你就是我的生命的另一半，我不允许别人改变你，这爱有点任性，对你，不曾更改；这爱是不言的浪漫，希望纸短情长，一句 两句，都能写进你的心坎。","link":"/2019/09/17/Poetry with Life/半截的诗/"},{"title":"发呆","text":"发呆—-我站在阳台向下看风抚着白千层的枝条摇曳阳光洒在阳台的仙人掌上金色的绒毛，像极了 发呆只有耳边的旋律冬日的阳光慵懒着，宠溺不想动只愿被光融化了飘散到小草 大树 行人身上留下一丝温热那就好了 这是在刚入大学时候，在宿舍写得诗，是的，这是一首诗，是我生活的诗，它看起来不美，但是足够敲打我的心弦。有时候，真感激当初的自己，在奔涌向前的时光流水里，留下了一只小纸船，让我能够在今后的某一天，回头时，能窥见自己当初的样貌。以前看着真傻，看着真好。","link":"/2019/07/28/Poetry with Life/发呆/"},{"title":"夏天的太阳","text":"夏天的太阳—-海子活在这珍贵的人间太阳强烈水波温柔一层层白云覆盖着我踩在青草上感到自己是彻底干净的黑土块活在这珍贵的人间 泥土高溅扑打面颊活在这珍贵的人间人类和植物一样幸福爱情和雨水一样幸福 在读过木心、汪国真的诗后，和海子的诗对比，木心先生的诗显得淡雅精致，读起来就像是在品一杯淡茶；汪国真的诗韵律巧妙，读起来就像是一首歌曲，抑扬顿挫。木心先生和汪国真先生也写感情，木心先生淡雅，汪国真先生则是直抒胸臆。看海子的诗，有种虽然没有木心先生的淡雅滋味，但是也不会像汪国真先生的开门见山，独有一份自己感受生活、抒发感情的态度与章法，可以说，我可能更喜欢海子的诗词，一种我能够品尝到，但是又不至于像汪国真先生诗词那样浓郁的爱意。另外，海子的诗应该更多的是对生活细节的体验，像一个可以触及的平常人，显得真实，但是在海子的笔触下又能写的出让人心血澎湃的感人诗句，平常之中孕育着不凡的灵魂。","link":"/2019/08/30/Poetry with Life/夏天的太阳/"},{"title":"怀想","text":"---汪国真 我不知道 是否 还在爱你 如果爱着 为什么 会有那样一次分离 我不知道 是否 早已不再爱你 如果不爱 为什么记忆没有随着时光 流去 回想你的笑靥 我的心 起伏难平 可恨一切 都已成为过去 只有婆娑的夜晚 一如从前 那样美丽","link":"/2019/08/14/Poetry with Life/怀想/"},{"title":"思念","text":"思念—-汪国真我叮咛你的你说不会遗忘你告诉我的我也全都珍藏对于我们来说记忆是飘不落的日子——永远不会发黄 相聚的时候总是很短期待的时候总是很长岁月的溪水边捡拾起多少闪亮的诗行如果你要想念我就望一望天上那闪烁的繁星有我寻觅你的目——光 “我叮咛你的 你说不会遗忘 你告诉我的 我也全都珍藏” 相遇、相知、相恋，这是一个多么美妙的过程呀，你我之间的互动，充满了恋人之间的不用言说的默契，只是一个眼神相遇，那便是千言万语，在彼此心里浮现，这就是恋人间所谓的心有灵犀吧！“记忆是飘不落的日子 永远不会发黄”，这一句里用了暗喻的手法，把记忆与叶子相互比喻，记忆不飘落，永远在树上，那便是常青于树上，所以不会发黄，也就是不会遗忘。说到这里，美好记忆永存也是很多人的共同愿望，但是，会不会永存呢？时间会告诉我们答案。","link":"/2019/08/01/Poetry with Life/思念/"},{"title":"我想和你虚度时光","text":"我想和你虚度时光—-李元胜我想和你虚度时光，比如低头看鱼比如把茶杯留在桌子上，离开浪费它们好看的阴影我还想连落日一起浪费，比如散步一直消磨到星光满天我还要浪费风起的时候 坐在走廊发呆，直到你眼中的乌云全部被吹到窗外我已经虚度了世界，它经过我疲惫，又像从未被爱过但是明天我还要这样，虚度满目的花草，生活应该像它们一样美好一样无意义，像被虚度的电影那些绝望的爱和赴死为我们带来短暂的沉默我想和你互相浪费 读到这首诗的时候，闭上眼，仿佛能够感受到阳光烘焙着空气的留下的淡淡暖意，空气中弥漫着一种慵懒的气息。她坐在窗边，头枕着手趴在桌子上，我坐在对面，安静的看着她，就是安静的看着，听着彼此的呼吸声、心跳声，不说话，然后目光相遇、对视、凝望，柔情暖意似水般涌动跳跃，此时墙壁上的挂钟滴答滴答的规律的跳动着，时间从指缝间溜走，就这么溜走，心甘情愿的让它跑了。我们就是这么对视着，不说话，但又好像说了很多话，一直在说话","link":"/2019/07/28/Poetry with Life/我想和你虚度时光/"},{"title":"是否","text":"——汪国真是否 你已把我遗忘不然为何 杳无音信天各一方是否 你已把我珍藏不然为何 微笑总在装饰我的梦 留下绮丽的幻想 是否 我们有缘 只是源头水尾 难以相见 是否 我们无缘 岁月留给我的将是 愁绪萦怀 寸断肝肠 汪国真的这首诗情人相隔远方，思念无法排遣的愁绪填满了字里行间，当诗句一句句映入脑海，似乎能够看见那相思柔情，这等柔情是凄美的，也是许多人所经历的，我虽然觉得诗词美，但终究是写的露骨，少些含蓄，在这一方面有所考究的话，或许能更符合我的喜好。“问世间情为何物，直教人生死相许”，难呀！ 今天重新部署了一下博客，换成了在Windows中写，这样就不用登录服务器了，而且还尝试了hexo-admin这个插件，可以直接在网页上写博文，现在先试试效果怎们样，测试一下！","link":"/2019/08/26/Poetry with Life/是否/"},{"title":"暮色","text":"—-席慕蓉 在一个年轻的夜里 听过一首歌 清冽 缠绵 如山风拂过百合 再渴望时 却声息寂灭 不见踪迹 亦无来处 空留那月光沁人肌肤 而在而十年后的一个黄昏里 有什么是与那夜相似 进而使那旋律翩然来临 山鸣谷应 直逼我心 回顾所来径啊 苍苍横着的翠微 这半生的坎坷啊 在暮色中 竟化为甜蜜的热泪 >青春易逝，当人至暮年，回首数十年前的风华正茂，不知道是什么感想，从席慕蓉的诗来看，似乎每当暮年回顾年轻的夜总是甜的，令人感叹的，值得珍惜的，可能也是这样的，在人文主义的角度看来，人的体验是神圣的，这是人生命意义的来源，因此不管在年少时经历过什么风雨，当人至暮年，即将走向生命的终点站的时候，这一切都是这一生意义的来源，都应该是值得感悟的经历。在这样一个年纪，伴随着荷尔蒙的分泌，或是激情四射，或是思绪萦绕，这一切都是青春的味道，这样的味道是值得人去铭记一生的，值得人去反复品味的。这让我想起了《钢铁是怎么炼成的》里的一段话，当人至暮年时，你会怎么看待你的人生呢？怎样的人生才能够让你无悔呢？才能够让你觉得没有白来人间走一遭呢？这是个值得思考的问题，我仍需多思考思考。","link":"/2019/09/14/Poetry with Life/暮色/"},{"title":"哪有你这样你","text":"哪有你这样你—-木心十五年前阴凉的晨 恍恍惚惚清晰的诀别 每夜，梦中的你梦中是你 与枕俱醒觉得不是你 另一些人扮演你入我梦中 哪有你，你这样好哪有你这样你","link":"/2019/07/27/Poetry with Life/机器学习名词解释/"},{"title":"爱情是棵树","text":"爱情是棵树—-木心我是锯子上行 你是锯子下行 合把那树锯断两边都可见年轮一堆清香的屑锯断了才知爱情是棵树树已很大了 第一次读到这首诗的时候，是大一了，不过并没有读懂，直到现在也没有读懂，倒是颇为好奇，这拿锯子锯树的方式到底是两个人拉着一把锯子，上下行的合作锯树呢，还是，两个人各拿着一把锯子，在树的两侧锯着，这两种方式都符合诗中的描述，所以我确实有些疑惑，不知道是哪种理解好，如果是第一种，倒是比较好理解，两人共同生活，共同为爱情负责嘛，力总是相互的，所以锯子一来一去，树倒了不是个人的责任，两者都是要负责的。但是，如果是第二种状况，那就可能是一方快些、多些，另一方慢些、少些，毕竟两把锯子，说以完全一样，那是不实际的。所以这个疑惑让我有些难以理解诗，不过，这个关注点确实有点剑走偏锋了。","link":"/2019/08/01/Poetry with Life/爱情是棵树/"},{"title":"陪伴","text":"陪伴我一辈子走过许多地方的路，行过许多地方的桥，看过许多次数的云，喝过许多种类的酒，却只爱过一个正当最好年龄的人。 —-沈从文 这是在今天出门随意走走时看到的一幅画面，两个耄耋老人随步履蹒跚，但却互相依靠、陪伴，这不禁让我心中涌起一股暖意，正如沈从文先生所说，一生只爱过一个正当最好年龄的人，一生之爱，一生陪伴，或许老人们并不曾谈及爱情，但是相濡以沫之情充溢着画幅，陪伴是最长情的告白！","link":"/2019/08/09/Poetry with Life/陪伴/"},{"title":"面朝大海","text":"面朝大海，春暖花开—-海子从明天起，做一个幸福的人 喂马，劈柴，周游世界 从明天起，关心粮食和蔬菜 我有一所房子，面朝大海，春暖花开 从明天起，和每一个亲人通信 告诉他们我的幸福，那幸福的闪电告诉我的 我将告诉每一个人 给每一条河每一座山取一个温暖的名字 陌生人，我也为你祝福 愿你有一个灿烂的前程 愿你有情人终成眷属 愿你在尘世获得幸福 我也愿面朝大海，春暖花开 现在是2020年1月31日，没想到2020年的开端是这样的，回顾2019年，生活依旧每日循环，走在安逸的校园里，追逐着追逐着。国内中美贸易战、香港修例风波，世界风云再起，可以说，2019年是有点糟糕的一年，但是这么难的一年，我们也是过来了。没想到2020的任务更加艰巨，刚开始就抛出了新型冠状病毒的难题，多少医护人员为了人民的生命安康而不顾自己的生命安危在抗疫一线奋斗不息，他们的舍己为人的精神值得所有人敬佩。从2019年12月底开始到现在，应该是发展一个月了，现在疫情已经达到了比较大的一个规模，形势比较严峻，需要全社会的共同努力，继续抗争。 我没有对于非典的记忆，那时还小，也还住在山村，脑海里没有一点相关的痕迹，现在正在经历的这一件事情让我真正见识到了传染病毒爆发的恐怖，当初的多少医护人员也是像现在所有奋斗在一线的医护人员一样，舍个人安危，救治每一个需要帮助的人，由衷的佩服这些医护人员。","link":"/2020/01/31/Poetry with Life/面朝大海/"},{"title":"Markdown学习笔记","text":"Introduction 接下来我将会以简单明了的方式介绍Markdown的语法，介绍的过程也是我学习实践的过程，这也是我第一次全文用Markdown语法书写文章，还很生疏，有些语法可能也还不太了解，所以会有翻车的现象。 Markdown 由 John Gruber 在2004年创建，至今已经是12年的时间。这门语言的宗旨是易读易写，而它的语法也确实如此，简单强大！接下来开始学习！ 基本写作1. 标题标题分为6级，如下 #一级标题 ##二级标题 ###三级标题 ####四级标题 #####五级标题 ######六级标题 效果如下 一级标题二级标题三级标题四级标题五级标题六级标题 2.强调 斜体—包括在两个 ‘’或‘_’ 之间的字符*黑体—包括在两个‘’或‘__’之间的字符删除线—包括在两个‘~~’之间的字符全部复合使用（有待补充） 3.列表 无序列表 苹果 香蕉有序列表 苹果 香蕉 4.引用引用如下 这里面是引用的内容，通过在段前添加‘&gt;’符号开始引用段落，在此段后空一行即结束引用，在段落间使用 这是嵌套引用 这也是 5.代码 print 'Hello world' 在两个反引号‘`’之间为行内代码 12def function(x): return x^2 在连用三个反引号‘`’之间的符号为代码区块，如上所示 6.分割线 如下（连续三个或以上‘*’），即表示分割线 或者如下（空一行，连续三个或以上‘-’） 7.表格通过如下方式可以渲染出表格 12345|table|head|here||:-----|:-----:|-----:||content1|content2|content3||content1|content2|content3||content1|content2|content3| 效果如下 table head here content1 content2 content3 content1 content2 content3 content1 content2 content3 ### 8.图片 ![Von Gogh](./Markdown简易教程/Von_Gogh.jpg) 就是上面这段格式的代码就是引用了这张梵高的图片了，注意的是其中的图片的地址，这个是同级目录下的直接引用，属于相对目录，也可以用绝对路径，不管相对还是绝对，路径一定要准确，这样才能定位到需要引用的图片上，才能展示出来 9.链接 链接就是比上面的图片的引用少了前面的‘！’，去掉这个感叹号后，再把需要的链接URL放在后面的括号里就可以了[My github page site](https://genening.github.io) My github page site或者百度首页 10.公式 公式的展示是采用LaTeX的语法，通过这个网站可以把公式对应的LaTeX的写法转换出来 如下为行内公式 $E=mc^2$对应 $E=mc^2$ 有待尝试 如下为公式块 1234$$e^{i\\theta} = \\cos \\theta +i\\sin \\theta \\e^z = 1 + \\frac{z}{1!} + \\frac{z^2}{2!} + \\frac{z^3}{3!} + \\cdots = \\sum_{n=0}^{\\infty}\\frac{z^n}{n!}$$ 效果出不来，比较尴尬 11.反斜线 如果想插入一些特殊字符的话需要转义符号反斜线‘\\’，引入这个反斜线后，就可以使用上面的那些特殊的字符了，这样不会触发对应的渲染效果 \\反斜线`反引号*星号_底线{}花括号如上符号等等 12.脚注脚注可以用于编辑参考文献 在文中使用[^1]的方式标记脚注 在文末使用[^1]:加入参考文献，注意要使用英文冒号，后面有无空格均可 效果如此[^1]，或者是参考文献[^1]: 此处尝试又出bug了，接着完善 结语Markdown是所谓的轻量标记语言(lightweight markup language)，它可以让我们在写文章的时候专注于写作本身，而不用费心于排版，这种思想和LaTeX的思想不谋而合。现在很多网站都支持Markdown，也有很多相应的编辑软件，例如Typora，这是个很好用的Markdown的编辑软件。另外，让Markdown更强大的是，Markdown支持直接插入HTML的语法，也能插入LaTeX的语法，这样的扩展能力，让Markdown即满足了轻便易用的基础上，进一步增强了能力，这也是Markdown现在炙手可热的原因之一。","link":"/2019/08/29/Practical skills/Markdown学习笔记/"},{"title":"Data_analysis","text":"前言自从进入信息时代以来，数据分析变得越来越重要，现在数据分析工作已经融合到了各个行业和领域，是我们了解领域、行业、公司、组织、个人的发展现状及趋势的必要工具，借助数据分析，我们可以更好的了解事件本身的内在规律和发展趋势，因此学习并掌握数据分析技能对于提升个人的竞争力非常有意义。 什么是数据分析？数据分析是从原始数据中提取信息的过程。在理解这个定义之前，我们要先明确一个观点：信息不等同于数据。信息一般指的是具有一定含义、可以直接理解，可以用于其他场合的结论；数据则是一种抽象的概念，现代生活中有着各种各样的传感器，可以收集各种各样的数据，这些数据的格式各异，内容各异，一般难以直接通过简单观察得出相关结论，只有对这些数据进行分析、提炼、概括，才能获取一些有效的信息，而这个过程就是数据分析。 数据分析的目的是抽取不易推断的信息，而一旦理解了这些信息，就能够对产生数据的系统的运行机制进行研究，从而对系统可能的响应和演变做出预测。数据分析的意义不止于数据建模，更重要的在于对于给定系统输入时对系统输出的预测。 数据分析内涵数据分析是一个综合概念，是一系列操作的概述。数据分析包括了从问题的定义，到数据的收集获取、整理清洗，再到数据的解析理解、可视化、数学建模、预测分析、模型测试、部署的一系列工作。简单来说就是数据分析是包含了从数据的收集到应用的全部操作过程。学习数据分析需要我们熟练的掌握计算机技术，能够通过编程进行数据分析、展示，同时还需要我们能够有较敏锐的统计学嗅觉，能够采用恰当的统计学方法分析数据，得出我们需要的数据特征。另外，根据解析出来的数据特征进行阐释也是非常重要一项工作，这需要我们对于对应的领域有一定的了解，这考验我们的综合能力，需要再平常是生活中多进行积累工作。 通用分析过程 问题定义 问题的定义指的是最开始问题的提出的过程，提问也是需要技巧的，提问要能够切中要害，接下来的分析结果才能够带来最有效的帮助。好的问题能给我们带来事半功倍的效果。 数据获取 在完成问题的定义之后，在分析数据之前，首先要做的就是获取数据。所采集的数据全面性、完整性、真实性非常重要，因为数据是我们进行分析的根本所在，如果数据本身存在着较大的偏差，那么后面的分析以及结果将毫无意义。因此在数据获取这一步必须要严谨。 数据准备（预处理） 在获取到数据之后，我们需要对这些数据进行规范化处理。我们获取的数据可能是从多个渠道获取的，那么这就很可能会导致我们数据的格式、表现方式不一致，这些问题会阻碍我们对数据的分析研究。这一步的内容也就是数据的预处理，包括数据规范化、清洗、去重、补缺等工作。 数据探索和可视化 探索数据本质上是指从图形或统计数字中搜寻数据，以发现数据中的模式、联系和关系。数据可视化是突出显示可能的模式的最佳工具。数据可视化发展迅猛，已经成为了一门真正的学科。数据探索包括初步的检验数据，这对于理解采集到的数据的类型和含义很重要。再结合问题定义阶段所获得的信息，确定数据类型，这决定着选用哪种数据分析方法最合适。探索过程大概包括以下内容。 总结数据 – 了解数据的大体性质，包括数据量的大小、存储格式、数据类型、变量数等内容 数据分组 – 在了解了数据的基本性质之后，对于数据进行进一步的了解、探索，通过分组的方法，探究数据之间的关系（相关性），从而得出一些中间结论 探索不同属性之间的关系 – 通过聚类的手段，对数据进行归并，划入不同的group，方便我们提炼信息 识别模式和趋势 – 数据分析的一个重要步骤就是识别（identification）数据中的关系、趋势和异常现象。这一一般需要对数据进行统计处理，然后对统计所得的数据进行可视化处理，通过直观的观察提炼相关的信息。 建立回归模型 建立分类模型其他的数据挖掘方法，如决策树和关联规则挖掘，以及机器学习算法，则是自动从数据中抽取重要的事实或规则，这些方法可以配合数据可视化配合使用，以便发现数据之间存在的各种关系。 预测模型 这个阶段是要创建或选择合适的统计模型来预测某一个结果的概念，比较常用的有聚类模型、回归模型、贝叶斯分析、马尔代夫链等模型，因此对于统计学能力有较高的要求。一般来说模型的功能一般是两种，一种是给定一个值预测系统的输出，这种需要回归模型来处理；另一种是对所掌握的数据进行分类、聚类，这需要采用聚类模型来处理，例如猫、狗识别模型，可以对输入的数据（图片）进行归类，到底是狗还是猫。 模型评估 在我们得到了模型之后，这个模型到底好不好呢？这就需要对模型进行评估，一般是评估模型的准确性、稳健性等性质，从而判断模型好不好。一般我们会对数据进行分组，一部分划分为训练集，一部分划分为测试集，这是一种可行的方法，帮助我们判断模型。不过现在有更好的办法，能够提高对数据的利用率，这就是交叉验证，这种方法是把数据分成若干份，然后轮流选取其中一份作为测试集，其余数据为训练集，训练模型，然后直至每一份数据都充分使用之后，对所得到的结果进行整合，从而得到更好的效果，其实这就是集成学习，这种方法现在被广泛使用，比如XGboot等方法。 部署 在评估模型之后，保留好的模型，把好的模型部署到实际的应用场景中，这是最后一步，也是非常重要的一步，只有完成这一步之后，才能够让我们做的工作真正的产生价值。一般模型的部署都属于传统的开发工作，这些工作需要我们有扎实的传统开发技能，所以平常我们不能疏忽了这方面的积累和锻炼。 总结在现在这个时代，信息就是价值，虽然互联网抹平了很多信息鸿沟，让信息变得比较对称，但是深层次的信息依然需要专门的挖掘才能够获得，这也是比较有价值的信息。掌握数据分析能力是我们提升个人能力，解构世界、了解社会的利器，因此有必要好好学习。 数据就是宝藏，信息就是财富。","link":"/2020/07/24/Python/Data-analysis/"},{"title":"正则学习笔记","text":"前言正则表达式，也可以称之为“标准表达式”，是通用的字符匹配工具，在各种文本中，采用正则都可以匹配自己想要的内容，即使不同文本的编写格式各异。这也是正则的强大之处。正则由普通字符、非打印字符、特殊字符、限定符、定位符5部分构成，通过这5部分的排列组合，可以用于匹配各种特征的字符，在我们需要动态查找、匹配、替换时，意义重大。 1. 普通字符“普通字符”顾名思义，就是在文本中最主要的构成部分，包括没有显式指定为元字符的所有可打印和不可打印字符。这包括所有大写和小写字母、所有数字、所有标点符号和一些其他符号。这些字符可以直接写在正则中，匹配指定特征的文本。 2. 非打印字符“非打印字符”指的是在文本的构建中会使用到用于格式化文本但是在打印显示中无法直接看到的字符，是文本编辑排版中必备的内容，但是就像是空气一样，作用与无形。下标列出了一些常见的非打印字符及含义。 字符 描述 \\cx x指的是A-Z a-z中的某个字符，和\\c组合起来表达特定的含义。 \\f 匹配一个换页符，等价于\\x0c和\\cL \\n 匹配一个换行符 \\r 匹配一个回车符 \\t 匹配一个制表符 \\v 匹配一个垂直制表符 \\s 匹配任何空白字符，包括空格、制表符、换页符、换行符等 \\S 匹配任何非空字符 3. 特殊字符在正则表达式中保留的具有特定含义的字符，在进行通用匹配的时候非常重要。如果要匹配这些保留字符本身，需要使用’'符号来进行转义。下表列出了正则中的特殊字符。 特别字符 描述 () 标记一个子表达式的开始和结束位置。提取后子表达式内容将单独保留到结果，其他字段不保留 * 匹配前一个表达式零次或者多次 + 匹配前一个表达式一次或者多次，不包括零次 ? 匹配前一个表达式零次或者一次，也可表达匹配为非贪婪（贪婪往长了匹配，非贪婪寻找最短匹配目标） . 匹配除\\n之外的任何单字符，在通用匹配时很重要。 [] 中括号表达式，标记中括号表达式 {} 标记限定符表达式的开始。 |转义字符，标志其他特殊字符转化为普通字符 $ 匹配输入字符串的结尾位置 ^ 匹配输入字符串的开始位置，在[]内时表示取反含义。 | 选择字符，在两项之中匹配一项 4. 限定符限定符用来指定特定表达式出现的次数设定，比如某个相同特征在同一个段区间内要重复几次或者未知若干次，那此时就可以使用限定符来限定次数。 字符 描述 * 匹配前一个表达式零次或者多次 + 匹配前一个表达式一次或者多次，不包括零次 ? 匹配前一个表达式零次或者一次，也可表达匹配为非贪婪（贪婪往长了匹配，非贪婪寻找最短匹配目标） {n} n为非负整数，表示匹配n次 {n,} 表示至少匹配n次，无上限 {n, m} 至少匹配n次，至多匹配m次 5. 定位符定位符可以将正则表达式固定到行首或者行尾。定位符用来描述字符串或单词的边界，^ 和 $ 分别指字符串的开始与结束，\\b 描述单词的前或后边界，\\B 表示非单词边界。 注意：不能将限定符与定位符一起使用。由于在紧靠换行或者单词边界的前面或后面不能有一个以上位置，因此不允许诸如 ^* 之类的表达式。若要匹配一行文本开始处的文本，请在正则表达式的开始使用 ^ 字符。不要将 ^ 的这种用法与中括号表达式内的用法混淆。若要匹配一行文本的结束处的文本，请在正则表达式的结束处使用 $ 字符。 6. 补充拓展 pattern使用 精确匹配 7. 优先级 总结在学习完一遍之后，发现其实正则也并不难，在我们观察文本找到我们需要匹配的文本时，只要选择好明确的特征，然后把这个特征用正则描述出来就能匹配到我们需要的内容，有些时候目标文本本身特点不好描述时，如果该目标文本内容的前后有一些特殊的字符串，那也可以使用这些特殊的字符串当特征来进行匹配，只需要使用“()”来将中间的目标字符串的表达式包括起来，那么在返回的结果中就会只返回这个子表达式的匹配内容，而其他的正则表达式就充当了定位符的作用。总而言之，熟能生巧，今天学习一遍之后并不觉得难，但是如果不常使用，终归是会淡忘的，所以要多注意，多尝试使用，温故而知新。","link":"/2020/05/17/Python/RE-studynotes/"},{"title":"numpy","text":"1. 前言Numpy和pandas是python当中两个非常重要的库，用于配合科学运算和矩阵处理。程序是算法与数据的结合，二者缺一不可，而numpy和pandas就是python中用于处理数据的利器，因此学习这两个package的使用非常重要。在使用matplotlib和seaborn进行数据可视化的时候，就会发现，所有作图的关键在于数据格式的把控，这两个package所提供的API就像是图表的模具，而整理好相应格式的数据就是原料，numpy和pandas就是整理数据的工具。接下来让我们开始学习这两个package吧。在这一篇博客中我们先学习numpy。 这是一幅python_package的总览图，由此我们可以大概了解，有哪些很值得我们学习的package。 2. Numpy学习 理解Numpy是一个科学运算包，里面包含了最重要的的内容就是一个ndarray（N dimension array）模型，即n维数组模型，如果学习过线性代数的数值运算，我们就会知道矩阵运算对于计算机而言有多重要，那我们就会知道ndarray模型对于python来说有多重要，可以说，正是因为这个模型，python才被赋予了科学运算的能力。除了ndarray模型的属性及其相关方法，在numpy包里面还包括了大量的数学函数、统计函数、字符串函数、随机函数等这些可以表达数学语言的内容，为我们构建复杂的数学算法奠定了基础，其中很多函数对于我们进行科学运算尤为重要。接下来我们先学习第一部分内容，ndarray的属性及其方法。 ndarray的属性 123456789101112131415161718192021222324252627282930313233343536373839In [1]: import numpy as npIn [2]: a = np.arange(15)In [3]: aOut[3]: array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14])In [4]: print(a.shape)(15,)In [5]: print(a.ndim)1In [6]: b = a.reshape(3, 5)In [8]: print(b.ndim)2In [9]: print(b)[[ 0 1 2 3 4] [ 5 6 7 8 9] [10 11 12 13 14]]In [10]: print(a.flags) C_CONTIGUOUS : True F_CONTIGUOUS : True OWNDATA : True WRITEABLE : True ALIGNED : True WRITEBACKIFCOPY : False UPDATEIFCOPY : FalseIn [11]: print(a.itemsize)4In [12]: y = np.array(a, dtype=np.float64)In [13]: print(y.itemsize)8 ndarray操作方法 创建数组 创建有5种方式，其中包括0矩阵，单位矩阵，内容随机矩阵以及自定义的矩阵，还有arange方法，可以便捷的创建一维矩阵。 123456789101112131415161718192021222324252627282930313233In [14]: x = np.empty([3,2], dtype=int)In [15]: xOut[15]:array([[ 2063158980, 32765], [ -536194544, 632], [ 0, -2147483648]])In [16]: y = np.zeros([3,2], dtype=float)In [17]: yOut[17]:array([[0., 0.], [0., 0.], [0., 0.]])In [18]: z = np.ones([3,2], dtype=int)In [19]: zOut[19]:array([[1, 1], [1, 1], [1, 1]])In [23]: m = np.arange(15)In [24]: mOut[24]: array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14])In [25]: a = np.array([1,2,3,4,5,6])In [26]: aOut[26]: array([1, 2, 3, 4, 5, 6]) np常用数学函数 np线性代数运算 np random函数 function introduction 理解 randn(d1, d2) 返回一个符合标准正太分布的随机数据，array对象 重点在于符合标准正太分布 randint(low, high, size) 返回size个[low, high)范围内的随机整数，array对象 生成整数时使用，与random_integers相似 random_integers(low, high, size) 返回size个在[low, high]闭区间内的随机整数，array对象 生成整数，但是不建议使用这个方法，直接使用randint即可 rand(d1, d2) 返回[0.0,1.0)范围内的随机数，array对象 产生大于等于0并小于1的随机浮点数 random_sample([size]) 返回size个在[0.0, 1.0)之间的随机浮点数，array对象 产生大于等于0并小于1的随机浮点数 random([size]) 返回size个[0.0, 1.0)范围内的随机浮点数，array对象 产生大于等于0并小于1的随机浮点数 ranf(size) 返回size个[0.0, 1.0)范围内的随机浮点数，array对象 产生大于等于0并小于1的随机浮点数 sample(size) 返回size个[0.0, 1.0)范围内的随机浮点数，array对象 产生大于等于0并小于1的随机浮点数 seed([seed]) 在生成随机数的算法中设定初始值，保证每次random函数生成同一批随机数 设定随机数种子，每批随机数一样，在生成数据样本时用得上 12345678910111213141516171819202122232425262728293031323334353637383940In [119]: np.random.rand(2,3)Out[119]:array([[0.05794981, 0.25660757, 0.51033178], [0.99525878, 0.14651568, 0.44951478]])In [120]: np.random.random(size=(2,3))Out[120]:array([[0.60144038, 0.09727249, 0.28873488], [0.7207999 , 0.55080606, 0.83857702]])In [121]: np.random.ranf((2,3))Out[121]:array([[0.58033135, 0.18457174, 0.61550212], [0.88695509, 0.51678928, 0.62614367]])In [122]: np.random.sample((2,3))Out[122]:array([[0.50505912, 0.90966258, 0.41330466], [0.53546872, 0.34264313, 0.12957678]])In [123]: np.random.random_sample((2,3))Out[123]:array([[0.66282175, 0.93567 , 0.61272918], [0.8427385 , 0.21790844, 0.90317174]])In [124]: np.random.randn(2,3)Out[124]:array([[ 0.16390425, 0.82951248, 0.75529996], [ 0.87948569, -0.2399563 , -0.69505665]])In [125]: np.random.randint(1,10,size=(2,3))Out[125]:array([[3, 4, 5], [7, 5, 3]])In [126]: np.random.random_integers(1,9,size=(2,3))C:\\Users\\13560\\Anaconda\\Scripts\\ipython:1: DeprecationWarning: This function is deprecated. Please call randint(1, 9 + 1) insteadOut[126]:array([[7, 9, 9], [5, 5, 8]]) 总结","link":"/2020/04/20/Python/numpy/"},{"title":"404problem","text":"在8月28日晚，也就是昨晚，在更新了一些_config.yml里面的内容后，输入hexo clean &amp;&amp; hexo g &amp;&amp; hexo d指令后，出现了页面404，不管是刷新还是改其他信息，都无法解决这个问题，在看了网上很多相关博客后也没有找到问题的根源，最终还是在仔细看了GitHub的404页面的提醒，如下图 才发现，原来是GitHub的page的repository的根目录里没有了index.html文件，导致在访问https://genening.github.io时GitHub无法找到静态网页的文件，所以显示不出来，跳转到了404页面找到了问题所在之后，我就开始想办法解决了。先是查看了另外一个网站—— https://193.148.16.42的根目录的index.html文件，看看里面的内容应该是什么，然后再查看了整个目录的结构，对比我部署到GitHub的repository里面的文件目录结构，发现有很多不同，如果自己从头做一份的话很麻烦，而且hexo博客应该会自动生成，怎么会没有了呢，于是我就重新运行了一遍hew new &quot;blogname&quot;，发现hexo博客又重新生成了一遍目录，这一次目录结构就和另一份博客的目录一致了，于是我紧接着hexo clean &amp;&amp; hexo g &amp;&amp; hexo d一遍，发现GitHub上面的目录又更新了，再次访问，就能显示正常的blog页面了。具体产生这个问题的原因还不知道，不过也算是吃一堑长一智了，下次注意一些就好，而且也找到了解决问题的思路，也算是花费了我半天时间找解决办法的收获吧。故写此博客用以记录。","link":"/2019/08/29/log/404problem/"},{"title":"Latex-formula","text":"前言：LaTeX在表达数学公式方面有着得天独厚的优势，表达简便，渲染效果良好，是用来展示数学公式的最佳工具。当然，LaTeX本身是一个非常好的排版工具，在发表文章时一般可以采用LaTeX进行排版，美观专业。 接下来我将学习一下LaTeX关于数学公式方面的内容。 嵌入方法： 行内嵌入：’$..$‘ 例如：$\\sum_i^nx_i$ 单独成行：’$$..$$‘ 例如：$$y_i=\\sum_i^nx_i$$ 1. The Greek alphabet 希腊字母是数学表达式中常用的符号，几乎所有数学公式包含着希腊字母，希腊字母的在数学中的地位举足轻重。 示例：1234567891011121314151617181920效果：$\\alpha\\beta,\\delta,\\zeta,\\eta,\\theta,\\Pi,\\Delta,\\rho$### 2. 上下标&gt;在数学表达式中，经常会有上下标，不管是指数还是表示平均数、向量、积分上下限等等，这类都需要上下标的形式展现。在LaTeX中以`_`为下标，`^`为上标，其他效果见下图。![up-bottom](Latex-formula/up-bottom.jpg)示例：```$x_i^2,\\hat{x},\\bar{y},\\vec{a},\\dot{n},\\widehat{x},\\ddot{a}$```&lt;br&gt;效果：$x_i^2,x_{i=1}^{n+2},\\hat{x},\\bar{y},\\vec{a},\\dot{n},\\ddot{a}$### 补充：* 在LaTeX中的log函数表示`$\\log_2 x$`，效果：$\\log_2 x$，$\\log_{10} y$* 在LaTeX中的空格为`$\\quad$`，效果：$ab,\\qquad a\\quad b$### 3. 括号&gt;在数学表达式中，一般会存在嵌套关系，也就是不能用简单的一次线性关系表示完整，所以需要额外的符号进行辅助，也就是需要括号帮忙了，具体见下图：![括号](Latex-formula/kuohao.jpg)示例： $$f(x)=\\begin{cases} x = \\cos(t) \\y=\\sin(t) \\z=\\frac xy \\end{cases}$$```效果：$$f(x)=\\begin{cases} x = \\cos(t) \\y=\\sin(t) \\z=\\frac xy \\end{cases}$$ 4. 矩阵 矩阵，一般很难渲染得好看，让我们来看看LaTeX的本领 示例：$\\begin{bmatrix} 0 &amp; -1 \\\\ 1 &amp; 0 \\end{bmatrix}$效果：$$A=\\begin{bmatrix} 0 &amp; -1 \\ 1 &amp; 0 \\end{bmatrix}$$$$B=\\begin{matrix} 0 &amp; -1 \\ 1 &amp; 0 \\end{matrix}$$ 5. 求和、积分、开方、分数 这些也是数学表达式中很常见的符号和表达方式了，需要掌握。 示例：$\\sum,\\int_\\infty^\\infty xdx$效果：$y_i=\\sum_{i=1}^nx_i,F(x)=\\int_{-\\infty}^\\infty xdx$ 示例：$\\sqrt{x^3},\\sqrt[3]{x^4-2},\\sqrt[4]{\\frac xy}$效果：$\\sqrt{x^3},\\sqrt[3]{x^4-2},\\sqrt[4]{\\frac xy}$ 示例：$\\frac {a-1}{b-2}={a-1\\over b-2}$效果：$\\frac {a-1}{b-2}={a-1\\over b-2}$ 6. 特殊符号 见下图 示例：$\\sin {x^2} + \\cos {x^2}=1$效果：$$\\sin {x^2} + \\cos {x^2}=1,\\quad\\lim_{x\\to\\infty}{1\\over x}=0$$ 示例：$\\times,\\quad \\div,\\quad \\pm,\\quad \\lt,\\quad \\gt,\\quad \\simeq$效果：$\\times,\\quad \\div,\\quad \\pm,\\quad \\lt,\\quad \\gt,\\quad \\simeq$ 7. 换字体 这个不是很care了，随便看看。 示例：$\\mathbb{ABCDE},\\mathrm{ABCED},\\mathcal{ABCDE},\\mathscr{ABCDE},\\mathfrak{ABCED},\\mathsf{ABCDE}$效果：$\\mathbb{ABCDE},\\mathrm{ABCED},\\mathcal{ABCDE},\\mathscr{ABCDE},\\mathfrak{ABCED},\\mathsf{ABCDE}$ 总结LaTeX确实是论文排版非常重要的一个工具，在表示数学表达式的时候尤为重要，为了能够更好的排版，让论文更加专业，LaTeX很值得一学。并且，LaTeX的语法也并不复杂，对于数学表达式，有很多都是可以笔下代码脑中图像，非常流畅。LaTeX语法就是每当有特殊标记的时候就会使用\\来标记，如\\lim等等，每个符号后面一般默认作用为一个字符，当需要作用到字符串上时，采用{}框住表达式即可, 举个栗子：$\\sum_x-y$：$\\sum_x-y$中-y不会被包括近下标中；$\\sum_{x-y}$：$\\sum_{x-y}$，当用{}框住后就会包括进下标中。","link":"/2019/11/19/log/Latex-formula/"},{"title":"firstblog","text":"hello world, this is my first blog, I do not know if this blog can work, so I just have a try.It seems like work perfectly, okay, let is begain our new way of life.","link":"/2019/07/27/log/firstblog/"},{"title":"hexo博客写作流程","text":"这篇文章的目的是为本博客的博文作者提供指导，介绍需要准备的工具、写作流程和发布方法。查看 Hexo 文档 了解关于博客框架的详细内容。 依赖 博文作者 nodejs - Hexo 基础 npm - Node.js 包管理器 hexo-cli - Hexo 博客框架 git - 版本控制系统 服务器 git - 版本控制系统 nginx - Web服务器 准备工作Hexo 是静态博客框架，因此每个页面在读者访问之前就已经被渲染完成，大部分的工作都落在博文作者处。Hexo 依赖 Node.js 运行时环境工作，请访问 nodejs.org 或使用包管理器获取 Node.js，同时安装其包管理器 npm。随后即可使用 npm 安装 Hexo，建议设置为全局安装： 1$ npm install -g hexo-cli 想起来了就升级一下所有包，获取安全和功能更新： 1$ npm update -g 为方便部署，我们使用 git 执行版本控制，访问 git-scm 或使用包管理器安装 git。博客的配置和源文件仓库都保存在服务器上，要同步不同作者的更改或从新环境开始，可 merge 或 clone 博客仓库以得到工作区。 Hexo 使用 Markdown 创作博文，因此建议使用带有 Markdowm 预览的编辑器。准备工作结束之后，就能开始愉快地写作了。 流程 新建页面 1$ hexo new [layout] [option] &lt;title&gt; 在制定布局的基础上新建文章，可用选项有 Option Description -p, --path 自定义文章路径 -r, --replace 覆盖指定博文 -s, --slug 自定义文章URL 写博文 使用 Markdown 撰写文章，但不限于 Markdown 的语法。可以使用 HTML 元素实现更灵活的布局，同时通过 Hexo 插件得到例如资源文件夹管理等更强大的功能。 博客写作的高级功能包括： MathJax 数学公式支持 使用 $ LaTex $ 插入行内公式，$$ LaTex $$ 插入行间公式。 资源文件夹管理 资源文件夹管理是成为正式组件的社区插件之一，在需要插入图片的地方使用一下格式引用资源文件夹 JavaScript1{% asset_img 文件名 文件描述 %} 插入页面目录 在 Front-matter 中指定 1toc: true Hexo 将在指定位置渲染页面目录，方便长文章的浏览。 发布 写作完成后，使用以下命令发布到服务器： 1$ hexo g -d 这会让 Hexo 渲染所有页面，并使用 git 将网页文件推送至远程的 publish 分支。在服务器上，git hooks 会在接收推送后自动将其复制到 nginx 的根目录，可以立即在博客上查看更改。 注意直接部署并不会推送博客配置文件以及文章源文件，要备份整个工作区，请手动推送 master 分支到服务器，这个过程与部署过程相独立。","link":"/2019/08/29/log/博客写作流程/"},{"title":"Electron","text":"简介 Electron是由Github开发，用HTML，CSS和JavaScript来构建跨平台桌面应用程序的一个开源库。 Electron通过将Chromium和Node.js合并到同一个运行时环境中，并将其打包为Mac，Windows和Linux系统下的应用来实现这一目的。简单来说，这是一个强大的工具，可以用于搭建跨平台桌面应用，而且有方便强大的API库提供，所以，开始一场新的探索吧，如果你永远只做你能做到的事情，那么你永远也只能停在当前的高度。 目录：暂略1. 环境准备 安装Node.js Node.js下载页面 npm 安装Node.js的时候已经安装了 检查npm node是否安装成功，在命令行工具中输入node -v npm -v，显示版本号则便是环境准备完成。就是这么简单，赶紧尝试一下吧！ 快速开始一个项目 12345678# 克隆示例项目的仓库$ git clone https://github.com/electron/electron-quick-start# 进入这个仓库$ cd electron-quick-start# 安装依赖并运行$ npm install &amp;&amp; npm start 效果如下： 这个quick-start项目提供了一个非常简约的模板，可以在这个基础上搭建自己的应用。看起来还是不错的，很期待打包成桌面应用的效果。 quick-start目录解析 node_modules：模块依赖 package.json：描述包的文件，这里默认已经将主进程入口文件配置为main.js main.js：主进程 renderer.js：渲染进程，它的操作跟web中的js操作大同小异，所以最好有node.js、js以及es6的语法的功底，这样开发起来，才能得心应手。","link":"/2019/11/08/web/Electron/"},{"title":"JS学习笔记","text":"第一部分：JS介绍及基础知识 目录 初识JavaScript 变量和常量的知识 基本数据类型 运算符 基本数据类型间的转换 流程控制语句 1. 初识JavaScript概述： JavaScript一种直译式脚本语言，是一种动态类型、弱类型、基于原型的语言，内置支持类型。它的解释器被称为JavaScript引擎，为浏览器的一部分，广泛用于客户端的脚本语言，最早是在HTML（标准通用标记语言下的一个应用）网页上使用，用来给HTML网页增加动态功能。 javascript组成 （1）ECMAScript，描述了该语言的语法和基本对象。（2）文档对象模型（DOM），描述处理网页内容的方法和接口。（3）浏览器对象模型（BOM），描述与浏览器进行交互的方法和接口 应用 （1）嵌入动态文本于HTML页面。（2）对浏览器事件做出响应。（3）读写HTML元素。（4）在数据被提交到服务器之前验证数据。（5）检测访客的浏览器信息。 （6）控制cookies，包括创建和修改等。（7）基于Node.js技术进行服务器端编程。 2. 变量与常量认识引入 javascript的引入有三种方式:（1）写在标签内（2）使用&lt;script&gt;&lt;/script&gt;标签（3）独立js文件，使用link引入 123456789101112标签内&lt;style type=\"text/css\"&gt; #div {background-color:red;} //#是id标签 .p {color:blue;} //'.'是class类标签&lt;/style&gt;script标签&lt;scipt tpye=\"text/javascript\"&gt;js语句&lt;/script&gt;引入&lt;script src=\"\" type=\"text/javascript\"&gt;&lt;/script&gt;&lt;link href=\"https://...\" rel=\"stylesheet\"&gt; 标识符与关键字 第二部分：jsDOM操作 目录 属性、文本操作 css操作 对象与数组 面向对象编程","link":"/2019/09/07/web/JS学习笔记/"},{"title":"Yellowseabream-record","text":"前言这是实验室的一个新项目，探讨的是Yellowseabream在生长发育中雄转雌现象的调控基因，这个项目有两个部分，取的样品有脑组织和普通组织，普通组织部分的分析结果已经完成，我负责完成脑组织转录组的分析。因此我将在这篇博客中记录下我在整个项目的过程中的操作，方便以后进行结果整理以及纠错。 1. trimmomatic质控在拿到测序数据之后，公司的给的data中有raw data也有clean data，但是为了更好的掌控分析，质控还是自己做比较好。这里选择的质控软件是trimmomatic。 commands: nohup java -jar /B313/public_software/Quality_control/Trimmomatic-0.38/trimmomatic-0.38.jar PE -threads 40 a-BR-1_RRAS30349-V_1.fq a-BR-1_RRAS30349-V_2.fq clean_a-BR-1_RRAS30349-V_1.fq unpaired_a-BR-1_RRAS30349-V_1.fq clean_a-BR-1_RRAS30349-V_2.fq unpaired_a-BR-1_RRAS30349-V_2.fq ILLUMINACLIP:/B313/public_software/Quality_control/Trimmomatic-0.38/adapters/TruSeq3-PE.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:50 &amp; tips:这里指令较长，且需要对每一组进行质控，每次重复输入非常麻烦，因此在第一次输完命令之后可以把命令整理到一个bash脚本中，下次需要重跑时只需要运行bash脚本即可，修改参数可以用Linux文件处理软件sub进行替换修改。 结果文件列表: 输出结果有两种类型文件，一种是clean文件，另一种是unpaired文件。其中clean文件就是质控之后的data，用于下一步分析，unpaired文件是被清理出来的data，只是起一个记录作用。 1.2 fastqc质控可视化在做质控之前和做完质控之后，要进行数据质量的可视化，用于直观判断质控的效果好坏以及测序数据本身质量的高低。这里采用的是fastqc进行这步工作。 fastqc [-o output dir] [--(no)extract] [-f fastq|bam|sam] [-c contaminant file] seqfile1 .. seqfileN commands: nohup fastqc -o ./fastqc_70/ -f fastq clean_* &amp; output file: 这里输出结果有html和zip文件，只需要查看html文件就可以知道data的质量的统计结果了，但是这里文件众多，一个个查看很麻烦，且不方便进行横向比较，因此可以采用multiqc进行fastqc的结果整合，方便查看。 1.3 multiqc可视化结果整合commands: multiqc /B313/Zjunlin/raw_data/fastqc_70/ -o /B313/Zjunlin/raw_data/multi_qcraw/ output file: 这里输出结果是一个html文件和一个data的文件夹，文件夹里面有相关的数据，如果需要自己重新作图的话可以利用这些数据作图。要查看整合的结果只需要把html下载到本地电脑中用浏览器打开查看即可，图片结果的解读在另一篇博客fastqc_multiqc结果解读，可以自行查看 2. Trinity拼接（Trinity-v2.8.4）Trinity是一款非常成熟的拼接软件，非常受大家欢迎，Trinity的使用方法查看无参转录组分析。 Trinity --seqType fq --left reads_1.fq --right reads_2.fq --CPU 6 --max_memory 20G commands: nohup Trinity –seqType fq –max_memory 100G –min_glue 10 –full_cleanup –output trinity_out_dir –left clean_a-BR-1_RRAS30349-V_1.fq clean_b-BR_RRAS27375-V_1.fq clean_c-BR-1_RRAS30350-V_1.fq clean_d-BR_RRAS30433-V_1.fq clean_e-BR_RRAS30435-V_1.fq clean_o-3-HD_RRAS47919-V_1.fq –right clean_a-BR-1_RRAS30349-V_2.fq clean_b-BR_RRAS27375-V_2.fq clean_c-BR-1_RRAS30350-V_2.fq clean_d-BR_RRAS30433-V_2.fq clean_e-BR_RRAS30435-V_2.fq clean_o-3-HD_RRAS47919-V_2.fq –CPU 30 –min_kmer_cov 2 &amp; 这里需要等待比较长的计算时间，并且在结果出来之后要进行统计，要是统计结果不太好则要重新调整参数重跑。所以还是把质量写进脚本run_trinity.sh比较方便。 output files: 输出的结果包括了三部分，分别是Trinity当中三部分程序的独立结果。在所有结果中最为重要的是Trinity.fasta文件，这就是我们要的最终的拼接结果的文件，后续分析以此为基础，我们需要先评估拼接的质量高低，满足要求就开始下一步定量分析。 2.1 TrinityStats.pl结果统计/B313/public_software/trinityrnaseq-Trinity-v2.8.4/util/TrinityStats.pl &lt;result_fasta_file&gt; commands: /B313/public_software/trinityrnaseq-Trinity-v2.8.4/util/TrinityStats.pl ./Trinity.fasta output: total genes total transcripts Contig N50 Percent GC 265295 319932 2913 46.50 由上结果来看，拼接出来的转录本数量有点多，这样对于在时候做差异分析时不利，下一回可以考虑在指控的时候把minlen参数设高一些，这样子对于后期的分析比较有利。这里拼接的N50是2913，还不错。 2.2 cd-hit去冗余commands: cd-hit-est -i input.fasta -o output.fasta -c 0.90 -n 8 -T 40 output: 2.3 corset聚类去冗余commands: corset [options] bowtie.bam 注意使用python2.7环境 3. RSEM_bowtie2定量分析与mappingcommands: 1234567891011nohup /B313/public_software/trinityrnaseq-Trinity-v2.8.4/util/align_and_estimate_abundance.pl --transcripts cdhit_new_Trinity.fasta --seqType fq --est_method RSEM --aln_method bowtie --trinity_mode --prep_reference --output_dir rsem_outdir_a --left clean_a-BR-1_RRAS30349-V_1.fq --right clean_a-BR-1_RRAS30349-V_2.fq &gt;nohup_a.out 2&gt;&amp;1 &amp;nohup /B313/public_software/trinityrnaseq-Trinity-v2.8.4/util/align_and_estimate_abundance.pl --transcripts cdhit_new_Trinity.fasta --seqType fq --est_method RSEM --aln_method bowtie --trinity_mode --prep_reference --output_dir rsem_outdir_b --left clean_b-BR_RRAS27375-V_1.fq --right clean_b-BR_RRAS27375-V_2.fq &gt;nohup_b.out 2&gt;&amp;1 &amp;nohup /B313/public_software/trinityrnaseq-Trinity-v2.8.4/util/align_and_estimate_abundance.pl --transcripts cdhit_new_Trinity.fasta --seqType fq --est_method RSEM --aln_method bowtie --trinity_mode --prep_reference --output_dir rsem_outdir_c --left clean_c-BR-1_RRAS30350-V_1.fq --right clean_c-BR-1_RRAS30350-V_2.fq &gt;nohup_c.out 2&gt;&amp;1 &amp;nohup /B313/public_software/trinityrnaseq-Trinity-v2.8.4/util/align_and_estimate_abundance.pl --transcripts cdhit_new_Trinity.fasta --seqType fq --est_method RSEM --aln_method bowtie --trinity_mode --prep_reference --output_dir rsem_outdir_d --left clean_d-BR_RRAS30433-V_1.fq --right clean_d-BR_RRAS30433-V_2.fq &gt;nohup_d.out 2&gt;&amp;1 &amp;nohup /B313/public_software/trinityrnaseq-Trinity-v2.8.4/util/align_and_estimate_abundance.pl --transcripts cdhit_new_Trinity.fasta --seqType fq --est_method RSEM --aln_method bowtie --trinity_mode --prep_reference --output_dir rsem_outdir_e --left clean_e-BR_RRAS30435-V_1.fq --right clean_e-BR_RRAS30435-V_2.fq &gt;nohup_e.out 2&gt;&amp;1 &amp;nohup /B313/public_software/trinityrnaseq-Trinity-v2.8.4/util/align_and_estimate_abundance.pl --transcripts cdhit_new_Trinity.fasta --seqType fq --est_method RSEM --aln_method bowtie --trinity_mode --prep_reference --output_dir rsem_outdir_o --left clean_o-3-HD_RRAS47919-V_1.fq --right clean_o-3-HD_RRAS47919-V_2.fq &gt;nohup_o.out 2&gt;&amp;1 &amp; tips: Each group needs to align independently. results: 黄色框中是RSEM定量的一些中间文件，红色框中是结果输出文件夹 genes.results和isoforms.results分别是基于基因和转录本水平的定量结果，这两者的区别可以在下面两幅图中看出来。isoforms.results中包含了转录本ID，基因ID，转录本长度，有效长度，expected_count，TPM，FPKM和IsoPct（该转录本表达量占基因总表达量的百分比）。genes.results中的内容与之类似，只是少了IsoPct。 接下来我们可以将FPKM提取出来，然后进行聚类分析，查看一下现有的各组样品之间的一个相关性，这类可以采用clustermap来展示。 4. edgeR差异分析5. blastx比对6. GO_KEGG注释7. stem趋势分析8. 结果整理与解读9. 后记","link":"/2020/04/17/Bioinformatics/Yellowseabream-record/"},{"title":"Trimommatic_qualitycontrol","text":"1. 前言质量控制是在测序信息的分析开始之前的尤为重要的一步，因为后期的分析都是建立在这些数据的基础上的，所以前期数据的好坏直接影响了后期分析结果的质量高低、优良，因此做好质量控制非常重要。在二代测序中，测出来的片段都不长，而且还有一些PCR使用的接头也在其中，这些都会影响到后面的分析，因此我们要做的就是先把接头去掉，然后再筛选掉比较短的序列，比如使用Trinity进行拼接时，kmer的长度最小为50bp，小于这个阈值就无法拼接，因此在这个基础上，我们可以开始做质量控制了。 二代测序数据的特点：大量的短序列（150-250bp）、双末端测序、末端质量较低。因此，在利用我们的测序数据进行分析之前，首先需要过滤掉低质量的碱基与序列，以确保分析结果的准确性。 2. 二代测序一般质控步骤 切除尾端碱基质量小于指定值（一般为20）的碱基。可以简单的单碱基修剪，也即从末端开始进行删除，直到读取碱基质量高于20；也可以进行滑窗修剪，也即从末端开始以指定碱基数目的滑窗开始修剪，直到滑窗内碱基平均质量高于20。 去除末端修剪后长度小于指定值的reads。不同项目指定值不同，一般宏基因组去掉小于50bp的reads（50bp已不够产生k-mer），而扩增子测序则根据raw reads长度和PCR插入片段的长度来确定，例如V4区大概260bp，那么可以去掉双末端reads之和小于280bp的（否则不足以拼接）。 其他一些要求，例如去除含有N（即无法读取位点）过多的reads、去除完全重复的reads等。 3. trimmomatic介绍trimmomatic是一款用来处理illumina测序数据的工具，可以是单条的single reads，也可以是成对的pairend reads。支持压缩格式数据。功能和其他数据处理的程序都差不多，主要包括， 去除adapter序列以及测序中其他特殊序列； 采用滑动窗口的方法，切除或者删除低质量碱基 去除头部低质量以及N碱基过多的reads； 去除尾部低质量以及N碱基过多的reads； 截取固定长度的reads； 丢掉小于一定长度的reads； Phred 质量值转换 Trimmomatic 发表的文章至今已被引用了 2810 次，是一个广受欢迎的 Illumina 平台数据过滤工具。其他平台的数据例如 Iron torrent ，PGM 测序数据可以用 fastx_toolkit 、NGSQC toolkit 来过滤。Trimmomatic 支持多线程，处理数据速度快，主要用来去除 Illumina 平台的 Fastq 序列中的接头，并根据碱基质量值对 Fastq 进行修剪。软件有两种过滤模式，分别对应 SE 和 PE 测序数据，同时支持 gzip 和 bzip2 压缩文件。另外也支持 phred-33 和 phred-64 格式互相转化，现在之所以会出现 phred-33 和 phred-64 格式的困惑，都是 Illumina 公司的锅（damn you, Illumina!），不过现在绝大部分 Illumina 平台的产出数据也都转为使用 phred-33 格式了。 4. trimmomatic使用 简单使用1java-jar trimmomatic-0.30.jar PE -threads 20 -phred33 R1.fq R2.fq clean.R1.fq unpaired.R1.fq clean.R2.fq unpaired.R2.fq ILLUMINACLIP:TruSeq3-PE.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:36 参数解释|参数|解释||:—-|:—-||PE|选择测序数据的类型，是单端（SE）还是双端（PE）||-threads|线程数量，并发计算，提高效率||-phred33|一种数据格式，illumina测序数据的格式，也有-phred64，可以不选||ILLUMINACLIP:TruSeq3-PE.fa:2:30:10|切除adapter序列。参数后面分别接adapter序列的fasta文件：允许的最大mismatch数：palindrome模式下匹配碱基数阈值：simple模式下的匹配碱基数阈值。||LEADING|切除首端质量不足n的碱基（n由自己设定）||TRAILING|切除尾端质量不足n的碱基（n由自己设定）||SLIDINGWINDOW:4:15|滑窗修剪，一个Windows的size是4个碱基，其平均碱基质量小于15，则切除。||MINLEN|最小reads长度，筛去过短序列||CROP: |保留reads到指定的长度||HEADCROP: |在reads的首端切除指定的长度||TOPHRED33 |将碱基质量转换为pred33格式||TOPHRED64 |将碱基质量转换为pred64格式| 实操 nohup java -jar /B313/public_software/Quality_control/Trimmomatic-0.38/trimmomatic-0.38.jar PE -threads 40 a-BR-1_RRAS30349-V_1.fq a-BR-1_RRAS30349-V_2.fq clean_a-BR-1_RRAS30349-V_1.fq unpaired_a-BR-1_RRAS30349-V_1.fq clean_a-BR-1_RRAS30349-V_2.fq unpaired_a-BR-1_RRAS30349-V_2.fq ILLUMINACLIP:/B313/public_software/Quality_control/Trimmomatic-0.38/adapters/TruSeq3-PE.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:50 &amp; nohup java -jar /B313/public_software/Quality_control/Trimmomatic-0.38/trimmomatic-0.38.jar PE -threads 40 a-BR-1_RRAS30349-V_1.fq a-BR-1_RRAS30349-V_2.fq clean_a-BR-1_RRAS30349-V_1.fq unpaired_a-BR-1_RRAS30349-V_1.fq clean_a-BR-1_RRAS30349-V_2.fq unpaired_a-BR-1_RRAS30349-V_2.fq ILLUMINACLIP:/B313/public_software/Quality_control/Trimmomatic-0.38/adapters/TruSeq3-PE.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:110 &amp; 日志输出 结果输出 结果分析 由上图可以看出，最终输出的是4个文件，两个大文件是质控之后clean的数据，而另外两个小文件是被clean out的data，两者组合起来就是原来的数据。 剩余数据记录 nohup java -jar /B313/public_software/Quality_control/Trimmomatic-0.38/trimmomatic-0.38.jar PE -threads 40 b-BR_RRAS27375-V_1.fq b-BR_RRAS27375-V_2.fq clean_b-BR_RRAS27375-V_1.fq unpaired_b-BR_RRAS27375-V_1.fq clean_b-BR_RRAS27375-V_2.fq unpaired_b-BR_RRAS27375-V_2.fq ILLUMINACLIP:/B313/public_software/Quality_control/Trimmomatic-0.38/adapters/TruSeq3-PE.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:110 &amp; nohup java -jar /B313/public_software/Quality_control/Trimmomatic-0.38/trimmomatic-0.38.jar PE -threads 40 c-BR-1_RRAS30350-V_1.fq c-BR-1_RRAS30350-V_2.fq clean_c-BR-1_RRAS30350-V_1.fq unpaired_c-BR-1_RRAS30350-V_1.fq clean_c-BR-1_RRAS30350-V_2.fq unpaired_c-BR-1_RRAS30350-V_2.fq ILLUMINACLIP:/B313/public_software/Quality_control/Trimmomatic-0.38/adapters/TruSeq3-PE.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:110 &amp; nohup java -jar /B313/public_software/Quality_control/Trimmomatic-0.38/trimmomatic-0.38.jar PE -threads 40 d-BR_RRAS30433-V_1.fq d-BR_RRAS30433-V_2.fq clean_d-BR_RRAS30433-V_1.fq unpaired_d-BR_RRAS30433-V_1.fq clean_d-BR_RRAS30433-V_2.fq unpaired_d-BR_RRAS30433-V_2.fq ILLUMINACLIP:/B313/public_software/Quality_control/Trimmomatic-0.38/adapters/TruSeq3-PE.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:110 &amp; nohup java -jar /B313/public_software/Quality_control/Trimmomatic-0.38/trimmomatic-0.38.jar PE -threads 40 e-BR_RRAS30435-V_1.fq e-BR_RRAS30435-V_2.fq clean_e-BR_RRAS30435-V_1.fq unpaired_e-BR_RRAS30435-V_1.fq clean_e-BR_RRAS30435-V_2.fq unpaired_e-BR_RRAS30435-V_2.fq ILLUMINACLIP:/B313/public_software/Quality_control/Trimmomatic-0.38/adapters/TruSeq3-PE.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:110 &amp; nohup java -jar /B313/public_software/Quality_control/Trimmomatic-0.38/trimmomatic-0.38.jar PE -threads 40 o-3-HD_RRAS47919-V_1.fq o-3-HD_RRAS47919-V_2.fq clean_o-3-HD_RRAS47919-V_1.fq unpaired_o-3-HD_RRAS47919-V_1.fq clean_o-3-HD_RRAS47919-V_2.fq unpaired_o-3-HD_RRAS47919-V_2.fq ILLUMINACLIP:/B313/public_software/Quality_control/Trimmomatic-0.38/adapters/TruSeq3-PE.fa:2:30:10 LEADING:3 TRAILING:3 SLIDINGWINDOW:4:15 MINLEN:110 &amp; 备注：在做完了质控之后要进行质量评价，先使用fastqc，然后使用multiqc进行整合，命令如下： fastqc [-o output dir] [–(no)extract] [-f fastq|bam|sam] [-c contaminant file] seqfile1 .. seqfileN multiqc /B313/Zjunlin/raw_data/fastqc_1/ -o /B313/Zjunlin/raw_data/multi_qcraw/ 5. 总结Trimmomatic是一个专门的软件，用于进行质量控制，对于illumina的测序数据尤为有效。其实一般情况下我们是通过编程手动进行质控的，但是得益于广大生信开发工作者的努力，我们才有了这么方便的分析工具可以使用，让我们能够更加专注于生物数据的意义的分析，因此我们应该心怀感恩，和所有工作人员一起努力，为生物科学的发展贡献一份力量。 参考资料：1. https://bio.biocoder.cn/book/15/content/5/ 2. https://www.jianshu.com/p/21ad76e5577c 3. http://www.360doc.com/content/18/0111/12/19913717_721031558.shtml","link":"/2020/04/14/Bioinformatics/Trimommatic-qualitycontrol/"},{"title":"无参转录组分析","text":"这是一篇关于使用Trinity进行无参转录组分析的文章，首先会简单介绍转录组学，接着会介绍Trinity软件的使用，最后会记录相关的下游分析 一、转录组学概述： 转录组广义上指在某一生理条件下，细胞内所有转录组产物的集合，包括：mRNA、ncRNA、rRNA等；狭义上指所有mRNA的集合。 转录组测序的研究对象为特定细胞在某一功能状态下所能转录出来的所有RNA的总和，主要包括mRNA和ncRNA。 转录组具有时间特异性、组织特异性、空间特异性等特点。 类型 特点 分析策略 有参转录组 需要所研究的物种有组装注释质量较好基因组序列，一般来说比对效率达到70%以上才能满足后续分析。 比对—定量—差异分析—功能富集分析（下游分析） 无参转录组 缺乏相应的质量较高的基因信息，需要从零开始组装分析 组装—定量—差异分析—功能富集分析（下游分析） 下游分析包括：GO基因功能注释、KEGG通路分析、mRNA表达量计算、差异基因筛选、功能和通路富集分析、时间序列分析、共表达网络分析等 二、Trinity介绍与使用基本介绍 是由Broad Institute 开发的转录组de novo组装软件，由三个独立的软件模块组成：Inchworm （虫）（C++） 、Chrysalis （蛹）（C++）、Butterfly （蝶）（Java）。三个软件依次来处理大规模的RNA-seq的reads数据。 组成部分 作用 工作过程 Inchworm （虫）（C++） 序列延伸 a. 将 reads切为 k-mers (k bp长度的短片段)b. 利用Overlap关系对k-mers进行延伸 (贪婪算法)c. 输出所有的序列 (“contigs”) Chrysalis （蛹）（C++） 构建de Brujin graph a. 聚类所有相似区域大于k-1bp的 contigsb. 构图 (区分不同的 “components”)c. 将reads比对回 components，进行验证 Butterfly （蝶）（Java） 解图，列举转录本 a. 拆分graph 为线性序列b. 使用reads以及 pairs关系消除错误序列 Trinity使用1. 下载与安装trinity最新版本是v2.4.0 下载官网在：https://github.com/trinityrnaseq/trinityrnaseq/wiki 12345nohup wget -c https://github.com/trinityrnaseq/trinityrnaseq/archive/Trinity-v2.4.0.tar.gz 1&gt;trinity.o 2&gt;trinity.e &amp; # 下载tar -zxvf Trinity-v2.4.0.tar.gz # 解压make # 编译 2. 使用命令：Trinity --seqType fq --left reads_1.fq --right reads_2.fq --CPU 6 --max_memory 20G 几个重要参数介绍： --seqType 支持输入数据格式为 fq 或者 fa双端测序：--left为read1 --right为read2 多个样品的reads由逗号隔开，不允许出现空格（依版本而定）单端测序：--single 加上文件 多个样品的reads由逗号隔开，不允许出现空格（依版本而定）--CPU 软件所用CPU数量--max_memory 内存控制（组装过程中，jellyfish这一步是最耗费资源的一步 所以这个内存主要由jellyfish控制）--SS_lib_type RF 链特异性文库需要加上这个参数，其中RF表示文库的构建方法，点这了解，最好看看--no_run_butterfly 不进行butterfly步骤，在Chrysalis这一步完成后中断--no_run_quantifygraph 不进行quantifygraph这一步--output trinity的输出文件夹，默认在trinity_out_dir/Trinity.fasta--no_version_check 不汇报版本信息 可选参数Misc：--SS_lib_type reads的方向。成对的reads: RF or FR; 不成对的reads: F or R。在数据具有链特异性的时候，设置此参数，则正义和反义转录子能得到区分。默认情况下，不设置此参数，reads被当作非链特异性处理。FR: 匹配时，read1在5’端上游, 和前导链一致, read2在3’下游, 和前导链反向互补. 或者read2在上游, read1在下游反向互补; RF: read1在5’端上游, 和前导链反向互补, read2在3’端下游, 和前导链一致;--output 输出结果文件夹。默认情况下生成trinity_out_dir文件夹并将输出结果保存到此文件夹中。--min_contig_length 报告出的最短的contig长度。默认为200--jaccard_clip 如果两个转录子之间有UTR区重叠，则这两个转录子很有可能在de novo组装的时候被拼接成一条序列，称为融合转录子(Fusion Transcript)。如果有fastq格式的paired reads，并尽可能减少此类组装错误，则选用此参数。值得说明的是：1. 适合于基因在基因组比较稠密，转录子经常在UTR区域重叠的物种，比如真菌基因组。而对于脊椎动物和植物，则不推荐使用此参数; 2. 要求fastq格式的paired reads文件(文件中reads名分别以/1和/2结尾，以利于软件识别)，同时还需要安装bowtie软件用于reads的比对; 3. 单独使用具有链特异性的RNA-seq数据的时候，能极大地减少UTR重叠区很小的融合转录子; 4. 此选项耗费运算，若没必要，则不用此参数。--prep 仅仅准备一些文件(利于I/O）并在kmer计算前停止程序运行--no_cleanup 保留所有的中间输入文件--full_cleanup 仅保留Trinity fasta文件，并重命名成${output_dir}.Trinity.fasta--cite 显示Trinity文献引证和一些参与的软件工具--version 报告Trinity版本并推出 Inchworm 和 K-mer 计算相关选项：--min_kmer_cov 使用Inchworm来计算K-mer数量时候，设置的Kmer的最小值。默认为1--inchworm_cpu Inchworm使用的CPU线程数，默认为6和–CPU设置的值中的小值。 Chrysalis相关选项：--max_reads_per_graph 在一个Bruijn图中锚定的最大的reads数目，默认为200000--no_run_chrysalis 运行Inchworm完毕，在运行chrysalis之前停止运行Trinity--no_run_quantifygraph 在平行化运算quantifygrahp前停止运行Trinity Butterfly相关选项：--bfly_opts Butterfly额外的参数--max_number_of_paths_per_node 从node A -&gt; B,最多允许多少条路径。默认为10--group_pairs_distance 最大插入片读长度，默认为500--path_reinforcement_distance 延长转录子路径时候，reads间最小的重叠碱基数。默认PE:75; SE：25--no_triplet_lock 不锁定triplet-supported nodes--bflyHeapSpaceMax 运行Butterfly时java最大的堆积空间，默认为20G--bflyHeapSpaceInit java初始的堆积空间，默认为1G--bflyGCThreads java进行无用信息的整理时使用的线程数，默认由java来决定--bflyCPU 运行Butterfly时使用的CPU线程数，默认为2--bflyCalculateCPU 计算Butterfly所运行的CPU线程数，由公式80% * max_memory / maxbflyHeapSpaceMax 得到--no_run_butterfly 在Chrysalis运行完毕后，停止运行Butterfly 结果 最终会得到一个Trinity.fasta的文件，在同文件夹下还会有其他的中间过程产生的文件，其中这个文件最为重要，是最终的组装结果 Trinity中自带脚本 for DifferentialExpression utils 三、下游分析（有待完善）1. GO功能注释2. KEGG通路分析3. 差异基因分析与筛选4. 功能和通路富集分析5. 结果可视化（火山图、箱线图、MAplot、pheatmap等）参考资料 https://github.com/trinityrnaseq/trinityrnaseq/wiki # 官方文档 https://www.meiwen.com.cn/subject/bnsokctx.html # 介绍博客 http://www.360doc.com/content/16/0910/15/35684706_589802384.shtml # 无参转录组分析 https://www.omicsclass.com/article/295 # 转录组学知识点 https://www.meiwen.com.cn/subject/tsutnqtx.html # 可视化分析（重点火山图） https://blog.csdn.net/niuhuihui_fei/article/details/72723781 # 介绍博客（Trinity参数）","link":"/2019/10/04/Bioinformatics/无参转录组分析/"},{"title":"fastqc_multiqc结果解读","text":"1. 前言在得到了测序结果之后，我们需要评估一下测序的质量，因此我们需要对测序的数据进行统计评价，这里采用的软件组合就是fastqc和multiqc，fastqc用于对每一组的测序结果进行评价并且输出html结果文件，但是当同时有比较多组的测序数据时，一份份html文件去翻阅是非常麻烦的，此时multiqc就派上用场了，multiqc可以对这些结果进行整合，生成一份html文件，方便查阅。 为什么要进行质量评价呢？因为我们后期的所有分析都是基于测序数据展开的，测序数据质量的高低直接影响了我们分析的结果，因此在所有分析步骤之前，我们必须要对测序的数据的质量进行评价，只有达到我们的要求的测序数据才能用于下一步分析。 2. fastqc使用fastqc是用于统计评估测序数据质量的常用软件，使用命令如下： fastqc [-o output dir] [--(no)extract] [-f fastq|bam|sam] [-c contaminant file] seqfile1 .. seqfileN nohup fastqc -o ./fastqc_70/ -f fastq clean_* &amp; 参数解释：|parameters|explanation||:—-|:—-||-o|output dir，选择输入结果的文件夹||-f|输入文件的格式，支持bam/sam/bam_mapped/sam_mapped/fastq||-h|帮助文档，查看参数和使用方式| 还有很多参数未一一列出 结果输出： 输出结果每组都有两个文件，一个是html，一个是zip文件。 3. multiqc使用命令如下：multiqc /B313/Zjunlin/raw_data/fastqc_70/ -o /B313/Zjunlin/raw_data/multi_qcraw/ 参数如下： 结果输出： 4. multiqc结果解读 fastqc的结果解读参数与multiqc一致，所以解读一份就好了。 4.1. General Statistics：所有样本数据基本情况统计%Dups——重复reads的比例 %GC——GC含量占总碱基的比例，比例越小越好 Length——测序长度 M Seqs——总测序量（单位：millions） 4.2. Sequence Counts：序列数量统计横坐标——序列的数量 纵坐标——样本 unique reads——蓝色 Duplicate reads——黑色 由图可以看出测序中的一些重复水平，这个重复水平和测序深度以及序列本身的表达情况有关。 4.3. Sequence Quality Histograms：每个read各位置碱基的平均测序质量横坐标——碱基的位置 纵坐标——质量分数 质量分数=-10log10p（p代表错误率），所以当质量分数为40的时候，p就是0.0001。此时说明测序质量非常好。 绿色区间——质量很好； 橙色区间——质量合理； 红色区间——质量不好。 可以看到在下面的图中，所有的样本的quality scores都在绿色区间，所以质量很好。 4.4. Per Sequence Quality Scores：具有平均质量分数的reads的数量横坐标——平均序列质量分数 纵坐标——reads数 绿色区间——质量很好 橙色区间——质量合理 红色区间——质量不好 当峰值小于27时——warning 当峰值小于20时——fail 由此图中可以看出低质量reads占整体reads的比例（估算各颜色区域曲线下面积）。如下图中，几乎所有数值都在绿色区间，所以质量非常好，但是如果存在50%的counts在红色区间，则表示测序结果很差，不可用。 4.5. Per Base Sequence Content：每个read各位置碱基ATCG的比列对所有reads的每一个位置，统计ATCG四种碱基的分布， 横坐标——碱基位置， 纵坐标——样本。 %A——绿色 %G——紫色 %C——蓝色 %T——红色 如果ATGC在任何位置的差值大于10%——warning 如果ATGC在任何位置的差值大于20%——fail reads每个位置的颜色显示由4种颜色的比例混合而成，哪一个碱基的比例大，则趋近于这个碱基所代表的颜色。正常情况下每个位置每种碱基出现的概率是相近的。如果ATGC在任何位置的差值大于10%—则warning（橙色），如果ATGC在任何位置的差值大于20%则fail（红色）。在图中可以看出，大概在前9bp的部分，颜色非常不均匀，也就是表示存在AGCT四种碱基的比例差异巨大的地方，这可能有过表达的序列的污染，所以12个样本都是fail。 4.6. Per Sequence GC Content：reads的平均GC含量横坐标——GC含量百分比 纵坐标——数量 正常的样本的GC含量曲线会趋近于正态分布曲线，曲线形状的偏差往往是由于文库的污染或是部分reads构成的子集有偏差（overrepresented reads）。形状接近正态但偏离理论分布的情况提示我们可能有系统偏差。 偏离理论分布的reads超过15%时——warning 偏离理论分布的reads超过30%时——fail 由下图可以看出，有6个样本与理论分布偏离超过了15%，有2个偏离超过了30%，有4个在合理的偏离度内。 4.7. Per Base N Content：每条reads各位置N碱基含量比例当测序仪器不能辨别某条reads的某个位置到底是什么碱基时，就会产生“N”，统计N的比率。正常情况下，N值非常小。 横坐标——read中的位置 纵坐标——N的数量比 当任意位置的N的比例超过5%——warning 当任意位置的N的比例超过20%——fail 由图可以看出，所有的样本在所有的位置都是属于绿色区间且数值比例非常小，可以看出测序的质量很高。这应该和现在测序仪工艺的进步，电脑处理能力的提升有着很大的关系，现在的测序仪能够高质量的完成测序。 4.8. Sequence Length Distribution：序列样本长度的分布情况横坐标——序列长度 纵坐标——数量 reads长度不一致时——warning reads有0长度时——fail 在这里可以看出测序的读长，也能够分析得出所有的小片段的长度分布情况，依据此我们可以判断测序效果的高低。在这里的结果呈现中，所有的结果都是在150bp，这也是二代测序的特点，测序的读长段，但是片段非常多，也就是通过将基因序列高度片段化分别同时测序，由此提高测序速度。 4.9. Sequence Duplication Levels：每个序列的相对重复水平横坐标——每个序列的相对重复水平 纵坐标——在文库中的比例 当非unique的reads占总数的比例大于20%时——warning 当非unique的reads占总数的比例大于50%时——fail 测序深度越高，越容易产生一定程度的duplication，这是正常的现象，但如果duplication的程度很高，就提示我们可能有bias的存在。 4.10. Overrepresented sequences：文库中过表达序列的比例横坐标——过表达序列的比例 纵坐标——样本 过表达序列的比例&gt;0.1%——warning 过表达序列的比例&gt;1%——warning 一条序列的重复数，因为一个转录组中有非常多的转录本，一条序列再怎么多也不太会占整个转录组的一小部分（比如1%），如果出现这种情况，不是这种转录本巨量表达，就是样品被污染。这个模块列出来大于全部转录组1%的reads序列，但是因为用的是前100,000条reads，所以其实参考意义不大。 4.11.Adapter Content：接头含量横坐标——碱基位置 纵坐标——占序列的百分比 百分比&gt;5%——warning 百分比&gt;10%——fail 由图可以看出，所有的样本的接头含量均低于5%，因此结果还算比较良好。 参考资料https://www.jianshu.com/p/85da4dcc6020","link":"/2020/04/16/Bioinformatics/fastqc-multiqc结果解读/"},{"title":"shell_script","text":"前言shell是一种专门用于和计算机内核通信并控制内核工作的窗口程序，因此，要想真正的掌握Linux的使用，学习使用shell是必不可少的。bash是Linux中默认的shell，使用者众，同时还有很多其他shell，如csh/tsh/bsh/ash等，这些都是shell，shell script可以在一般的like Unix系统上运行。个人感悟是shell script还是非常高效方便的，因为我可以直接在脚本里使用我在命令行里使用的工具，就像是把所有指令写到一起，但是又能够采用编程的方式进行组织，工作效率plus，接下来就让我们学习一下shell script吧。 1. 一个简单的shell script1234567891011121314151617181920#!/bin/bash# author: Genening# date: 2019/12/16# version: 1.0# describe: This is a shell script that help you to get the full information from the the original RSEM files by your interest_trans_id. This script need three parameter, Just follow the guidance.id=$1target=$2output=$3cat ${id}|while read linedoif [ $output == \"\" ];then grep ${line} ${target} &gt;&gt; output.txtelse grep ${line} ${target} &gt;&gt; ${output}fidoneecho \"Your file is successfully extract from the target file!\" 上面是一个简单的shell script，用于通过id从另一个文件中提取出这些id及其完整信息，这在我做转录组分析的过程中是很重要的一个工具，而且效率还行。现在我们来分析一下这个脚本。 首先，我们可以看到的是，整个脚本大体分为两个部分。第一部分是上面的注释，#!/bin/bash这一行不是注释，而是声明，声明这个shell是采用bash这个shell来运行的，这一行很重要，这是脚本能够运行的基础，其余的几行就是注释了，这是一种个人习惯，记录一下相关信息，这个倒不一定一定要有，只不过有这个会比较规范，在后期如果需要重写或者其他人使用你的脚本的时候能够更明确，这里一般会记录作者、日期、版本、描述，还可以加上使用的环境变量、系统语系，这些都是挺重要的信息，如果能记上会更好。 接下来我们来看第二部分。第二部分就是脚本的正文了，在这里就有点类似我们普通的编程了，但是这里写的可以直接运行的命令，这个脚本不用经过编译就可以执行，这也是shell script和一般编程的最大区别，这个更像是以前的批处理命令，不过shell script又有更强大的一点，那就是shell script提供了循环、判断、选择、数组等这些结构供我们使用，让我们能够编辑出功能更为复杂的script来实现我们的功能。在上面的这个脚本中，首先是我们通过$1 $2 $3这3个指令获取到了从命令行里输入的3个参数，然后分别赋给了3个变量。接下来我们进入逻辑处理部分，在这部分里我们首先通过cat命令按行读取文件输出到标准输出，再使用管道符把这些行输出变成行输入进入while循环，通过read把输入赋给line这个变量，这样我们就可以按行处理文件了。再紧接着我们采用了一个判断语句，判断有没有“输出文件名”这个输入，如果是空那就执行第一个grep匹配命令，数据流重定向追加写入到output.txt这个文件中，如果不是空，那就执行第二个grep匹配命令，数据重定向到用户指定的文件名中，最后工作完成后就打印输出”Your file is successfully extract from the target file!”，提示用户任务完成。 通过这个简单的例子我们就可以一窥shell script的全貌了，一个shell script需要具备的有以下几部分： #!/bin/bash shell的声明，保证shell的正常运行 相关必要的注释，保证用户方便使用，注释信息可以包括作者、日期、版本、联系方式、描述、环境变量、系统语系等信息 用户变量输入或变量定义 逻辑结构，包括循环、条件判断、选择等逻辑关系 处理语句，这里是使用Linux中可以直接使用的命令来完成 上面列举的几部分就是一个shell script应该有的基本框架了，在刚开始学习的时候就要养成良好的编程习惯，虽然这算不上真正的编程语言，但是也要像其他编程一样，规范写代码，追求高效简约。 2. 创建一个shell script在Linux环境中创建一个shell script是很简单的，就是新建一个文件，命名以.sh结尾即可。（其实在Linux中后缀不是很必要的一个东西，因为运行的软件一般都是你使用的指令，但是这个后缀只是为了让你自己更加好分别这个文件的性质，所以一般还是写上，但是不写也不会影响脚本的运行，但是在windows中这个后缀就很重要了，因为Windows中用户一般都不是特别了解自己的电脑，电脑是通过后缀自动选择默认运行的软件的，所以没有后缀在Windows中是不能直接运行的。），创建的指令是touch name.sh，新建完了后，就可以进入文件进行编辑了，在Linux中有自带的文本编辑软件vim、vi，一般使用vim就可以了，这个对于编程会更加友好，因为这个编辑器会根据编程内容采用不同颜色标注，方便我们的编写、阅读。命令是vim name.sh，接下来就是编写部分了。 3. 变量在shell script中有默认的变量，比如$0、$1、$2….等等，这里分别对应的是如下的关系：这意味着，我们可以通过这些变量名从命令行里获得参数，分别可以按照位置获得。 4. 逻辑结构4.1 条件判断这里有基础语法和高级语法使用，如下： 1234567891011121314151617181920# 基本语法if [条件判断];then commandfi# 两判断if [条件判断];then commandelse commandfi# 多判断if [条件判断];then commandelseif [条件判断];then commandelse commandfi 其中多判断是可以一直扩展的，但是一直扩展下去，这个就有点太繁杂了，很是麻烦，所以还有下面这种多判断的方式： 1234567891011121314151617case $variable in var1) command ;; var2) command ;; var3) command ;; var4) command ;; *) #这里表示其他所有值 exit 1 ;;esac 此处可以套娃，只要你有需要，这里的case可以一直写。另外，这里有意思的是，shell script里面表示一个判断逻辑结束的方式是把单词倒过来写。 4.2 function这里的function是利用了复用的思想设计的，在我们编写shell script的过程中，因为有可能有些部分要重复写，当写的太多了的时候，一个是非常烦扰，另一个是代码会变得非常的冗长，所以这里可以采用编写成function的方法，编写在script开头，在接下来调用就可以了，格式如下： 123function func_name(){ command} 接下来只要调用func_name就可以了，这个格式倒是和c以及JavaScript的function有点像，anyway，应该还是挺好用的。 4.3 循环这里的循环也是分为不定循环和固定循环，有几种写法，如下： 1234567891011121314151617181920212223# 不定循环1while [condition]do commanddone# 不定循环2until [condition]do commanddone# 固定循环1for var in con1 con2 con3...do commanddone# 固定循环2for var in $(seq 1 n) #这里seq是sequence，指1到n的整数序列do commanddone 4.4 语法纠错一种不运行script就可以检查语法的方法，这个应该可以挺实用的，虽然我没有用过，如下： 5. 调用bash命令这一部分是shell script里面最cool的地方了，那就是常用的bash工具可以直接使用，比如我们的文本三大件——grep、sed、awk，这个用起来相当的酸爽，还有cat、wc、|这些都可以直接使用，这个简直太棒了，这样子就可以强强联手极大提高工作效率了，下面列举一些常用的bash的工具 grep——查询、搜索匹配 sed——插入、删除、替换 awk——提取整列、合并文件 wc——统计数量 cat——按行查看文件 ls——按行列举目录下文件 sort——排序 uniq——统计、去重 join——合并文件中相同部分的行，可选左、右、全连接3种连接方式 paste——合并文件 split——拆分文件 expand——把文件中的tab转换成空格，unexpand把空格转换成tab tee——把标准输出写入批量文件 tr——查询然后全局替换，有点像Word中的替换功能 col——将说明文本转化为纯文本，方便阅读(eg. man col|col -b&gt; file) One more thing在这里介绍一个有趣的shell script 12345678910111213141516171819202122232425262728293031323334conda () { if [ \"$#\" -lt 1 ]; then \"$CONDA_EXE\" $_CE_M $_CE_CONDA; else \\local cmd=\"$1\"; shift; case \"$cmd\" in activate | deactivate) __conda_activate \"$cmd\" \"$@\" ;; install | update | upgrade | remove | uninstall) OLDPATH=\"${PATH}\"; __add_sys_prefix_to_path; \"$CONDA_EXE\" $_CE_M $_CE_CONDA \"$cmd\" \"$@\"; \\local t1=$?; PATH=\"${OLDPATH}\"; if [ $t1 = 0 ]; then __conda_reactivate; else return $t1; fi ;; *) OLDPATH=\"${PATH}\"; __add_sys_prefix_to_path; \"$CONDA_EXE\" $_CE_M $_CE_CONDA \"$cmd\" \"$@\"; \\local t1=$?; PATH=\"${OLDPATH}\"; return $t1 ;; esac; fi} 仔细阅读以下这个shell script的代码，应该不难发现，这个就是我们现在常用的conda的命令运行的脚本了，平常我们使用conda进行install/upgrade/update/uninstall的时候，调用的应该就是这个脚本，现在我们能一窥conda的内部运行脚本，是不是非常cool，另外这些信息是通过set这个命令展示出来的。anyway，好好学习，天天向上！","link":"/2019/12/17/Linux/shell-script/"},{"title":"Python基础","text":"这是一篇关于python基础知识复习的博客，除了基础之外，还会涉及进阶以及算法设计。 第一部分 Python基础语法1. 认识Python1.1. Python简介Python的创始人为Guido van Rossum Python 的设计目标： 一门简单直观的语言并与主要竞争者一样强大 开源，以便任何人都可以为它做贡献 代码像纯英语那样容易理解 适用于短期开发的日常任务 Python设计哲学 优雅、明确、简单 Python开发者的哲学是：用一种方法，最好只有一种方法来做一件事 Python是完全面向对象的语言，在Python中一切皆对象 可扩展性：如果需要一段关键代码运行的更快或者希望某些算法不公开，可以把这部分程序用C或者C++编写，然后在Python程序中使用它们。 1.2. 第一个Python程序执行Python程序的三种方式：解释器、交互式运行、IDE运行 Python是一个格式非常严格的程序设计语言。Python2.x默认不支持中文。 ASCII字符只包含256个字符，不支持中文 Python2.x的解释器名称是Python Python3.x的解释器名称是Python3 2. 注释 注释的作用 使用自己熟悉的语言，在程序中对某些代码进行标注说明，增强程序的可读性 2.1. 单行注释 以 # 开头，# 右边的所有东西都被当做说明文字，而不是真正要执行的程序，只起到辅助说明作用1print(\"hello python\") # 输出'hello python' 为了保证代码的可读性，# 后面建议先添加一个空格，然后再编写相应的说明文字；为了保证代码的可读性，注释和代码之间 至少要有 两个空格。 2.2. 多行注释 要在 Python 程序中使用多行注释，可以用 一对 连续的 三个 引号(单引号和双引号都可以)123456\"\"\"这是一个多行注释在多行注释之间，可以写很多很多的内容……\"\"\"print(\"hello python\") 提示： 注释不是越多越好，对于一目了然的代码，不需要添加注释 对于 复杂的操作，应该在操作开始前写上若干行注释 对于 不是一目了然的代码，应在其行尾添加注释（为了提高可读性，注释应该至少离开代码 2 个空格） 绝不要描述代码，假设阅读代码的人比你更懂 Python，他只是不知道你的代码要做什么 2.3. 代码规范： Python 官方提供有一系列 PEP（Python Enhancement Proposals） 文档，其中第 8 篇文档专门针对 Python 的代码格式 给出了建议，也就是俗称的 PEP 8： 文档地址：https://www.python.org/dev/peps/pep-0008/ 谷歌有对应的中文文档：http://zh-google-styleguide.readthedocs.io/en/latest/google-python-styleguide/python_style_rules/ 3. 运算符3.1. 算数运算符 算数运算符是完成基本的算术运算使用的符号，用来处理四则运算，而“+”和“ * ”还可以用来处理字符串。 运算符 描述 实例 + 加 10 + 10 = 20 - 减 10 - 10 = 0 * 乘 10 * 20 = 200 / 除 10 / 20 = 0.5 // 取整除 返回除法的整数部分（商） 9 // 2 输出结果 4 % 取余数 返回除法的余数 9 % 2 = 1 ** 幂 又称次方、乘方，2 ** 3 = 8 3.2. 比较（关系）运算符 运算符 描述 == 检查两个操作数的值是否 相等，如果是，则条件成立，返回 True != 检查两个操作数的值是否 不相等，如果是，则条件成立，返回 True &gt; 检查左操作数的值是否 大于 右操作数的值，如果是，则条件成立，返回 True &lt; 检查左操作数的值是否 小于 右操作数的值，如果是，则条件成立，返回 True &gt;= 检查左操作数的值是否 大于或等于 右操作数的值，如果是，则条件成立，返回 True &lt;= 检查左操作数的值是否 小于或等于 右操作数的值，如果是，则条件成立，返回 True python2.x中判断 不等于 还可以使用&lt;&gt;运算符 python2.x中同样可以使用!=来判断不等于 3.3. 赋值运算符 在 Python 中，使用 = 可以给变量赋值。在算术运算时，为了简化代码的编写，Python 还提供了一系列的 与 算术运算符 对应的 赋值运算符，注意：赋值运算符中间不能使用空格。 运算符 描述 实例 = 简单的赋值运算符 c = a + b 将 a + b 的运算结果赋值为 c += 加法赋值运算符 c += a 等效于 c = c + a -= 减法赋值运算符 c -= a 等效于 c = c - a *= 乘法赋值运算符 c *= a 等效于 c = c * a /= 除法赋值运算符 c /= a 等效于 c = c / a //= 取整除赋值运算符 c //= a 等效于 c = c // a %= 取 模 (余数)赋值运算符 c %= a 等效于 c = c % a **= 幂赋值运算符 c *= a 等效于 c = c * a 3.4. 身份运算符身份运算符比较两个对象的内存位置。常用的有两个身份运算符，如下所述： 运算符 描述 示例 is 判断两个标识符是不是引用同一个对象 x is y，类似 id(x) == id(y) is not 判断两个标识符是不是引用不同对象 x is not y，类似 id(a) != id(b) 辨析 is 用于判断 两个变量引用的对象是否为同一个 == 用于判断 引用变量的值 是否相等 3.5. 成员运算符Python成员运算符测试给定值是否为序列中的成员。 有两个成员运算符，如下所述： 运算符 描述 in 如果在指定的序列中找到一个变量的值，则返回true，否则返回false。 not in 如果在指定序列中找不到变量的值，则返回true，否则返回false。 3.6. 逻辑运算符| 运算符 逻辑表达式 描述 and x and y 只有 x 和 y 的值都为 True，才会返回 True否则只要 x 或者 y 有一个值为 False，就返回 False or x or y 只要 x 或者 y 有一个值为 True，就返回 True只有 x 和 y 的值都为 False，才会返回 False not not x 如果 x 为 True，返回 False如果 x 为 False，返回 True 3.7. 运算符优先级 以下表格的算数优先级由高到最低顺序排列： 运算符 描述 ** 幂 (最高优先级) * / % // 乘、除、取余数、取整除 + - 加法、减法 &lt;= &lt; &gt; &gt;= 比较运算符 == != 等于运算符 = %= /= //= -= += = *= 赋值运算符 is is not 身份运算符 in not in 成员运算符 not or and 逻辑运算符 &lt;补&gt;程序执行原理 操作系统会首先让 CPU 把 Python 解释器 的程序复制到 内存 中 Python 解释器 根据语法规则，从上向下 让 CPU 翻译 Python 程序中的代码 CPU 负责执行翻译完成的代码Python 的解释器有多大？ 执行以下终端命令可以查看 Python 解释器的大小12345678# 1. 确认解释器所在位置$ which python# 2. 查看 python 文件大小(只是一个软链接)$ ls -lh /usr/bin/python# 3. 查看具体文件大小$ ls -lh /usr/bin/python2.7 4. 变量4.1. 变量定义 在 Python 中，每个变量 在使用前都必须赋值，变量 赋值以后 该变量 才会被创建 可以用 其他变量的计算结果 来定义变量 变量名 只有在 第一次出现 才是 定义变量1变量名 = 值 使用交互式方式，如果要查看变量内容，直接输入变量名即可，不需要使用print函数使用解释器执行，如果要输出变量的内容，必须要要使用print函数 4.2. 变量的类型 在 Python 中定义变量是 不需要指定类型（在其他很多高级语言中都需要），Python 可以根据 = 等号右侧的值，自动推导出变量中存储数据的类型 数据类型可以分为 数字型 和 非数字型 数字型 整型 (int)：Python3中的所有整数都表示为长整数。 因此，长整数没有单独的数字类型。 浮点型（float） 布尔型（bool） ：真 True 非 0 数 —— 非零即真，假 False 0。 复数型 (complex)：复数是由x + yj表示的有序对的实数浮点数组成，其中x和y是实数，j是虚数单位。 非数字型：有些运算符还支持这些数据类型，详见4.4.5.3 运算符。 字符串（str）：加号(+)是字符串连接运算符，星号(*)是重复运算符。 列表（list） 元组（tuple） 字典（dict） 提示：在 Python 2.x 中，整数 根据保存数值的长度还分为： int（整数） long（长整数） 使用 type 函数可以查看一个变量的类型 1In [1]: type(name) &lt;补&gt;不同类型变量之间的计算 数字型变量 之间可以直接计算 在 Python 中，两个数字型变量是可以直接进行 算数运算的 如果变量是 bool 型，在计算时 True 对应的数字是 1 False 对应的数字是 0 字符串变量 之间使用 + 拼接字符串 字符串变量 可以和 整数 使用 * 重复拼接相同的字符串 数字型变量 和 字符串 之间 不能进行其他计算&lt;补&gt;从键盘获取输入信息：input 在 Python 中可以使用 input 函数从键盘等待用户的输入 用户输入的 任何内容 Python 都认为是一个 字符串1字符串变量 = input(\"提示信息：\")","link":"/2019/09/29/Python/Python基础/"},{"title":"data_visualization","text":"开始学习数据可视化，在此记录，目前主要是先学Python中的matplotlib这个包，这个包和MATLAB中的作图很相似，具有很高的自定义性，等到这个学习的比较深入后再考虑了解拓展其他工具的作图。附上学习链接：https://matplotlib.org/gallery/index.html 直接到官网学习是最方便的，文档详细，demo生动。 补充 这是新发现的一张思维导图，非常清晰的展示了matplotlib的结构，非常helpful。 目录 1. 基础画图 2. 画图进阶 第一部分：基础画图 1. 通用画图步骤123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263① 引包：import matplotlib.pyplot as plt；② 创建画布：fig = plt.figure()；③ 添加分区：ax = fig.add_subplot(311) # 3个数字分别代表 行、列 及 本图所处位置；④ ax.axis([-5,5,0,1]) # 设置x轴最小、最大值，y轴最小、最大值；⑤ ax.scatter(xcord1,ycord1, s=30, c='red', marker='s') # 用于画散点图；⑥ ax.plot(x, y) # 用于画线图；⑦ plt.title(“this is a title”) # 图形添加标题；⑧ plt.xlabel('x')、plt.ylabel('y') # 分别对坐标轴添加标题⑨ plt.savefig(\"/path_way/img_name.jpg\") #调整好后，保存图片⑩ plt.show() # 显示图形；——————————————————————————————————————————————————————————# 第一步：导入工具包——matplotlib、numpy、matplotlib.pyplotimport matplotlibimport matplotlib.pyplot as pltimport numpy as np# 第二步：数据准备——将数据处理成对应图片所需的格式labels = ['G1', 'G2', 'G3', 'G4', 'G5']men_means = [20, 34, 30, 35, 27]women_means = [25, 32, 34, 20, 25]x = np.arange(len(labels)) # the label locations# 第三步：图片通用设置# Add some text for labels, title and custom x-axis tick labels, etc.ax.set_ylabel('Scores')ax.set_title('Scores by group and gender')ax.set_xticks(x)ax.set_xticklabels(labels)ax.legend()# 第四步：作图fig, ax = plt.subplots()rects1 = ax.bar(x - width/2, men_means, width, label='Men')rects2 = ax.bar(x + width/2, women_means, width, label='Women')width = 0.35 # the width of the barsdef autolabel(rects): \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\" for rect in rects: height = rect.get_height() ax.annotate('{}'.format(height), xy=(rect.get_x() + rect.get_width() / 2, height), xytext=(0, 3), # 3 points vertical offset textcoords=\"offset points\", ha='center', va='bottom')autolabel(rects1)autolabel(rects2)fig.tight_layout()# 第六步：确认完成后，以img_name.jpg文件名保存到目标路径下plt.savefig(\"D:/TECHENICLEARNING/Data_visualization/img_name.jpg\") # 导入.savefig()方法，添加路径，将图片保存到目标路径下### 这一步必须放在show之前，可以考虑先注释掉这一步，等到图片调整好后再取消注释保存图片！！！# 第五步：展示图片plt.show() # 调用.show()方法展示图片，以便修改 示例图片如下： 图中的绿色圈为后来添加上的，圈出的地方表示可以自定义 2. 散点图与折线图代码如下： 1234567891011121314151617181920212223242526272829303132333435363738# 导入工具包import matplotlibimport matplotlib.pyplot as pltimport numpy as np# 数据准备labels = ['G1', 'G2', 'G3', 'G4', 'G5']x = [20, 34, 30, 35, 27]y = [25, 32, 34, 20, 25]z = [[20,26],[34, 32],[30, 34],[35, 20],[27, 25]]# 初始化，全局标题fig = plt.figure()plt.title(\"plot and scatter\")# 第1幅散点图ax = fig.add_subplot(2, 2, 1)ax.scatter(x,y, s=30, c='red', marker='s')# 第2幅折线图ax2 = fig.add_subplot(2, 2, 2)ax2.plot(x)ax2.plot(y)# 第3幅折线图ax3 = fig.add_subplot(2, 2, 3)ax3.plot(z)# 第4幅折线与散点图ax4 = fig.add_subplot(2, 2, 4)ax4.scatter([1,2,3,4,5],y)ax4.plot([1,2,3,4,5],x)# 保存图片plt.savefig(\"D:/TECHENICLEARNING/Data_visualization/plot-scatter.jpg\")# 图片展示，会将上面所有的图都展示出来plt.show() 输出图片： 感悟：在这里是直接使用matplotlib自动画的折线图和散点图，做好的关键在与数据的准备，如果准备的数据好的话，可以直接调用.plot()输入一个变量画折线图，默认横轴为数组的序数；如果采用scatter画图的话，需要传入两个参数，第一个是x，一般是横轴，第二个是y，一般是因变量的值，需要注意这一点，参数数量要对。另外还有其他能够自定义图片的地方，后续会进一步介绍。 3. 柱状图代码如下： 1234567891011121314151617181920212223242526import matplotlibimport matplotlib.pyplot as pltimport numpy as npplt.figure(3)x_index = np.arange(5) #柱的索引x_data = ('A', 'B', 'C', 'D', 'E')y1_data = (20, 35, 30, 35, 27)y2_data = (25, 32, 34, 20, 25)bar_width = 0.35 #定义一个数字代表每个独立柱的宽度rects1 = plt.bar(x_index, y1_data, width=bar_width,alpha=0.4, color='b',label='legend1')#参数：左偏移、高度、柱宽、透明度、颜色、图例rects2 = plt.bar(x_index + bar_width, y2_data, width=bar_width,alpha=0.5,color='g',label='legend2')#参数：左偏移、高度、柱宽、透明度、颜色、图例#关于左偏移，不用关心每根柱的中心不中心，因为只要把刻度线设置在柱的中间就可以了plt.xticks(x_index + bar_width/2, x_data) #x轴刻度线plt.legend() #显示图例plt.title(\"bar\")plt.tight_layout() #自动控制图像外部边缘，此方法不能够很好的控制图像间的间隔plt.savefig(\"E:/Desktop/bar.png\") #格式采用.jpg或者.png都可以，注意的是，save图片需要再show之前，不然输出的图片为空白，可能原因是在show之后图片数据就清空了，所以保存后为空白plt.show() 图片输出： 感悟：在这里最好指定bar的宽度，否则默认的宽度画图非常奇怪，另外，如果要同时画多个柱形图在一个刻度上，需要指定好偏移度，避免bar相互重合，具体的参数可以看上面的设置。 4. 饼图代码如下： 12345678910111213141516171819202122import matplotlibimport matplotlib.pyplot as pltimport numpy as npx_data = ('A', 'B', 'C', 'D', 'E')y1_data = (20, 35, 30, 35, 27)y2_data = (25, 32, 34, 20, 25)colors = 'r','y','b','g','m'explode = (0, 0.1, 0, 0,0) # only \"explode\" the 2nd slice (i.e. 'E')fig = plt.figure()# plt.title(\"pie\")pie1 = fig.add_subplot(2,1,1)pie1.pie(y1_data, colors=colors,shadow=True, startangle=90, explode=explode, labels=x_data, autopct='%1.1f%%')pie1.axis('equal') # set the pie to be a circle.pie2 = fig.add_subplot(2,1,2)pie2.pie(y2_data, colors=colors,shadow=True, startangle=90, explode=explode, autopct='%1.1f%%', labels=x_data)pie2.axis('equal') # set the pie to be a circle.fig.savefig(\"pie.png\")plt.show() 图片输出： 感悟：这个饼状图是真的丑，颜色的透明度没有办法设置，而且默认不是圆形的，看起来实在难看，可以考虑有没有替代的方案。&gt;有.axis(‘equal’)这个参数可以设置为圆形。 补充 线条标记种类： 图片中颜色 如果这些颜色不够用的话，还可以这么做：（1）通过十六进制字符串 color=’#123456’指定或者使用合法的HTML颜色名字（’red’,’chartreuse’等）。(2)传入一个归一化到[0,1]的RGB元组，如color=(0.3,0.3,0.4) 第二部分：画图进阶1. 数学函数图像命令如下： 123456789101112131415161718192021222324252627282930import matplotlibimport matplotlib.pyplot as pltimport numpy as np# Data for plottingt = np.arange(-2.0, 2.0, 0.01)x = np.arange(-2.0, 2.0, 0.01)x2 = np.arange(0.1, 2.0, 0.1)s = np.sin(2 * np.pi * t)S = np.cos(2 * np.pi * t)y = 2*x*xz = np.log10(x2) + 1a = 4*x*x*x + x*x + xfig, ax = plt.subplots()ax.axis([-2, 2, -2, 4])ax.plot(t, s, label='y = sinx')ax.plot(t, S, label='y = cosx')ax.plot(x, y, label='y = x^2')ax.plot(x2, z, label='y = log10(x) + 1')ax.plot(x, a, label='y = 4x^3 + x^2 + x')ax.legend() # 显示图例，不调用默认不显示ax.set(xlabel='x axis', ylabel='y axis', title='About as simple as it gets, folks') # 通用设置ax.grid()fig.savefig(\"test.png\")plt.show() 输出图片： Be careful about that, in python, if you want to draw a image with mathmatics functions, you should import the pakage named numpy. 2. 虚线代码如下： 12345678910111213141516171819import matplotlibimport matplotlib.pyplot as pltimport numpy as npx = np.linspace(0, 10, 500)y = np.sin(x)fig, ax = plt.subplots()# Using set_dashes() to modify dashing of an existing lineline1, = ax.plot(x, y, label='Usinghahahha')line1.set_dashes([2, 2, 10, 2]) # 2pt line, 2pt break, 10pt line, 2pt break# Using plot(..., dashes=...) to set the dashing when creating a lineline2, = ax.plot(x, y - 0.2, dashes=[6, 2], label='Using the dashes parameter')ax.legend()plt.show() 图片输出： 3. 引入ggplot stylecode: 123456789101112131415161718192021222324252627282930313233343536373839404142import numpy as npimport matplotlib.pyplot as pltplt.style.use('ggplot') # use the line of code to import the ggplot style# Fixing random state for reproducibilitynp.random.seed(19680801)fig, axes = plt.subplots(ncols=2, nrows=2)ax1, ax2, ax3, ax4 = axes.ravel()# scatter plot (Note: `plt.scatter` doesn't use default colors)x, y = np.random.normal(size=(2, 200))ax1.plot(x, y, 'o')# sinusoidal lines with colors from default color cycleL = 2*np.pix = np.linspace(0, L)ncolors = len(plt.rcParams['axes.prop_cycle'])shift = np.linspace(0, L, ncolors, endpoint=False)for s in shift: ax2.plot(x, np.sin(x + s), '-')ax2.margins(0)# bar graphsx = np.arange(5)y1, y2 = np.random.randint(1, 25, size=(2, 5))width = 0.25ax3.bar(x, y1, width)ax3.bar(x + width, y2, width, color=list(plt.rcParams['axes.prop_cycle'])[2]['color'])ax3.set_xticks(x + width)ax3.set_xticklabels(['a', 'b', 'c', 'd', 'e'])# circles with colors from default color cyclefor i, color in enumerate(plt.rcParams['axes.prop_cycle']): xy = np.random.normal(size=2) ax4.add_patch(plt.Circle(xy, radius=0.3, color=color['color']))ax4.axis('equal')ax4.margins(0) # control the elements to be on the right place.plt.show() 图片输出 图片一下子的就精美多了，当然主要是替换了图片的通用设置，图片中元素的颜色还是需要自己搭配的。","link":"/2019/10/27/Python/data-visualization/"},{"title":"seaborn_visualization","text":"New package for data visualization. This package is base on the matplotlib, numpy, scipy , and pandas. The shining point is that this package is easier for people to create a beautiful picture for your dataset, this is essential for the data science. So let’s get it. This is the official URL: http://seaborn.pydata.org/ You can turn to there for help. 目录 可视化统计关系 绘制分类数据 数据分布 双变量分布 Step0：什么是seaborn Seaborn是基于matplotlib的图形可视化python包。它提供了一种高度交互式界面，便于用户能够做出各种有吸引力的统计图表。 Seaborn是在matplotlib的基础上进行了更高级的API封装，从而使得作图更加容易，在大多数情况下使用seaborn能做出很具有吸引力的图，而使用matplotlib就能制作具有更多特色的图。应该把Seaborn视为matplotlib的补充，而不是替代物。同时它能高度兼容numpy与pandas数据结构以及scipy与statsmodels等统计模式。 seaborn 有五种风格，分别为”darkgrid”, “whitegrid”, “dark”, “white”, “ticks” Step1: 数据来源 https://datahack.analyticsvidhya.com/contest/wns-analytics-hackathon-2018-1/https://datahack.analyticsvidhya.com/contest/enigma-codefest-machine-learning-1/ 这是从相关的数据科学竞赛网站上下载的数据，需要先注册并且参加竞赛才能下载数据。 Step2: 准备工作1234567891011121314151617181920212223242526272829### 导入package和依赖包import numpy as npimport matplotlib.pyplot as pltimport seaborn as snsimport pandas as pdfrom scipy import stats# 导入数据，探索数据，了解数据data1 = pd.read_csv(r'./data/hr.csv')# data1.head()# data1.describe()# data1.columns\"\"\"da.columnsOut[6]: Index(['employee_id', 'department', 'region', 'education', 'gender', 'recruitment_channel', 'no_of_trainings', 'age', 'previous_year_rating', 'length_of_service', 'KPIs_met &gt;80%', 'awards_won?', 'avg_training_score', 'is_promoted'], dtype='object')\"\"\"# seaborn使用测试test = sns.relplot(x=\"Views\", y=\"Upvotes\", data = data2) # sns.relplot()作图test.savefig('test.png') # 保存图片，通用操作plt.show() # 展示图片 1. 可视化统计关系 包括以下内容： Scatter plot (散点图) SNS.relplot Hue plot (Hue图) 数据： 散点图123456# 开始使用seabornscatters = sns.relplot(x='Views', y='Upvotes', data=data2)plt.title('scatters with seaborn')scatters.savefig('scatters.png')plt.show() 图片输出： relplot的参数： relplot(x=None, y=None, hue=None, size=None, style=None, data=None, row=None, col=None, col_wrap=None, row_order=None, col_order=None, palette=None, hue_order=None, hue_norm=None, sizes=None, size_order=None, size_norm=None, markers=None, dashes=None, style_order=None, legend=’brief’, kind=’scatter’, height=5, aspect=1, facet_kws=None, **kwargs) SNS.relplot是来自SNS类的relplot函数，SNS类是我们在上面与其他依赖项一起导入的一个seaborn类。这里，参数是x、y，数据有在X,Y轴上表示的变量和我们要分别画出来的数据点，通过图片，我们发现了views和upvotes之间的关系。 展示标签12scatters = sns.relplot(x='Views', y='Upvotes', hue='Tag', data=data2)# 增加hue（色调），可以展示标签 图片输出： Hue图 我们可以在色调(Hue)的帮助下在我们的图片中添加另一个维度，即在二维的基础上通过颜色来添加其他信息，通过为点赋予颜色来实现，每种颜色都有一些附加的意义。 1scatters = sns.relplot(x='Views', y='Upvotes', hue='Answers', data=data2) 图片输出： size标签1scatters = sns.relplot(x='Views', y='Upvotes', hue='Answers', data=data2) 图片输出： 2. 绘制分类数据 包括以下内容： 抖动图 Hue图 箱线图 小提琴图 Pointplot 数据： 抖动图1shake = sns.catplot(x=\"education\", y=\"avg_training_score\", jitter = True, data=df2) 图片输出： 改动jitter参数为False，可见下图： hue引入另一个维度1shake = sns.catplot(x=\"education\", y=\"avg_training_score\", hue='gender', data=df2) 引入了性别参数，可以看出数据中的性别差异 图片输出： 可以引入kind中的swarm参数，展示效果如下： 箱线图1shake = sns.catplot(x=\"education\", y=\"avg_training_score\", hue='gender', kind='box', data=df2) 图片输出： 箱线图（Boxplot）也称箱须图（Box-whisker Plot），是利用数据中的五个统计量：最小值、第一四分位数、中位数、第三四分位数与最大值来描述数据的一种方法，它也可以粗略地看出数据是否具有有对称性，分布的分散程度等信息，特别可以用于对几个样本的比较。 小提琴图1shake = sns.catplot(x=\"education\", y=\"avg_training_score\", hue='gender', kind='violin', data=df2) 图片输出： 小提琴图结合了箱线图和核密度估计程序，以提供更丰富的值分布描述。四分位数值显示在小提琴内部。当色调语义参数是二值时，我们还可以拆分小提琴，这也可能有助于节省绘图空间。 增加划分功能 1shake = sns.catplot(x=\"education\", y=\"avg_training_score\", hue='gender', kind='violin',split = True, data=data1) 图片输出： bar 结合 box1shake = sns.catplot(x=\"education\", y=\"avg_training_score\", hue='gender', kind='bar', data=data1) 图片输出： point图1shake = sns.catplot(x=\"education\", y=\"avg_training_score\", hue='gender', kind='point', data=data1) 图片输出： 3. 数据分布 数据分布图12tr = sns.distplot(data1.length_of_service)# tr = sns.distplot(data1.age) 通过displot()方法可以了解数据集的分布情况，也是一个不错的工具。 图片输出： 数据直方图1tr = sns.distplot(data1.length_of_service, kde=False, rug = True) 图片输出： 下面展示的转录组数据的分布 由此可以看出绝大部分的转录本都分布在1000bp左右，超过10000bp的转录本只有几个。 4. 双变量分布包括以下内容： Hex图 KDE 图 Boxen 图 Ridge 图 (Joy图) joinplot()默认为scatter1sns.jointplot(x=\"avg_training_score\", y=\"age\", data=data1, color='r', alpha = 0.2, kind='hex') 图片输出： 如果有时候要展示两个变量之间的关系，同时又想展示单个变量的分布时，可以用这种展示方法 直方图hex1sns.jointplot(x=\"age\", kind='hex', y=\"avg_training_score\", data=data1, color='b') 图片输出： KDE1sns.jointplot(x=\"age\", kind='kde', y=\"avg_training_score\", data=data1, color='b') 图片输出： heatmap()123456corrmat = data1.corr()f, ax = plt.subplots(figsize=(9, 6)) # figsize参数可以指定图片大小比例sns.heatmap(corrmat, vmax=.8, square=True)plt.savefig('./img/heatmap.png', dpi=1080) # dpi参数能够指定图片质量（像素） dpi参数能够指定图片质量（像素） figsize参数可以指定图片大小比例图片输出： heatmap中，每一个变量都是一种颜色，方便区分。 boxen图 catplot()1sns.catplot(x=\"age\", y=\"avg_training_score\", data=data1, kind=\"boxen\",height=4, aspect=2.7, hue = \"is_promoted\") 图片输出： 结语这一篇博客介绍了很多seaborn画图的图标，接下来需要做的就是在实际工作生活中应用这些画图的方式，并且可以尝试着更多的其他类型的图片，充分展示数据。继续学习！！！","link":"/2019/11/02/Python/seaborn-visualization/"},{"title":"数据结构与算法","text":"写在前面 这是篇为了记录计算机基础知识学习笔记的博客，希望自己能够充分利用博客这个工具，提高自己的学习效率和改善学习效果。另外，这也是在接下来的计算机二级考试的备考内容之一。这会是一篇持续更新的博客，希望能够写得完善一些。 1. 什么是算法？ 算法是指对解决方案的准确而完整的描述，简单来说，就是解决问题的步骤。 基本特征 具体描述 可行性 步骤可以实现，执行能够达到预期目的 确定性 步骤明确，每一步意义清晰 有穷性 有限步骤，有限时间内能完成 算法的复杂度 时间复杂度：执行算法所需要的计算工作量 空间复杂度：执行算法需要的内存空间 2. 数据结构的基本概念（1）什么是数据结构？ 数据结构值得是相互有关联的数据元素的集合。 逻辑结构 存储结构 （2）数据结构的表示1234B = (D, R)B表示数据结构D是数据元素的集合R是数据各元素之间的关系 基本概念 含义 根节点 数据结构中没有前件的节点 叶子节点 没有后件的节点 内部节点 除了根节点和叶子节点外的节点 ### （3）线性结构与非线性结构 基本概念 含义 —- —- 线性结构 非空，每一个节点最多有一个前件和一个后件 非线性结构 非空，每一个节点可以有多于一个前件或后件的数据结构 3. 线性表及其顺序储存结构（1）线性表 线性表即线性数据结构，表示如下 1(a1, a2, a3, ...,ai,...,an) 通常，线性表可以采用顺序存储和链接存储两种存储结构|基本概念|含义||—-|—-||顺序存储|存储地址是连续的、相邻的||链接存储|存储地址可以不连续，不过在逻辑上是连续的，由头指针和尾指针进行定位| 采用顺序存储是表示线性表最为简单的方法，这种顺序表示的线性表也称顺序表 4. 栈（Stack）与队列（Queue）(1)栈及其基本运算 栈是一种特殊的线性表，先进后出、后进先出原则，操作有三：入栈、退栈、读栈 （2）队列及其基本运算 队列也是一种特殊的线性表，先进先出、后进后出，运算多为指针头移动 循环队列：即最后一个元素的指针将指回第一个元素前一个空元素的位置，导致前后相连。 5. 线性链表（1）线性链表的基本概念 线性链表是指在线性表的链式存储结构，简称链表。占用空间除了数据域，还有指针域，故而会比较多空间。一般链表为单向链表，但是如果在每一项之中添加一个指向前一个元素的指针，那就可以变成双向链表。 类型 优点 缺点 顺序表 （1）可以随机存取表中任意节点（2）地址顺序即逻辑顺序，无需额外空间 （1）插入和删除的运算效率低下（2）储存空间不便于扩展（3）不便于储存空间动态分配 链表 （1）插入、删除只需移动指针，无需移动储存位置（2）空间易扩展，方便动态分配空间 需要额外的空间储存逻辑关系，存储密度比顺序表低 （2）循环链表 在单链表的第一个节点前增加一个表头节点，队头指针指向表头节点，最后一个节点的指针域的值由NULL改为指向表头节点，这样就形成了一个循环链表。 在循环链表中，只要知道了一个节点的位置，就可以从它出发访问到所有的节点，并且在链表为空时，链表中仍然还有表头节点这个节点，因此能够当成非空链表运算，因此统一了空链表与非空链表的运算。 6. 树与二叉树（1）树的基本概念 树（Tree）是一种简单的非线性结构。例如家族族谱关系等。 基本概念 含义 例子 父节点 在树结构中，每一个节点只有一个前件，即它的父节点，所有节点最终的父节点为根节点。 如从一棵树的所有枝丫都发源于树干，树干有个根。 子节点 每个节点可以有多个后件，这些后件即为子节点，没有后件的节点称为叶子节点 正如树的叶子是树的结构的末端，其后没有其他节点了 度 一个节点拥有的后件的个数称为该节点的度，所有节点中最大的度即为该树的度（注意，后件的后件不是该节点的后件）。 例如树的各个枝丫上有数量不同的枝丫或者树叶，这个数量就是目前枝丫的度 深度 定义根所在的层次为1，其他节点所在层次等于它的父节点所在层次+1，树最大的层次即为树的深度 如果从根算起，最多共经过4次转折到达了叶子的节点，那么这棵树的深度即为4+1=5 子树 在树中，以某一个节点为根构建的新的树，称为该树的子树 相当于树上的大树枝，如果把大树枝看成是起点的话，那这个构建的新的树就是原来树的子树 （2）二叉树 二叉树与树不同，但是与树的结构很相似。（我个人觉得，二叉树就是一种特殊的树结构） 特点如下 可以为空，空的二叉树没有节点，非空的二叉树有且只有一个根节点。 每个节点最多有两棵子树，即不存在度大于2的节点。 子树有左右之分，次序不能颠倒。 性质如下 在二叉树的第k层上最多有2^(k-1)个几点，k大于等于1。 深度为m的二叉树，最多有2^m-1个节点（比如深度为4的树最多有2^4-1=16-1=15个节点）。 任何一棵树，其中度为0的叶子节点总是比度为2的节点的数量多1个（可以自己分类讨论进行验证）。 具有n个节点的二叉树，其深度至少为n对2取对数运算的整数部分（记为d），则深度至少为d+1,即相当于是2^d-1=n。 具有n个节点的完全二叉树的深度为d+1（d含义同上）。 满二叉树是指除了最后一层外，其他层的节点都是度为2的节点。完全二叉树是指除了最后一层外，每一层的节点数均达到最大值，在最后一层上只缺少右边的若干节点的二叉树。 （3）二叉树的存储结构 在计算机中，二叉树通常采用链式存储结构。其存储的节点由数据域与指针域两部分组成，由于每一个节点可以有两个后件，所以每个节点需要有2个指针，分别为左指针域和右指针域。二叉树的链式存储结构也叫二叉链表。 遍历方法 含义 前序遍历 首先访问根节点，然后按照先左后右的原则一直访问到叶子节点，例子如下图前序遍历，遍历顺序为A B D H E I C F G。 中序遍历 首先遍历左子树，然后访问根节点，最后遍历右子树。例子如下图，遍历顺序为H D B E I A C G F。 后序遍历 先左子树，后右子树，最后根节点。如下图，遍历顺序为H D I E B G F C A。 7. 查找技术 概念 含义 分析 顺序查找 从线性表的第一个元素开始，逐个比较最终找出目标元素，查找停止 （1）最好的时候，第一个即为查找元素，比较次数为1。（2）最坏情况，最后一个才是目标元素，比较次数为n。综上，平均为(n+1)/2 二分法查找 在有序的线性表中，将查找元素与中间元素进行比较，以指数级速度缩小查找范围，最终锁定目标元素。 最好情况为比较1次，最差情况为比较log..n次（‘..’表示数字‘2’）。 8. 排序技术 概念 含义 分析 冒泡排序法 通过两两相邻的数据之间比较，不断调整大小顺序，不断的重复，直到所有的数据都有序为止 最坏情况，比较次数为(n-1)n/2 快速排序法 在待排序元素中选取一个数K（一般为第一个数），以K为标准，大于K的排在K后面，小于K的排在K前面，得到两个子表，对这两个子表进行同样的操作，直到所有的数均有序后停止。 最坏情况，比较次数为(n-1)n/2 简单插入排序法 把待排序的n个元素看成是有序表和无序表，最开始有序表只有1个元素，无序表有n-1个元素，接下来在无序表中选取元素插入有序表中的正确位置，重复操作直到所有元素都插入到了正确的位置即停止。 最多比较(n-1)n/2次 希尔排序 将n个元素分为a1个组（a &lt; n），在每一组之间进行简单排序，再将序列分为a2个组（a2 &lt; a1），重复操作，直到最后an=1，即只有一个分组时排序完成。 最多次数为n^r，其中（1&lt; r &lt; 2）。 简单选择排序法 先选出最小元素与第一个元素交换位置，剩下元素再找最小元素与第二个元素交换位置，直到所有排序完毕。 最多比较(n-1)n/2次 堆排序法 堆是一个比较特别的完全二叉树的结构，在这种特殊情况下进行特殊排序，称为堆排序法。 最多nlog..n次，其中‘..’表示数字‘2’。 堆是用数组表示的二叉树，在堆里面，只有数组元素的排列顺序，没有额外的逻辑数据，在某些条件下，堆的优势很明显。 结语 写到这，关于数据结构与算法的基础入门知识就是这些了，还有一些其他内容还要进一步学习。","link":"/2019/09/03/计算机基础/数据结构与算法/"},{"title":"无参转录组实战case_1","text":"这是来自实验室的数据，开始做无参转录组第一次尝试，在此记录过程。这个记录会详尽的记录在时间过程中遇到的问题以及一些学习的细节，但是为了避免文章过于冗长，采用了Markdown中插入HTML标签的方式进行折叠，需要查看即点击查看即可。 前情提要：分析流程 目录 序列拼接 去冗余 cd-hit-est 拼接结果参数统计 TrinityStats.pl 表达水平评估 align_and_estimate_abundance.pl 生成数量矩阵 abundance_estimates_to_matrix.pl 下游分析 差异表达分析 run_DE_analysis.pl 提取最长链 提取蛋白编码区 transcripts_to_best_scoring_ORFs.pl 功能注释 生成报告 数据可视化 1. 拼接序列 Trinity Trinity –seqType fq –max_memory 50G –output trinity_out_dir3 –left LJG-134_TTAGGC_L008_R1_001.fastq_trimmed,LJG-144_TGACCA_L008_R1_001.fastq_trimmed LJG-155_ACAGTG_L008_R1_001.fastq_trimmed,LJG-531_GCCAAT_L008_R1_001.fastq_trimmed,LJG-543new_CAGATC_L008_R1_001.fastq_trimmed,LJG-556_ACTTGA_L008_R1_001.fastq_trimmed,LJG-B1_CGATGT_L008_R1_001.fastq_trimmed,LJG-Xnew_GATCAG_L008_R1_001.fastq_trimmed,LJG-Y1_ATCACG_L008_R1_001.fastq_trimmed –right LJG-134_TTAGGC_L008_R2_001.fastq_trimmed,LJG-144_TGACCA_L008_R2_001.fastq_trimmed,LJG-155_ACAGTG_L008_R2_001.fastq_trimmed,LJG-531_GCCAAT_L008_R2_001.fastq_trimmed,LJG-543new_CAGATC_L008_R2_001.fastq_trimmed,LJG-556_ACTTGA_L008_R2_001.fastq_trimmed,LJG-B1_CGATGT_L008_R2_001.fastq_trimmed,LJG-Xnew_GATCAG_L008_R2_001.fastq_trimmed,LJG-Y1_ATCACG_L008_R2_001.fastq_trimmed –CPU 25 –min_kmer_cov 2+ nohup Trinity –seqType fq –max_memory 100G –min_glue 10 –full_cleanup –output trinity_out_dir –left LJG-134_TTAGGC_L008_R1_001.fastq_trimmed,LJG-144_TGACCA_L008_R1_001.fastq_trimmed,LJG-155_ACAGTG_L008_R1_001.fastq_trimmed,LJG-531_GCCAAT_L008_R1_001.fastq_trimmed,LJG-543all_CAGATC_L008_R1_001.fastq_trimmed,LJG-556_ACTTGA_L008_R1_001.fastq_trimmed,LJG-B1_CGATGT_L008_R1_001.fastq_trimmed,LJG-Xall_GATCAG_L008_R1_001.fastq_trimmed,LJG-Y1_ATCACG_L008_R1_001.fastq_trimmed –right LJG-134_TTAGGC_L008_R2_001.fastq_trimmed,LJG-144_TGACCA_L008_R2_001.fastq_trimmed,LJG-155_ACAGTG_L008_R2_001.fastq_trimmed,LJG-531_GCCAAT_L008_R2_001.fastq_trimmed,LJG-543all_CAGATC_L008_R2_001.fastq_trimmed,LJG-556_ACTTGA_L008_R2_001.fastq_trimmed,LJG-B1_CGATGT_L008_R2_001.fastq_trimmed,LJG-Xall_GATCAG_L008_R2_001.fastq_trimmed,LJG-Y1_ATCACG_L008_R2_001.fastq_trimmed –CPU 30 –min_kmer_cov 2 &amp; 补充：Trinity –show_full_usage_info 可以查看Trinity所有参数 –min_glue 5 # a small trick to get a better output 输出文件：trinity_out_dir/ 反馈如下 展开查看 （1）实测失败，报错如下 Error, not recognizing read name formatting: [GWZHISEQ02:77:C3VBWACXX:8:1203:2388:]If your data come from SRA, be sure to dump the fastq file like so: SRA_TOOLKIT/fastq-dump –defline-seq ‘@$sn[_$rn]/$ri’ –split-files file.sra Thread 2 terminated abnormally: Error, cmd: seqtk-trinity seq -A /home/gaodong/Zhangjlin/LJG-144_TGACCA_L008_R2_001.fastq &gt;&gt; right.fa died with ret 512 at /B313/public_software/trinityrnaseq-Trinity-v2.8.4/util/insilico_read_normalization.pl line 762.Error, conversion thread failed at /B313/public_software/trinityrnaseq-Trinity-v2.8.4/util/insilico_read_normalization.pl line 333.Error, cmd: /B313/public_software/trinityrnaseq-Trinity-v2.8.4/util/insilico_read_normalization.pl 已解决 通过 df -h查看到系统中内存不足，切换到新的文件夹后可以运行，同时将.fastq文件替换成了.fastq_trimed 新反馈如下 补充：Linux技巧 考虑到在运行过程中可能会突然断网，在运行程序是切换到后台运行，有两种方法，第一种是nohub，第二种是screen，具体的操作方法可以使用-h参数查看 （2）报错如下 Trinity run failed. Must investigate error above.There is insufficient memory for the Java Runtime Environment to continue.Cannot create GC thread. Out of system resources.An error report file with more information is saved as:/B313/gaodong/Zhangjlin/trinity_out_dir/read_partitions/Fb_0/CBin_23/c2318.trinity.reads.fa.out/hs_err_pid142493.logwarning, cmd: /B313/public_software/trinityrnaseq-Trinity-v2.8.4/util/support_scripts/../../Trinity –single “/B313/gaodong/Zhangjlin/trinity_out_dir/read_partitions/Fb_0/CBin_163/c16382.trinity.reads.fa” –output “/B313/gaodong/Zhangjlin/trinity_out_dir/read_partitions/Fb_0/CBin_163/c16382.trinity.reads.fa.out” –CPU 1 –max_memory 1G –run_as_paired –seqType fa –trinity_complete –full_cleanup –min_kmer_cov 2 failed with ret: 65280, going to retry. （3）报错如下 Thursday, October 24, 2019: 00:40:07 CMD: /B313/public_software/trinityrnaseq-Trinity-v2.8.4/trinity-plugins/BIN/ParaFly -c recursive_trinity.cmds -CPU 30 -v -shuffleNumber of Commands: 38759succeeded(1) 0.00258005% completed. Error occurred during initialization of VM (4)报错如下 Error encountered:: &lt;!—-CMD: /B313/public_software/trinityrnaseq-Trinity-v2.8.4/trinity-plugins/BIN/ParaFly -c /B313/gaodong/Zhangjlin/trinity_out_dir/read_partitions/Fb_0/CBin_163/c16382.trinity.reads.fa.out/chrysalis/butterfly_commands -shuffle -CPU 1 -failed_cmds failed_butterfly_commands.138858.txt 2&gt;tmp.138858.1571848809.stderr 拼接结果 2.1 去冗余 cd-hit-est命令如下 cd-hit-est -i input.fasta -o output.fasta -c 0.90 -n 8 -T 12 输出文件：output.fasta 反馈如下 展开查看 参数注释|参数|参数注释||:—-:|:—-:|| -i|input filename in fasta format, required, can be in .gz format|| -o|output filename, required||-c|sequence identity threshold, default 0.9this is the default cd-hit’s “global sequence identity” calculated as:number of identical amino acids or bases in alignmentdivided by the full length of the shorter sequence||-G|use global sequence identity, default 1if set to 0, then use local sequence identity, calculated as :number of identical amino acids or bases in alignmentdivided by the length of the alignmentNOTE!!! don’t use -G 0 unless you use alignment coverage controlssee options -aL, -AL, -aS, -AS||-b|band_width of alignment, default 20||-M|memory limit (in MB) for the program, default 800; 0 for unlimitted;||-T|number of threads, default 1; with 0, all CPUs will be used||-n|word_length, default 5, see user’s guide for choosing it||-l|length of throw_away_sequences, default 10||-t|tolerance for redundance, default 2||-d|length of description in .clstr file, default 20||if|set to 0, it takes the fasta defline and stops at first space||-s|length difference cutoff, default 0.0|if set to 0.9, the shorter sequences need to beat least 90% length of the representative of the cluster||-S|length difference cutoff in amino acid, default 999999if set to 60, the length difference between the shorter sequencesand the representative of the cluster can not be bigger than 60||-aL|alignment coverage for the longer sequence, default 0.0,if set to 0.9, the alignment must covers 90% of the sequence||-AL|alignment coverage control for the longer sequence, default 99999999if set to 60, and the length of the sequence is 400,then the alignment must be &gt;= 340 (400-60) residues||-aS|alignment coverage for the shorter sequence, default 0.0, if set to 0.9, the alignment must covers 90% of the sequence||-AS|alignment coverage control for the shorter sequence, default 99999999, if set to 60, and the length of the sequence is 400,then the alignment must be &gt;= 340 (400-60) residues||-A|minimal alignment coverage control for the both sequences, default 0, alignment must cover &gt;= this value for both sequences||-uL|maximum unmatched percentage for the longer sequence, default 1.0, if set to 0.1, the unmatched region (excluding leading and tailing gaps), must not be more than 10% of the sequence||-uS|maximum unmatched percentage for the shorter sequence, default 1.0, if set to 0.1, the unmatched region (excluding leading and tailing gaps), must not be more than 10% of the sequence||-U|maximum unmatched length, default 99999999, if set to 10, the unmatched region (excluding leading and tailing gaps), must not be more than 10 bases||-B|1 or 0, default 0, by default, sequences are stored in RAM, if set to 1, sequence are stored on hard drive !! No longer supported !!||-p|1 or 0, default 0, if set to 1, print alignment overlap in .clstr file||-g|1 or 0, default 0, by cd-hit’s default algorithm, a sequence is clustered to the first cluster that meet the threshold (fast cluster). If set to 1, the program will cluster it into the most similar cluster that meet the threshold(accurate but slow mode), but either 1 or 0 won’t change the representatives of final clusters||-sc|sort clusters by size (number of sequences), default 0, output clusters by decreasing length, if set to 1, output clusters by decreasing size||-sf|sort fasta/fastq by cluster size (number of sequences), default 0, no sorting, if set to 1, output sequences by decreasing cluster size, this can be very slow if the input is in .gz format||-bak|write backup cluster file (1 or 0, default 0)||-h| print this help| 再做一次结果统计 /B313/public_software/trinityrnaseq-Trinity-v2.8.4/util/TrinityStats.pl /B313/gaodong/Zhangjlin/trinity_out_dir/Trinity90.fasta 结果如下： 2.2 corset 聚类去冗余2.2.1 bowtie多序列比对得到.bam文件Step1： nohup bowtie2-build Trinity.fasta Trinity &gt;bow-nohup.out 2&gt;&amp;1 &amp; #建立索引bowtie2 -p 10 -x Trinity -1 input_1.fq -2 input_2.fq | samtools sort -O bam -@ 10 -o - &gt; 134.bam #开始比对，-p是线程数，-x是索引文件，-1 -2是双端比对的左右端，使用通配符把结果输入给samtools转换为.bam文件格式 2.2.2 corset使用 corset [options] bowtie.bam 需要在表达丰度定量操作中得到的bowtie.bam文件来进行下一步的操作，最终得到counts.txt和clusters.txt两个文件，可以进行下一步的拼接和差异表达分析。 补充通过counts.txt clusters.txt文件反向提取fasta文件 nohup python /home/gaodong/software_private/corset-1.09-linux64/Corset-tools-master/fetchClusterSeqs.py -i Trinity.fasta -t interest_cluster.csv -o Trinity-corset.fasta -c clusters.txt &gt;cluster-nohup.out 2&gt;&amp;1 &amp; # 说明：Trinity.fasta文件是转录本文件，interest_cluster.csv是一个cluster_id的单列数据，clusters.txt是corset输入的结果文件，最终输出文件为Trinity-corset.fasta文件，再重新进行表达丰度评估，接着做表达差异分析 注意：进入Python2.7环境运行 nohup corset -l ../rsem_outdir134/bowtie.bam ../rsem_outdir144/bowtie.bam ../rsem_outdir155/bowtie.bam ../rsem_outdir531/bowtie.bam ../rsem_outdir543all/bowtie.bam ../rsem_outdir556/bowtie.bam ../rsem_outdirB1/bowtie.bam ../rsem_outdirXall/bowtie.bam ../rsem_outdirY1/bowtie.bam &gt;corset-nohup.out 2&gt;&amp;1 &amp; 3. 拼接结果参数统计 TrinityStats.pl 统计结果包括统计在文件里面，各种长度的contig的数量，然后进行筛选。在筛选Contig的时候可以采用225作为一个阈值，这相当于是75个氨基酸的长度，这是针对于整个基因组寻找一些经典基因而采用的经验阈值，当然，也可以根据具体的分析目的进行设计。要熟悉在Linux中例如像Excel中常用的一些数据处理的指令。 命令如下 /B313/public_software/trinityrnaseq-Trinity-v2.8.4/util/TrinityStats.pl Trinity.fasta 结果反馈如下： 展开查看 数据处理： perl /home/gaodong/length_calculate.pl Trinity90.fasta &gt; Trinity90_counts #读取Trinity90.fasta文件中的length参数进行统计，并输出到Trinity90_counts中 awk ‘$4&gt;225’ Trinity90_counts &gt; Trinity90_counts_225 # 使用awk处理文件，筛选其中长度大于225的行 awk ‘{print $2}’ Trinity90_counts_225 &gt; Trinity90_counts_lens # 使用awk处理文件，筛选第二行 awk -F ‘[=]’ ‘{print $NF}’ Trinity90_counts_lens &gt; Trinity90_counts_list #使用awk处理文件，处理第二行中的字符串，输出所有数据到list里 sort -n -r number.txt -o number.txt #按照数值（-n）进行倒序（-r）排序，输出文件（-o） 参数如下： 展开查看 -f：忽略大小写；-b：忽略每行前面的空白部分；-n：以数值型进行排序，默认使用字符串排序；-r：反向排序；-u：删除重复行。就是 uniq 命令；-t：指定分隔符，默认分隔符是制表符；-k [n,m]：按照指定的字段范围排序。从第 n 个字段开始，到第 m 个字（默认到行尾）； 补充：提取最长链 get_longest_isoform_seq_per_trinity_gene.pl perl /B313/public_software/trinityrnaseq-Trinity-v2.8.4/util/misc/get_longest_isoform_seq_per_trinity_gene.pl Trinity90.fasta &gt; longest_transcript.fasta 输出文件：longest_transcript.fasta 坑：这里输入的文件第一行是统计数据，第二行是序列，两列一组，然后由长到短排序，如果要提取某一行序列数据，则设置提取偶数行即可。 4. 表达水平评估 align_and_estimate_abundance.pl命令如下：注意：每一对reads要单独mapping nohup /B313/public_software/trinityrnaseq-Trinity-v2.8.4/util/align_and_estimate_abundance.pl –transcripts ./glue5-trinity_out_dir/Trinity-corset.fasta –seqType fq –est_method RSEM –aln_method bowtie –trinity_mode –prep_reference –output_dir rsem_outdirY1 –left LJG-Y1_ATCACG_L008_R1_001.fastq_trimmed –right LJG-Y1_ATCACG_L008_R2_001.fastq_trimmed &gt;nohupY1.out 2&gt;&amp;1 &amp; nohup /B313/public_software/trinityrnaseq-Trinity-v2.8.4/util/align_and_estimate_abundance.pl –transcripts ./glue5-trinity_out_dir/Trinity-corset.fasta –seqType fq –est_method RSEM –aln_method bowtie –trinity_mode –prep_reference –output_dir rsem_outdirXall –left LJG-Xall_GATCAG_L008_R1_001.fastq_trimmed –right LJG-Xall_GATCAG_L008_R2_001.fastq_trimmed &gt;nohupXnew.out 2&gt;&amp;1 &amp; nohup /B313/public_software/trinityrnaseq-Trinity-v2.8.4/util/align_and_estimate_abundance.pl –transcripts ./glue5-trinity_out_dir/Trinity-corset.fasta –seqType fq –est_method RSEM –aln_method bowtie –trinity_mode –prep_reference –output_dir rsem_outdirB1 –left LJG-B1_CGATGT_L008_R1_001.fastq_trimmed –right LJG-B1_CGATGT_L008_R2_001.fastq_trimmed &gt;nohupB1.out 2&gt;&amp;1 &amp; nohup /B313/public_software/trinityrnaseq-Trinity-v2.8.4/util/align_and_estimate_abundance.pl –transcripts ./glue5-trinity_out_dir/Trinity-corset.fasta –seqType fq –est_method RSEM –aln_method bowtie –trinity_mode –prep_reference –output_dir rsem_outdir556 –left LJG-556_ACTTGA_L008_R1_001.fastq_trimmed –right LJG-556_ACTTGA_L008_R2_001.fastq_trimmed &gt;nohup556.out 2&gt;&amp;1 &amp; nohup /B313/public_software/trinityrnaseq-Trinity-v2.8.4/util/align_and_estimate_abundance.pl –transcripts ./glue5-trinity_out_dir/Trinity-corset.fasta –seqType fq –est_method RSEM –aln_method bowtie –trinity_mode –prep_reference –output_dir rsem_outdir543all –left LJG-543all_CAGATC_L008_R1_001.fastq_trimmed –right LJG-543all_CAGATC_L008_R2_001.fastq_trimmed &gt;nohup543all.out 2&gt;&amp;1 &amp; nohup /B313/public_software/trinityrnaseq-Trinity-v2.8.4/util/align_and_estimate_abundance.pl –transcripts ./glue5-trinity_out_dir/Trinity-corset.fasta –seqType fq –est_method RSEM –aln_method bowtie –trinity_mode –prep_reference –output_dir rsem_outdir531 –left LJG-531_GCCAAT_L008_R1_001.fastq_trimmed –right LJG-531_GCCAAT_L008_R2_001.fastq_trimmed &gt;nohup531.out 2&gt;&amp;1 &amp; nohup /B313/public_software/trinityrnaseq-Trinity-v2.8.4/util/align_and_estimate_abundance.pl –transcripts ./glue5-trinity_out_dir/Trinity-corset.fasta –seqType fq –est_method RSEM –aln_method bowtie –trinity_mode –prep_reference –output_dir rsem_outdir155 –left LJG-155_ACAGTG_L008_R1_001.fastq_trimmed –right LJG-155_ACAGTG_L008_R2_001.fastq_trimmed &gt;nohup155.out 2&gt;&amp;1 &amp; nohup /B313/public_software/trinityrnaseq-Trinity-v2.8.4/util/align_and_estimate_abundance.pl –transcripts ./glue5-trinity_out_dir/Trinity-corset.fasta –seqType fq –est_method RSEM –aln_method bowtie –trinity_mode –prep_reference –output_dir rsem_outdir144 –left LJG-144_TGACCA_L008_R1_001.fastq_trimmed –right LJG-144_TGACCA_L008_R2_001.fastq_trimmed &gt;nohup144.out 2&gt;&amp;1 &amp; nohup /B313/public_software/trinityrnaseq-Trinity-v2.8.4/util/align_and_estimate_abundance.pl –transcripts ./glue5-trinity_out_dir/Trinity-corset.fasta –seqType fq –est_method RSEM –aln_method bowtie –trinity_mode –prep_reference –output_dir rsem_outdir134 –left LJG-134_TTAGGC_L008_R1_001.fastq_trimmed –right LJG-134_TTAGGC_L008_R2_001.fastq_trimmed &gt;nohup134.out 2&gt;&amp;1 &amp; 输出文件如下 4.1 根据表达量（如FPKM）筛选将RSEM的结果中的FPKM提取出来组成新矩阵，根据平均值大于0.5的标准筛选出表达量高的转录本，在筛选完后，要根据这些高表达的转录本的id回到原来的RSEM结果文件中把这些转录本的信息提取出来，这里可以使用如下指令cat trans_id |while read line;do grep $line ../glue5-trinity_out_dir/Trinity-corset.fasta &gt;&gt; transcripts;done，这个指令可以按行读取trans_id中的id，然后在Trinity-corset.fasta文件中搜索这个id，再将结果追加写入transcript文件中，好处是使用Linux自带的命令工具，效率比较高。 5. 生成数量矩阵 abundance_estimates_to_matrix.pl命令如下： /B313/public_software/trinityrnaseq-Trinity-v2.8.4/util/abundance_estimates_to_matrix.pl –est_method RSEM –gene_trans_map none RSEM.isoforms.results 翻车提示：这里转换后，id对不上原来的id，这里是怎么回事？在进行匹配提取序列的时候失败，blast无法进行—–标注：这里没有翻车，对得上，尝试手动搜索后能够匹配上结果如下(自动输出文件到当前目录) 相关参数 展开查看 Usage: ./abundance_estimates_to_matrix.pl –est_method sample1.results sample2.results … or 如下./abundance_estimates_to_matrix.pl –est_method –quant_files file.listing_target_files.txt Note, if only a single input file is given, it’s expected to contain the paths to all the target abundance estimation files. Required:–est_method RSEM|eXpress|kallisto|salmon (needs to know what format to expect)–gene_trans_map the gene-to-transcript mapping file. (if you don’t want gene estimates, indicate ‘none’. Options:–cross_sample_norm TMM|UpperQuartile|none (default: TMM)–name_sample_by_basedir name sample column by dirname instead of filename–basedir_index default(-2)–out_prefix default: value for –est_method–quant_files file containing a list of all the target files. 6. 下游分析 6.1 差异表达分析 run_DE_analysis.pl注意，这一步需要第5步中输出文件*RSEM.isoform.counts.matrix***命令如下： /B313/public_software/trinityrnaseq-Trinity-v2.8.4/Analysis/DifferentialExpression/run_DE_analysis.pl –matrix RSEM.isoform.counts.matrix –method edgeR –output edgeR_results 实际使用： nohup /B313/public_software/trinityrnaseq-Trinity-v2.8.4/Analysis/DifferentialExpression/run_DE_analysis.pl –matrix rsem_outdir134/0.5-134RSEM.isoform.counts.matrix rsem_outdir531/0.5-531RSEM.isoform.counts.matrix –method edgeR –output 13vs53-edgeR_results &amp; 参数如下： 展开查看 补充 根据得到的上下调基因的id提取出对应的序列，应用的脚本是fetchClusterSeqs.py 命令如下： nohup python fetchClusterSeqs.py -i Trinity90.fasta -t upgenes.csv -o upgenes.fasta &amp;nohup python fetchClusterSeqs.py -i Trinity90.fasta -t downgenes.csv -o downgenes.fasta &gt;nohup2.out 2&gt;&amp;1 &amp; 6.2 提取最长链 get_longest_isoform_seq_per_trinity_gene.pl perl /B313/public_software/trinityrnaseq-Trinity-v2.8.4/util/misc/get_longest_isoform_seq_per_trinity_gene.pl Trinity90.fasta &gt; longest_transcript.fasta # 这里输出的是一个排序好的文件，最长链排在最前面，需要使用awk提取一下才是真正的最长链 输出文件：longest_transcript.fasta 坑：这里输入的文件第一行是统计数据，第二行是序列，两列一组，然后由长到短排序，如果要提取某一行序列数据，则设置提取偶数行即可。 7. 功能注释下载的软件：Trinotate、Trinity、sqlite、NCBI Blast、HMMER、signalP v4、tmhmm v2、RNAMMER 比对数据库：SwissProt、Uniref90、Pfam domains 标准化数据： 123makeblastdb -in uniprot_sprot.fasta -dbtype protmakeblastdb -in uniref90.fasta -dbtype prothmmpress Pfam-A.hmm blast比对（比对的数据库可以换成nr/Uniref90） 12345# search Trinity transcriptsblastx -query Trinity.fasta -db uniprot_sprot.fasta -num_threads 8 -max_target_seqs 1 -outfmt 6 -evalue 1e-5 &gt; blastx.outfmt6# search Transdecoder-predicted proteinsblastp -query transdecoder.pep -db uniprot_sprot.fasta -num_threads 8 -max_target_seqs 1 -outfmt 6 -evalue 1e-5 &gt; blastp.outfmt6 功能域 1hmmscan --cpu 8 --domtblout TrinotatePFAM.out Pfam-A.hmm transdecoder.pep &gt; pfam.log 信号肽 1tmhmm --short &lt; transdecoder.pep &gt; tmhmm.out 识别rRNA 123/TRINOTATE_HOME/util/rnammer_support/RnammerTranscriptome.pl --transcriptome Trinity.fasta --path_to_rnammer /usr/bin/software/rnammer_v1.2/rnammer# 输出：Trinity.fasta.rnammer.gff Tips：功能富集分析工具 7. Load transcripts and coding regions8. Output an Annotation Report 9. 数据可视化（见下一篇）","link":"/2019/10/21/Bioinformatics/无参转录组实测1/"},{"title":"latex_layout","text":"前言：最近参加比赛，要求是LaTeX排版，所以顺便学习了一下。在毕业论文排版的时候也能够用得上，也算是有点作用，在此记录一下学习笔记的模板，这里只记录代码了，至于效果暂时不展示了。 完整论文框架代码 第一部分：全局格式初始化设置123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869%!TEX program = xelatex %第一部分：全局格式初始化设置\\documentclass[10pt,onecolumn,a4paper]{article}%格式初始化，字号10pt；纸张A4；内容类型article\\usepackage{ctex} % 支持中文 \\songti %设置字体为宋体% \\zihao{-7} %设置字号，0对应初号\\usepackage{fontspec} \\setmonofont[Mapping={}]{DejaVu Sans Mono} %英文引号之类的正常显示，相当于设置英文字体，windows下用Consolas字体，Linux下用DejaVu Sans Mono字体 \\setmonofont{DejaVu Sans Mono}% \\setmainfont{Consolas} %设置文章主体部分的英文字体% 页面设置\\usepackage{geometry} % 设置页边距\\geometry{left = 2.2cm, right=2.2cm, top = 2.5cm, bottom=2.5cm}\\usepackage{fancyhdr} % 添加页眉页脚 % 设置 plain style 的属性 \\fancypagestyle{plain} \\fancyhf{} % 清空当前设置 % 设置页眉 (head) 备注，底下这三个设置的顺序不能随便调，结果不可预测 \\renewcommand{\\headrulewidth}{0.5pt} % 页眉与正文之间的水平线粗细 \\fancyhead[]{\\centering Latex学习笔记} %中间显示 \\fancyhead[LO]{页眉左边} % 出现在页眉左边 \\fancyhead[RO]{页眉右边}% 出现在页眉右边 %设置页脚（foot） \\fancyfoot[RO]{\\it Typesetting with \\LaTeX} % 页脚右侧斜体显示书名 \\renewcommand{\\footrulewidth}{0pt} \\pagestyle{fancy} % 选用 fancy style \\cfoot{\\thepage} % 页脚中央显示页码 %章节字体\\usepackage{titlesec} \\titleformat*{\\section}{\\centering\\bf\\large} %设置章节字体\\usepackage{indentfirst} % 首行缩进 \\setlength{\\parindent}{2em} % 设置首行缩进两字符%对齐方式\\makeatletter %使\\section中的内容左对齐\\renewcommand{\\section}{\\@startsection{section}{1}{0mm} {-\\baselineskip}{0.5\\baselineskip}{\\bf\\leftline}}\\makeatother %节标题左对齐，默认居中% 引入文章内容编辑的宏\\usepackage{amssymb} % symbol 符号\\usepackage{amsthm} % proof\\usepackage{courier} % 代码字体\\usepackage{graphicx,subfigure} % figures图片\\usepackage{xcolor,mdframed} % mdframed代码\\usepackage{amsmath} %数学公式\\usepackage{enumerate} % 项目编号\\usepackage{listings} % 项目列表\\usepackage{url} %引入超链接的包\\setcounter{tocdepth}{2} % 设置目录深度\\definecolor{mycolor}{RGB}{207, 226, 243} % 自定义颜色，可以设置代码背景\\usepackage{breqn} \\renewcommand\\d{\\mathop{}\\!\\mathrm{d}}\\usepackage{multirow} \\newtheorem{theorem}{Theorem} \\renewcommand{\\proofname}{\\emph{\\textbf{Proof}}} 第二部分：文章内容排版123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317%第二部分：文章内容排版\\begin{document} % 文章标题 \\title{\\LaTeX {} study notes} \\author{Geneningz}% \\today \\maketitle % 文章摘要 \\begin{onecolabstract} \\noindent\\textbf{摘要：}这篇文档的主要目的是学习LaTeX的基本排版方式，类比于Word的排版，排版需要考虑的因素如下：（1）纸张设置：纸张类型、页边距、页眉、页脚（2）内容设置：字号、字体、段间距、行间距、对齐方式、内容排列方式（一版、两版）（3）文章排版设置：标题、作者、摘要、关键字、一级标题、二级标题、三级标题、目录、加粗、斜体、引用、图片、公式、参考文献、附录\\par % 摘要内容，\\noindent要求在“摘要”二字之前不缩进 \\noindent\\textbf{关键字: } LaTeX; 论文排版; 格式设置%“\\par在段首，表示另起一行，“\\textbf{}”,花括号内的内容加粗显示 \\end{onecolabstract}% 文章目录\\tableofcontents% 从新页开始\\newpage% 章节内容\\section{概述}\\par Latex的排版类似于HTML和Markdown的排版，都是通过标签进行排版，在需要排版的内容之前打上不同标签就可以渲染出不同的效果。在LaTeX中的基本标签是begin{}和end{}标签，这两个是一组标签，展示如下： \\begin{mdframed}[backgroundcolor=mycolor,hidealllines=true] \\begin{verbatim} \\begin{mdframed}[backgroundcolor=mycolor,hidealllines=true] \\begin{verbatim} content... \\ end{verbatim} \\end{mdframed} \\end{verbatim} \\end{mdframed}\\section{初始化设置}\\subsection{纸张}\\par 在文章的纸张类型中，对于学术论文，纸张为A4纸，至于其他类型的排版，可以根据实际的需求进行选择，比如书籍的排版可选择更大或者更小的纸张。\\subsubsection{学术论文}\\subsubsection{书籍}\\subsubsection{杂志}\\subsection{纸内设置}\\par 纸内的设置包括页边距、页脚、页眉、栏数，而这些设置都在文章开始前的初始化进行了声明，声明方式主要是\\ usepackage{package}，这些设置可以如下：\\begin{mdframed}[backgroundcolor=mycolor,hidealllines=true] \\begin{verbatim} \\usepackage{geometry} % 设置页边距 \\geometry{left = 2.2cm, right=2.2cm, top = 2.5cm, bottom=2.5cm} \\usepackage{amssymb} % symbol \\usepackage{amsthm} % proof \\usepackage{courier} % 代码字体 \\usepackage{graphicx,subfigure} % figures \\usepackage{xcolor,mdframed} % mdframed \\usepackage{amsmath} \\usepackage{fancyhdr} % 添加页眉页脚 \\usepackage{titlesec} \\titleformat*{\\section}{\\centering\\bf\\large} %设置章节字体 \\usepackage{indentfirst} % 首行缩进 \\setlength{\\parindent}{2em} % 设置首行缩进两字符 \\end{verbatim}\\end{mdframed}\\section{文章内容框架}\\subsection{题目、作者、时间}\\par 作为一篇学术论文，论文题目、作者、写作时间（发表时间）这些都是基本的信息，是文章的第一部分内容，这是必需的。没有这些内容，论文就是不完整的。代码如下： \\begin{mdframed}[backgroundcolor=mycolor,hidealllines=true] \\begin{verbatim} \\title{content...} \\author{names} \\today \\maketitle %这一句是为了以上的设置运行，真正显示出来 \\end{verbatim} \\end{mdframed}\\subsection{摘要、关键词}\\par 在LaTeX中也有直接的标签是设置这两个的，直接设置时会显示默认格式，也就是会出现“摘要”二字居中，这种格式是偏向于英文论文的习惯，但是对于中文学术论文的排版习惯会像这篇笔记的开始的摘要排版格式，这种设置的方式如下： \\begin{mdframed}[backgroundcolor=mycolor,hidealllines=true] \\begin{verbatim} %1. 中文论文习惯 \\begin{onecolabstract} \\noindent\\textbf{摘要：}摘要内容 % 摘要内容，\\noindent要求在“摘要”二字之前不缩进 \\noindent\\textbf{关键字: } LaTeX; 论文排版; 格式设置 %“\\par在段首，表示另起一行，“\\textbf{}”,花括号内的内容加粗显示 \\end{onecolabstract} %2. 英文习惯 \\begin{abstract} 摘要内容 \\noindent\\textbf{关键字: } LaTeX; 论文排版; 格式设置 %“\\par在段首，表示另起一行，“\\textbf{}”,花括号内的内容加粗显示 \\end{abstract} \\end{verbatim} \\end{mdframed}\\subsection{标题}\\par 文章的标题分为好几个级别，在LaTeX中采用\\ section表示标题，这里是默认一级标题，二级标题及三级标题分别在前面累加sub即可，代码如下： \\begin{mdframed}[backgroundcolor=mycolor,hidealllines=true] \\begin{verbatim} \\section %一级标题 \\subsection %二级标题 \\subsubsection %三级标题 \\end{verbatim} \\end{mdframed}\\subsection{目录}\\par 在LaTeX中引入目录非常简单，在书写文章的时候，我们采用了section来标记标题，在标记完毕后，我们只需要在任意位置添加tableofcontents就可以插入目录了，代码如下： \\begin{mdframed}[backgroundcolor=mycolor,hidealllines=true] \\begin{verbatim} \\tableofcontents \\end{verbatim} \\end{mdframed}\\section{书写内容}\\subsection{文本段落}\\par 插入文本的标签如下，在此标签后的内容就是段落内容，以及还有其他，如下： \\begin{mdframed}[backgroundcolor=mycolor,hidealllines=true] \\begin{verbatim} \\par %表示段落 \\newpage %从新的一页开始 \\end{verbatim} \\end{mdframed}\\subsection{代码插入}\\par 在写一些比如数学建模比赛的论文时，有时候我们需要插入一些必要的代码进行展示，在LaTeX中有很好的支持，可以很完美的插入代码，代码如下： \\begin{mdframed}[backgroundcolor=mycolor,hidealllines=true] \\begin{verbatim} \\begin{mdframed}[backgroundcolor=mycolor,hidealllines=true] %backgroundcolor设置的是代码栏的颜色，mycolor参数在全局设置的时候进行了设置， %全局设置\\definecolor{mycolor}{RGB}{207, 226, 243}， %hidealllines=true表示无边框 \\begin{verbatim} 需要插入的代码内容 \\ end{verbatim} \\end{mdframed} \\end{verbatim} \\end{mdframed}\\subsection{列表插入}\\par 列表应该是在博客中经常使用的一种排版方式，或者出现在杂志中，但是对于学术论文来说，应该是不规范的。所以在学术论文中不应使用。 \\begin{itemize} \\item[-] good morning... \\item[-] good morning.... \\end{itemize} \\begin{enumerate} \\item good morning \\item good morning \\end{enumerate}\\par 代码如下： \\begin{mdframed}[backgroundcolor=mycolor,hidealllines=true] \\begin{verbatim} \\begin{itemize} \\item[-] good morning... \\item[-] good morning.... \\end{itemize} \\begin{enumerate} \\item good morning \\item good morning \\end{enumerate} \\end{verbatim} \\end{mdframed}\\subsection{图片}\\par 不管是在学术论文中还是普通博客中，图片是非常常见的数据展示形式，因此掌握插入图片是排版的基本要求。\\subsubsection{单张图片}\\par 单张图片： \\begin{figure}[h]%%图 \\centering %插入的图片居中表示 \\includegraphics[width=0.9\\linewidth]{figures/test1} %插入的图，包括JPG,PNG,PDF,EPS等，放在源文件目录下 \\caption{this is a figure.} %图片的名称 \\label{fig:test1} %标签，用作引用 \\end{figure}\\par 代码如下： \\begin{mdframed}[backgroundcolor=mycolor,hidealllines=true] \\begin{verbatim} \\begin{figure}[h]%%图 \\centering %插入的图片居中表示 \\includegraphics[width=0.9\\linewidth]{figures/test1} %插入的图，包括JPG,PNG,PDF,EPS等，放在源文件目录下，width是指插入图片的大小 \\caption{this is a figure.} %图片的名称 \\label{fig:test1} %标签，用作引用 \\end{figure} \\end{verbatim} \\end{mdframed}\\subsubsection{两栏图片}\\par 两栏图片： \\begin{figure}[h] \\begin{minipage}[t]{0.4\\linewidth}%并排放两张图片，每张占行的0.4，下同 \\centering %插入的图片居中表示 \\includegraphics[width=1.2\\textwidth]{figures/test1} \\caption{this is a figure3.}%图片的名称 \\label{fig:liuchengtu1}%标签，用作 \\end{minipage} \\hfill \\begin{minipage}[t]{0.4\\linewidth} \\centering \\includegraphics[width=1.2\\textwidth]{figures/test1} \\caption{this is a figure4.}%图片的名称 \\label{fig:liuchengtu2} \\end{minipage} \\end{figure}\\par 代码如下： \\begin{mdframed}[backgroundcolor=mycolor,hidealllines=true] \\begin{verbatim} \\begin{figure}[h] \\begin{minipage}[t]{0.4\\linewidth} %并排放两张图片，每张占行的0.4，下同 \\centering %插入的图片居中表示 \\includegraphics[width=1.2\\textwidth]{figures/test1} \\caption{this is a figure3.}%图片的名称 \\label{fig:liuchengtu1}%标签，用作 \\end{minipage} \\hfill \\begin{minipage}[t]{0.4\\linewidth} \\centering \\includegraphics[width=1.2\\textwidth]{figures/test1} \\caption{this is a figure4.}%图片的名称 \\label{fig:liuchengtu2} \\end{minipage} \\end{figure} \\end{verbatim} \\end{mdframed}\\subsection{表格}\\par 在文章中加入表格是非常常规的操作，一般需要展示符号、数据时使用。这里坑很大，不明白为什么在这里好端端的表格会浮上去，不和这一段一起，这是非常奇怪的，需要查找资料找出原因。最终原因是：LaTeX插入表格和图片是默认是会浮动的，这就导致我们排版完成后，图片和表格却不一定在我们想要的位置上，所以很烦恼，但是，我们可以通过begin{table}[h]中的h参数取消浮动，这样表格和图片就不会乱跑了。表格示例如下：\\begin{table}[h] % 这里的h应该是取消浮动的意思，太恶心了，找了很久 \\centering \\caption{basic structure} \\vspace{20pt} \\begin{tabular}{p{2cm}p{3cm}p{2.5cm}p{2.5cm}p{2.5cm}p{2.5cm}} \\hline Gene name &amp; Gene accession No. &amp; CDS length (bp) &amp; Protein size (aa) &amp; Protein MW (kDa) \\\\ \\hline 001 &amp; 01g009860.2 &amp; 819 &amp; 272 &amp; 31.34 \\\\ 002 &amp; 01g021730.2 &amp; 798 &amp; 265 &amp; 30.37 \\\\ 003 &amp; 01g094490.2 &amp; 630 &amp; 209 &amp; 24.58 \\\\ 004 &amp; 01g102740.2 &amp; 1242 &amp; 413 &amp; 46.94 \\\\ 005 &amp; 01g104900.2 &amp; 597 &amp; 198 &amp; 22.85 \\\\ 006 &amp; 02g036430.1 &amp; 1698 &amp; 565 &amp; 64.88 \\\\ 007 &amp; 02g061780.2 &amp; 735 &amp; 244 &amp; 28.23 \\\\ 008 &amp; 02g061870.1 &amp; 660 &amp; 219 &amp; 25.21 \\\\ 009 &amp; 02g061900.1 &amp; 915 &amp; 304 &amp; 34.61 \\\\ 010 &amp; 02g061910.1 &amp; 795 &amp; 264 &amp; 29.92 \\\\ \\hline \\end{tabular} \\label{bs2}\\end{table}\\par 代码如下： \\begin{mdframed}[backgroundcolor=mycolor,hidealllines=true] \\begin{verbatim} \\begin{table}[h] % 这里的h应该是取消浮动的意思，太恶心了，找了很久 \\centering \\caption{basic structure} \\vspace{20pt} \\begin{tabular}{p{2cm}p{3cm}p{2.5cm}p{2.5cm}p{2.5cm}p{2.5cm}} \\hline Gene name &amp; Gene accession No. &amp; CDS length (bp) &amp; Protein size (aa) &amp; Protein MW (kDa) \\\\ \\hline 001 &amp; 01g009860.2 &amp; 819 &amp; 272 &amp; 31.34 \\\\ 002 &amp; 01g021730.2 &amp; 798 &amp; 265 &amp; 30.37 \\\\ 003 &amp; 01g094490.2 &amp; 630 &amp; 209 &amp; 24.58 \\\\ 004 &amp; 01g102740.2 &amp; 1242 &amp; 413 &amp; 46.94 \\\\ 005 &amp; 01g104900.2 &amp; 597 &amp; 198 &amp; 22.85 \\\\ 006 &amp; 02g036430.1 &amp; 1698 &amp; 565 &amp; 64.88 \\\\ 007 &amp; 02g061780.2 &amp; 735 &amp; 244 &amp; 28.23 \\\\ 008 &amp; 02g061870.1 &amp; 660 &amp; 219 &amp; 25.21 \\\\ 009 &amp; 02g061900.1 &amp; 915 &amp; 304 &amp; 34.61 \\\\ 010 &amp; 02g061910.1 &amp; 795 &amp; 264 &amp; 29.92 \\\\ \\hline \\end{tabular} \\label{bs2} \\end{table} \\end{verbatim} \\end{mdframed}\\subsection{数学公式}\\par 论文里的公式以及相关符号是非常常见的，而latex对于数学公式的支持是非常高的，latex渲染的数学公式的美感是声名在外的，所以非常值得学习。\\begin{enumerate} \\item 行内插入 \\item 独立一行\\end{enumerate}\\par 示例1：$x=\\sum_i^ny_i$\\par 示例2：$$y=\\lim_xy_i+\\frac{1+5y}{2-y_i}$$\\par 示例3：\\begin{equation} y=\\lim_xy_i+\\frac{1+5y}{2-y_i}\\end{equation}\\subsection{链接}\\par 链接在论文书写中并不常用，但是对于博客来说很常用，但是一般我们在博客中也不采用latex写作，所以其实不是非常有必要学习这个，纯当是了解就好，代码如下： \\begin{mdframed}[backgroundcolor=mycolor,hidealllines=true] \\begin{verbatim} \\usepackage{url} %首先在开头引入包 \\url{https://genening.github.io/} \\end{verbatim} \\end{mdframed}\\par 超链接效果如此：\\url{https://genening.github.io/}\\section{参考文献}\\par 当论文写完后，千万别忘了写上参考文献，否则会被认为引用他人文献而不声明，即盗窃，是学术不端，所以一定要罗列，效果如下：\\begin{thebibliography}{99} \\bibitem{ref1}Zheng L, Wang S, Tian L, et al., Query-adaptive late fusion for image search and person re-identification, Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015: 1741-1750. \\bibitem{ref2}Arandjelović R, Zisserman A, Three things everyone should know to improve object retrieval, Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on, IEEE, 2012: 2911-2918. \\bibitem{ref3}Lowe D G. Distinctive image features from scale-invariant keypoints, International journal of computer vision, 2004, 60(2): 91-110. \\bibitem{ref4}Philbin J, Chum O, Isard M, et al. Lost in quantization: Improving particular object retrieval in large scale image databases, Computer Vision and Pattern Recognition, 2008. CVPR 2008, IEEE Conference on, IEEE, 2008: 1-8.\\end{thebibliography}\\par 代码如下： \\begin{mdframed}[backgroundcolor=mycolor,hidealllines=true] \\begin{verbatim} \\begin{thebibliography}{99} \\bibitem{ref1}参考文献1 \\bibitem{ref2}参考文献2 \\bibitem{ref3}参考文献3 \\bibitem{ref4}参考文献4 \\end{thebibliography} \\end{verbatim} \\end{mdframed}\\end{document}","link":"/2019/12/01/Practical skills/latex-layout/"},{"title":"pandas","text":"1. 前言Numpy和pandas是python当中两个非常重要的库，用于配合科学运算和矩阵处理。程序是算法与数据的结合，二者缺一不可，而numpy和pandas就是python中用于处理数据的利器，因此学习这两个package的使用非常重要。在使用matplotlib和seaborn进行数据可视化的时候，就会发现，所有作图的关键在于数据格式的把控，这两个package所提供的API就像是图表的模具，而整理好相应格式的数据就是原料，numpy和pandas就是整理数据的工具。接下来让我们开始学习这两个package吧。在这上一篇博客中我们学习了numpy，这一篇博客中我们来学习pandas。 这是一幅python_package的总览图，由此我们可以大概了解，有哪些很值得我们学习的package。 2. Pandas学习2.1 理解 Pandas是一个基于numpy构建的库，有这个库可以方便我们操作处理大型的数据文件，包括文件内容的增删改查等，这也是用python从事数据科学工作的基础。pandas中有一个dataframe数据对象，与numpy中的array类似，不过dataframe是二维的情况，这个对象对于我们处理数据、清洗数据非常有用，除此之外，pandas还能够非常好的支持时序数据的处理。 pandas是python的一个数据分析包，是基于NumPy的一种工具，该工具是为了解决数据分析任务而创建的。pandas纳入了大量库和一些标准的数据模型，提供了高效地操作大型数据集所需的工具。pandas提供了大量能使我们快速便捷地处理数据的函数和方法。 2.2 features 补充：pandas发展史 pandas最初被作为金融数据分析工具而开发出来，因此，pandas为时间序列分析提供了很好的支持。 Pandas的名称来自于面板数据（panel data）和python数据分析（data analysis）。panel data是经济学中关于多维数据集的一个术语，在Pandas中也提供了panel的数据类型。 2.3 数据结构 pandas中主要有两种数据结构，分别是series和dataframe。series专注于一维数组的处理，dataframe专注于二维数组，在这两个数据结构的基础上，pandas包提供了很多方便我们处理一维数组和二维数组的方法，接下来我们详细的了解一下这些方法。 3. series结构3.1 创建seriesSeries()是一维数组，可以通过数组和列表来创建，也可以通过字典来创建，如下示例： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364In [4]: pd.Series(np.random.randn(7))Out[4]:0 -0.2139141 -0.5188662 -0.2187653 0.9415374 1.2181885 1.6445236 0.527248dtype: float64In [5]: index = ['周一', '周二', '周三', '周四', '周五', '周六', '周日']In [6]: pd.Series(np.random.randn(7), index=index)Out[6]:周一 0.252932周二 0.019784周三 1.738989周四 0.918056周五 0.221937周六 -0.007848周日 1.168715dtype: float64In [7]: dic = {'周一': 13, '周二': 15, '周三': 18, '周四':19, '周五':33, '周 ...: 六': 12, '周日': 11}In [8]: pd.Series(dic)Out[8]:周一 13周二 15周三 18周四 19周五 33周六 12周日 11dtype: int64In [9]: pd.Series(index)Out[9]:0 周一1 周二2 周三3 周四4 周五5 周六6 周日dtype: objectIn [12]: tu = (1,3,4,3,5,3,5,353,53,53)In [13]: pd.Series(tu, name='apple')Out[13]:0 11 32 43 34 55 36 57 3538 539 53Name: apple, dtype: int64 Series支持的参数： class Series(data=None, index=None, dtype=None, name=None, copy=False, fastpath=False) data就是传入的数据，可以是array-like的数据，包括array/tuple/list，也可以是dict，其中如果传入的是dictionary，那dict的key值就会变成index索引，value就是默认的值。 3.2 Series操作我们从Series数据结构中取值，可以利用自定义的索引标签，也可以利用默认的位置索引。 12345678910111213In [17]: dic = {'周一': 13, '周二': 15, '周三': 18, '周四':19, '周五':33, ' ...: 周六': 12, '周日': 11}In [18]: dic_series = pd.Series(dic)In [19]: dic_series['周二']Out[19]: 15In [20]: dic_series[2]Out[20]: 18In [21]: dic_series[1]Out[21]: 15 我们可以通过索引或者默认的序号进行读取值。Series有很多其他实用的操作方法，在这就不一一展示了，在下面学习dataframe的时候，其中很多方法也同样适用于Series。 4. dataframe结构4.1 dataframe创建dataframe是pandas中非常重要的一类二维的数表形式的数据结构，这在我们处理数据的时候非常常用。dataframe的创建方法是通过pd.Dataframe()结合传入的数据完成的的。 Examples: 1234567891011121314151617181920212223242526272829303132333435363738In [23]: pd.DataFrame(np.random.randn(5,5))Out[23]: 0 1 2 3 40 -0.336786 -0.364133 -0.529895 0.604086 -0.0052131 -0.405479 -1.774013 -0.390448 0.503312 -1.1345152 0.191531 -0.299245 -0.965893 1.605850 0.9979853 -0.746576 1.980630 -0.203215 1.806443 0.4387694 -0.313353 0.742876 0.344750 -0.422681 0.741933In [24]: li_data = [[1,2,3,3], [2,3,45,3], [4,5,2,15]]In [25]: pd.DataFrame(li_data)Out[25]: 0 1 2 30 1 2 3 31 2 3 45 32 4 5 2 15In [30]: dic_data ={'A':[1,2,3,4], 'B':[2,3,4,5], 'C':[3,21,42,5], 'D':[3,3, ...: 3,2]}In [31]: pd.DataFrame(dic_data)Out[31]: A B C D0 1 2 3 31 2 3 21 32 3 4 42 33 4 5 5 2In [32]: pd.DataFrame(np.random.randn(5,5), index=['a','b','c','d','e'], col ...: umns=['A','B','C','D','E'])Out[32]: A B C D Ea 1.626478 -0.532220 0.070663 -0.525011 0.172189b 0.823984 0.524810 1.843861 0.705791 1.185754c -0.054392 -2.268764 -0.539865 -0.775574 0.125962d -3.127147 -1.948969 -1.025611 -0.547833 -1.346364e 0.784173 -1.027632 -0.320775 -1.707289 1.388755 12345678910111213141516In [26]: tu_data = ((1,2,3,3), (2,3,45,3), (4,5,2,15))In [27]: pd.DataFrame(tu_data)---------------------------------------------------------------------------ValueError Traceback (most recent call last)ValueError: DataFrame constructor not properly called!In [28]: tu_dataOut[28]: ((1, 2, 3, 3), (2, 3, 45, 3), (4, 5, 2, 15))In [29]: pd.Series(tu_data)Out[29]:0 (1, 2, 3, 3)1 (2, 3, 45, 3)2 (4, 5, 2, 15)dtype: object 通过上面的例子我们可以看出，dataframe接受的数据包括array/list/dict，但是不包含tuple类型，但是pd.Series()支持，所以要注意区分。另外，在创建的时候可以指定index和columns，也就是数据表的行索引和列索引。 4.2 DataFrame的操作方法dataframe是非常重要的一种二维数据结构，是pandas的精华，必须牢牢掌握。 基本属性——“.index/.columns/.values/.describe()”1234567891011121314151617181920import pandas as pd import numpy as np data3 = np.random.randn(4,5)index = ['q1', 'q2', 'q3', 'q4']columns = ['A', 'B', 'C', 'D', 'E']da_frame = pd.DataFrame(data3, index=index, columns=columns)print(da_frame)print('-'*50)print(da_frame.index)print(da_frame.columns)print('-'*50)print(da_frame.values)print('-'*50)print(da_frame.describe())print('-'*50) results: 123456789101112131415161718192021222324 A B C D Eq1 -0.412286 -0.821873 -1.748176 0.469806 -0.205702q2 -0.405459 -0.836046 0.712505 -0.076337 -2.325968q3 0.050220 -0.478746 1.684107 -0.550641 1.240566q4 1.563217 1.044990 0.609966 -0.820632 -0.267008--------------------------------------------------Index(['q1', 'q2', 'q3', 'q4'], dtype='object')Index(['A', 'B', 'C', 'D', 'E'], dtype='object')--------------------------------------------------[[-0.41228585 -0.82187332 -1.74817556 0.46980637 -0.20570188] [-0.405459 -0.83604631 0.71250499 -0.07633717 -2.32596807] [ 0.05021952 -0.47874572 1.6841073 -0.550641 1.24056574] [ 1.56321731 1.04498969 0.60996593 -0.82063159 -0.26700756]]-------------------------------------------------- A B C D Ecount 4.000000 4.000000 4.000000 4.000000 4.000000mean 0.198923 -0.272919 0.314601 -0.244451 -0.389528std 0.934927 0.894001 1.457871 0.566910 1.466948min -0.412286 -0.836046 -1.748176 -0.820632 -2.32596825% -0.407166 -0.825417 0.020431 -0.618139 -0.78174850% -0.177620 -0.650310 0.661235 -0.313489 -0.23635575% 0.428469 -0.097812 0.955406 0.060199 0.155865max 1.563217 1.044990 1.684107 0.469806 1.240566-------------------------------------------------- 通过df.describe()获取的数据描述信息，包含了每一列数据的总个数count、每一列平均值mean、每一列标准差std、每一列最小值min、每一列最大值max等信息。 排序——“.sorted_index(axis=0/1,by=’index_name’, ascending=True/False)” axis表示选择行还是列，行索引是0，列索引是1；ascending表示降序还是升序*123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263In [34]: print(data_frame.sort_index(axis=0, ascending=False)) #表示按行索引 ...: 垂直降序排序 A B C D9月 0.449812 -0.527682 -0.761678 0.2024138月 -2.048352 0.107163 -2.185515 -0.1069797月 -0.747604 -0.209058 1.402563 0.5254636月 0.584561 -0.707748 0.513108 0.9369995月 0.574763 0.123533 0.684210 -0.8464414月 -0.340184 -2.080648 0.312747 -0.5222353月 -0.935644 1.343663 0.617676 -0.9468232月 0.203031 -0.157956 -1.911171 0.8754801月 0.722934 0.111053 0.489415 -0.18996012月 -0.851821 0.830529 -0.854471 0.91938911月 -0.357822 -1.809013 -0.462926 -0.01732510月 0.025842 1.338289 -0.007398 0.392586In [35]: print(data_frame) A B C D1月 0.722934 0.111053 0.489415 -0.1899602月 0.203031 -0.157956 -1.911171 0.8754803月 -0.935644 1.343663 0.617676 -0.9468234月 -0.340184 -2.080648 0.312747 -0.5222355月 0.574763 0.123533 0.684210 -0.8464416月 0.584561 -0.707748 0.513108 0.9369997月 -0.747604 -0.209058 1.402563 0.5254638月 -2.048352 0.107163 -2.185515 -0.1069799月 0.449812 -0.527682 -0.761678 0.20241310月 0.025842 1.338289 -0.007398 0.39258611月 -0.357822 -1.809013 -0.462926 -0.01732512月 -0.851821 0.830529 -0.854471 0.919389In [36]: print(data_frame.sort_index(axis=1, ascending=False)) #表示按列索引 ...: 水平降序排序 D C B A1月 -0.189960 0.489415 0.111053 0.7229342月 0.875480 -1.911171 -0.157956 0.2030313月 -0.946823 0.617676 1.343663 -0.9356444月 -0.522235 0.312747 -2.080648 -0.3401845月 -0.846441 0.684210 0.123533 0.5747636月 0.936999 0.513108 -0.707748 0.5845617月 0.525463 1.402563 -0.209058 -0.7476048月 -0.106979 -2.185515 0.107163 -2.0483529月 0.202413 -0.761678 -0.527682 0.44981210月 0.392586 -0.007398 1.338289 0.02584211月 -0.017325 -0.462926 -1.809013 -0.35782212月 0.919389 -0.854471 0.830529 -0.851821In [39]: print(data_frame.sort_index(axis=0, by='A', ascending=False)) #表示 ...: 按行索引降序排序C:\\Users\\13560\\Anaconda\\Scripts\\ipython:1: FutureWarning: by argument to sort_index is deprecated, please use .sort_values(by=...) A B C D1月 0.722934 0.111053 0.489415 -0.1899606月 0.584561 -0.707748 0.513108 0.9369995月 0.574763 0.123533 0.684210 -0.8464419月 0.449812 -0.527682 -0.761678 0.2024132月 0.203031 -0.157956 -1.911171 0.87548010月 0.025842 1.338289 -0.007398 0.3925864月 -0.340184 -2.080648 0.312747 -0.52223511月 -0.357822 -1.809013 -0.462926 -0.0173257月 -0.747604 -0.209058 1.402563 0.52546312月 -0.851821 0.830529 -0.854471 0.9193893月 -0.935644 1.343663 0.617676 -0.9468238月 -2.048352 0.107163 -2.185515 -0.106979 由上面的结果可以清晰的看到排序的返回结果，加上by=参数，可以按照某一列或行结果排序，排序完成的很好。排序是一步很重要的数据处理的工作。 4.3 选择数据DataFrame选择数据可以通过行、列标签名df.loc[标签名]，也可以通过索引df.iloc[索引值或切片]，也可混合上面两种取值df.ix[标签名, 索引值或切片] .loc[]演示（通过标签取值） 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849In [46]: data_frame.loc['1月': '5月']Out[46]: A B C D1月 0.722934 0.111053 0.489415 -0.1899602月 0.203031 -0.157956 -1.911171 0.8754803月 -0.935644 1.343663 0.617676 -0.9468234月 -0.340184 -2.080648 0.312747 -0.5222355月 0.574763 0.123533 0.684210 -0.846441In [47]: data_frame.loc['1月': '5月', 'A']Out[47]:1月 0.7229342月 0.2030313月 -0.9356444月 -0.3401845月 0.574763Name: A, dtype: float64In [48]: data_frame.loc['1月': '5月', 'A':'C']Out[48]: A B C1月 0.722934 0.111053 0.4894152月 0.203031 -0.157956 -1.9111713月 -0.935644 1.343663 0.6176764月 -0.340184 -2.080648 0.3127475月 0.574763 0.123533 0.684210In [49]: data_frame.loc[,'A':'C'] File \"&lt;ipython-input-49-a821fd8f12f8&gt;\", line 1 data_frame.loc[,'A':'C'] ^SyntaxError: invalid syntaxIn [50]: data_frame.loc[:,'A':'C']Out[50]: A B C1月 0.722934 0.111053 0.4894152月 0.203031 -0.157956 -1.9111713月 -0.935644 1.343663 0.6176764月 -0.340184 -2.080648 0.3127475月 0.574763 0.123533 0.6842106月 0.584561 -0.707748 0.5131087月 -0.747604 -0.209058 1.4025638月 -2.048352 0.107163 -2.1855159月 0.449812 -0.527682 -0.76167810月 0.025842 1.338289 -0.00739811月 -0.357822 -1.809013 -0.46292612月 -0.851821 0.830529 -0.854471 最直接的是通过.loc[行标签]直接取行，也可以取列，.loc[:, 列标签]，不过这里美中不足的一点是没办法取得两个不相邻的列，相邻的列可以通过切片完成。 .iloc[]通过数字索引取值123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869In [55]: data_frameOut[55]: A B C D1月 0.722934 0.111053 0.489415 -0.1899602月 0.203031 -0.157956 -1.911171 0.8754803月 -0.935644 1.343663 0.617676 -0.9468234月 -0.340184 -2.080648 0.312747 -0.5222355月 0.574763 0.123533 0.684210 -0.8464416月 0.584561 -0.707748 0.513108 0.9369997月 -0.747604 -0.209058 1.402563 0.5254638月 -2.048352 0.107163 -2.185515 -0.1069799月 0.449812 -0.527682 -0.761678 0.20241310月 0.025842 1.338289 -0.007398 0.39258611月 -0.357822 -1.809013 -0.462926 -0.01732512月 -0.851821 0.830529 -0.854471 0.919389In [56]: data_frame.iloc[0]Out[56]:A 0.722934B 0.111053C 0.489415D -0.189960Name: 1月, dtype: float64In [57]: data_frame.iloc[0:3]Out[57]: A B C D1月 0.722934 0.111053 0.489415 -0.1899602月 0.203031 -0.157956 -1.911171 0.8754803月 -0.935644 1.343663 0.617676 -0.946823In [58]: data_frame.iloc[:, 0]Out[58]:1月 0.7229342月 0.2030313月 -0.9356444月 -0.3401845月 0.5747636月 0.5845617月 -0.7476048月 -2.0483529月 0.44981210月 0.02584211月 -0.35782212月 -0.851821Name: A, dtype: float64In [59]: data_frame.iloc[:, 0:2]Out[59]: A B1月 0.722934 0.1110532月 0.203031 -0.1579563月 -0.935644 1.3436634月 -0.340184 -2.0806485月 0.574763 0.1235336月 0.584561 -0.7077487月 -0.747604 -0.2090588月 -2.048352 0.1071639月 0.449812 -0.52768210月 0.025842 1.33828911月 -0.357822 -1.80901312月 -0.851821 0.830529In [60]: data_frame.iloc[0:3, 0:2]Out[60]: A B1月 0.722934 0.1110532月 0.203031 -0.1579563月 -0.935644 1.343663 通过这种方式取值可以不用管标签是什么，直接能够按照行号与列号取出数据，但是美中不足的一点也是么有办法同时取出不相邻的两列 .ix[]通过标签与索引混合取值1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950In [61]: data_frameOut[61]: A B C D1月 0.722934 0.111053 0.489415 -0.1899602月 0.203031 -0.157956 -1.911171 0.8754803月 -0.935644 1.343663 0.617676 -0.9468234月 -0.340184 -2.080648 0.312747 -0.5222355月 0.574763 0.123533 0.684210 -0.8464416月 0.584561 -0.707748 0.513108 0.9369997月 -0.747604 -0.209058 1.402563 0.5254638月 -2.048352 0.107163 -2.185515 -0.1069799月 0.449812 -0.527682 -0.761678 0.20241310月 0.025842 1.338289 -0.007398 0.39258611月 -0.357822 -1.809013 -0.462926 -0.01732512月 -0.851821 0.830529 -0.854471 0.919389In [62]: data_frame.ix[0]C:\\Users\\13560\\Anaconda\\Scripts\\ipython:1: DeprecationWarning:.ix is deprecated. Please use.loc for label based indexing or.iloc for positional indexingOut[62]:A 0.722934B 0.111053C 0.489415D -0.189960Name: 1月, dtype: float64In [63]: data_frame.ix['1月']C:\\Users\\13560\\Anaconda\\Scripts\\ipython:1: DeprecationWarning:.ix is deprecated. Please use.loc for label based indexing or.iloc for positional indexingOut[63]:A 0.722934B 0.111053C 0.489415D -0.189960Name: 1月, dtype: float64In [64]: data_frame.ix['1月':'3月',0:3]C:\\Users\\13560\\Anaconda\\Scripts\\ipython:1: DeprecationWarning:.ix is deprecated. Please use.loc for label based indexing or.iloc for positional indexingOut[64]: A B C1月 0.722934 0.111053 0.4894152月 0.203031 -0.157956 -1.9111713月 -0.935644 1.343663 0.617676 这个方法倒是还能用，但是官方已经不提倡这么使用了，希望大家能够用.loc和.iloc取值就好了，所以这个可记可不记。 4.4 空数据处理4.5 数据合并5. 数据导入或导出5.1 数据导入pandas可以读取多种文件，返回dataframe类型。 read_csv() read_excel() read_txt() 5.2 数据导出pandas可以将dataframe数据等array数据写出成各种格式，比如： to_csv() to_excel() to_json() 6. 总结手握numpy和pandas两个强大的package，处理数据便可游刃有余、得心应手。这是数据科学的基础能力，同时也是现在基于大数据的机器学习算法的学习基础。","link":"/2020/04/23/Python/pandas/"}],"tags":[{"name":"blast","slug":"blast","link":"/tags/blast/"},{"name":"gene anotation","slug":"gene-anotation","link":"/tags/gene-anotation/"},{"name":"blast2go","slug":"blast2go","link":"/tags/blast2go/"},{"name":"KEGG","slug":"KEGG","link":"/tags/KEGG/"},{"name":"GO","slug":"GO","link":"/tags/GO/"},{"name":"edgeR","slug":"edgeR","link":"/tags/edgeR/"},{"name":"基因差异分析","slug":"基因差异分析","link":"/tags/基因差异分析/"},{"name":"bash","slug":"bash","link":"/tags/bash/"},{"name":"linux shell","slug":"linux-shell","link":"/tags/linux-shell/"},{"name":"Linux管道符","slug":"Linux管道符","link":"/tags/Linux管道符/"},{"name":"Linux软硬链接","slug":"Linux软硬链接","link":"/tags/Linux软硬链接/"},{"name":"Linux文本操作","slug":"Linux文本操作","link":"/tags/Linux文本操作/"},{"name":"机器学习概览","slug":"机器学习概览","link":"/tags/机器学习概览/"},{"name":"analysis of variance","slug":"analysis-of-variance","link":"/tags/analysis-of-variance/"},{"name":"线性方程组","slug":"线性方程组","link":"/tags/线性方程组/"},{"name":"概率论与数理统计","slug":"概率论与数理统计","link":"/tags/概率论与数理统计/"},{"name":"nlp数据处理","slug":"nlp数据处理","link":"/tags/nlp数据处理/"},{"name":"文本分类","slug":"文本分类","link":"/tags/文本分类/"},{"name":"nlp","slug":"nlp","link":"/tags/nlp/"},{"name":"-nlp -天池","slug":"nlp-天池","link":"/tags/nlp-天池/"},{"name":"生活的诗","slug":"生活的诗","link":"/tags/生活的诗/"},{"name":"博客建设","slug":"博客建设","link":"/tags/博客建设/"},{"name":"Markdown","slug":"Markdown","link":"/tags/Markdown/"},{"name":"data analysis","slug":"data-analysis","link":"/tags/data-analysis/"},{"name":"Regular expression","slug":"Regular-expression","link":"/tags/Regular-expression/"},{"name":"numpy","slug":"numpy","link":"/tags/numpy/"},{"name":"latex for mathematics formula","slug":"latex-for-mathematics-formula","link":"/tags/latex-for-mathematics-formula/"},{"name":"Try new thing","slug":"Try-new-thing","link":"/tags/Try-new-thing/"},{"name":"electron开发","slug":"electron开发","link":"/tags/electron开发/"},{"name":"js学习","slug":"js学习","link":"/tags/js学习/"},{"name":"转录组分析实践","slug":"转录组分析实践","link":"/tags/转录组分析实践/"},{"name":"Quality Controling","slug":"Quality-Controling","link":"/tags/Quality-Controling/"},{"name":"转录组","slug":"转录组","link":"/tags/转录组/"},{"name":"质控可视化","slug":"质控可视化","link":"/tags/质控可视化/"},{"name":"shell_script","slug":"shell-script","link":"/tags/shell-script/"},{"name":"python基础","slug":"python基础","link":"/tags/python基础/"},{"name":"数据可视化","slug":"数据可视化","link":"/tags/数据可视化/"},{"name":"seaborn-visualization","slug":"seaborn-visualization","link":"/tags/seaborn-visualization/"},{"name":"数据结构与算法","slug":"数据结构与算法","link":"/tags/数据结构与算法/"},{"name":"无参转录组实战","slug":"无参转录组实战","link":"/tags/无参转录组实战/"},{"name":"latex-layout","slug":"latex-layout","link":"/tags/latex-layout/"},{"name":"pandas","slug":"pandas","link":"/tags/pandas/"}],"categories":[{"name":"Bioinformatics","slug":"Bioinformatics","link":"/categories/Bioinformatics/"},{"name":"Linux","slug":"Linux","link":"/categories/Linux/"},{"name":"Machinelearning","slug":"Machinelearning","link":"/categories/Machinelearning/"},{"name":"Mathematics","slug":"Mathematics","link":"/categories/Mathematics/"},{"name":"NLP","slug":"NLP","link":"/categories/NLP/"},{"name":"Poetry with Life","slug":"Poetry-with-Life","link":"/categories/Poetry-with-Life/"},{"name":"Python","slug":"Python","link":"/categories/Python/"},{"name":"log","slug":"log","link":"/categories/log/"},{"name":"web","slug":"web","link":"/categories/web/"},{"name":"计算机基础","slug":"计算机基础","link":"/categories/计算机基础/"},{"name":"Practical skills","slug":"Practical-skills","link":"/categories/Practical-skills/"}]}